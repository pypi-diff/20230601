# Comparing `tmp/read_rapidpe-0.5.2.tar.gz` & `tmp/read_rapidpe-0.5.3.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "read_rapidpe-0.5.2.tar", max compression
+gzip compressed data, was "read_rapidpe-0.5.3.tar", max compression
```

## Comparing `read_rapidpe-0.5.2.tar` & `read_rapidpe-0.5.3.tar`

### file list

```diff
@@ -1,11 +1,11 @@
--rw-r--r--   0        0        0     2399 2023-04-17 13:15:50.969962 read_rapidpe-0.5.2/README.md
--rw-r--r--   0        0        0      627 2023-05-17 02:52:48.887006 read_rapidpe-0.5.2/pyproject.toml
--rw-r--r--   0        0        0      303 2023-05-17 02:31:18.161235 read_rapidpe-0.5.2/read_rapidpe/__init__.py
--rw-r--r--   0        0        0    11295 2023-05-13 16:31:51.989559 read_rapidpe-0.5.2/read_rapidpe/grid_point.py
--rw-r--r--   0        0        0     3445 2023-05-16 05:02:30.343719 read_rapidpe-0.5.2/read_rapidpe/io.py
--rw-r--r--   0        0        0     6083 2023-04-16 13:20:01.614285 read_rapidpe-0.5.2/read_rapidpe/p_astro.py
--rw-r--r--   0        0        0     9718 2023-04-07 15:04:09.805939 read_rapidpe-0.5.2/read_rapidpe/parser.py
--rw-r--r--   0        0        0     1802 2023-05-17 02:28:36.535926 read_rapidpe-0.5.2/read_rapidpe/plot.py
--rw-r--r--   0        0        0    24661 2023-05-17 02:20:02.934691 read_rapidpe-0.5.2/read_rapidpe/result.py
--rw-r--r--   0        0        0     5048 2023-05-17 02:39:13.305432 read_rapidpe-0.5.2/read_rapidpe/transform.py
--rw-r--r--   0        0        0     3330 1970-01-01 00:00:00.000000 read_rapidpe-0.5.2/PKG-INFO
+-rw-r--r--   0        0        0     2399 2023-04-17 13:15:50.969962 read_rapidpe-0.5.3/README.md
+-rw-r--r--   0        0        0      627 2023-06-01 09:59:41.474820 read_rapidpe-0.5.3/pyproject.toml
+-rw-r--r--   0        0        0      303 2023-05-17 02:31:18.161235 read_rapidpe-0.5.3/read_rapidpe/__init__.py
+-rw-r--r--   0        0        0    11295 2023-05-13 16:31:51.989559 read_rapidpe-0.5.3/read_rapidpe/grid_point.py
+-rw-r--r--   0        0        0     3445 2023-05-16 05:02:30.343719 read_rapidpe-0.5.3/read_rapidpe/io.py
+-rw-r--r--   0        0        0     6083 2023-04-16 13:20:01.614285 read_rapidpe-0.5.3/read_rapidpe/p_astro.py
+-rw-r--r--   0        0        0     9718 2023-04-07 15:04:09.805939 read_rapidpe-0.5.3/read_rapidpe/parser.py
+-rw-r--r--   0        0        0     1802 2023-05-17 02:28:36.535926 read_rapidpe-0.5.3/read_rapidpe/plot.py
+-rw-r--r--   0        0        0    27219 2023-06-01 09:53:24.865061 read_rapidpe-0.5.3/read_rapidpe/result.py
+-rw-r--r--   0        0        0     7203 2023-05-21 08:35:06.127055 read_rapidpe-0.5.3/read_rapidpe/transform.py
+-rw-r--r--   0        0        0     3330 1970-01-01 00:00:00.000000 read_rapidpe-0.5.3/PKG-INFO
```

### Comparing `read_rapidpe-0.5.2/README.md` & `read_rapidpe-0.5.3/README.md`

 * *Files identical despite different names*

### Comparing `read_rapidpe-0.5.2/pyproject.toml` & `read_rapidpe-0.5.3/pyproject.toml`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 [tool.poetry]
 name = "read-rapidpe"
-version = "0.5.2"
+version = "0.5.3"
 description = "Read and analyse results generated by rapidpe-rift-pipe"
 authors = ["Cory Chu <cory@gwlab.page>"]
 readme = "README.md"
 packages = [{include = "read_rapidpe"}]
 homepage = "https://github.com/c0rychu/read-rapidpe"
 repository = "https://git.ligo.org/yu-kuang.chu/read-rapidpe"
```

### Comparing `read_rapidpe-0.5.2/read_rapidpe/grid_point.py` & `read_rapidpe-0.5.3/read_rapidpe/grid_point.py`

 * *Files identical despite different names*

### Comparing `read_rapidpe-0.5.2/read_rapidpe/io.py` & `read_rapidpe-0.5.3/read_rapidpe/io.py`

 * *Files identical despite different names*

### Comparing `read_rapidpe-0.5.2/read_rapidpe/p_astro.py` & `read_rapidpe-0.5.3/read_rapidpe/p_astro.py`

 * *Files identical despite different names*

### Comparing `read_rapidpe-0.5.2/read_rapidpe/parser.py` & `read_rapidpe-0.5.3/read_rapidpe/parser.py`

 * *Files identical despite different names*

### Comparing `read_rapidpe-0.5.2/read_rapidpe/plot.py` & `read_rapidpe-0.5.3/read_rapidpe/plot.py`

 * *Files identical despite different names*

### Comparing `read_rapidpe-0.5.2/read_rapidpe/result.py` & `read_rapidpe-0.5.3/read_rapidpe/result.py`

 * *Files 12% similar despite different names*

```diff
@@ -2,14 +2,15 @@
 Read and parse RapidPE output result for post-processing.
 Author: Cory Chu <cory@gwlab.page>
 """
 
 from functools import cached_property
 from pathlib import Path
 from joblib import Parallel, delayed
+from configparser import ConfigParser
 import re
 import numpy as np
 import h5py
 # import pandas as pd
 from .grid_point import RapidPE_grid_point
 from .io import load_event_info_dict_txt
 from .io import load_injection_info_txt
@@ -88,27 +89,59 @@
         if result is None:
             self.grid_points = np.empty(0, dtype=object)
             self._keys = []
         else:
             self.grid_points = result.grid_points
             self._keys = result._keys
 
-            for attr in result._keys + ["event_info", "injection_info"]:
+            for attr in result._keys + ["event_info",
+                                        "injection_info",
+                                        "config_info"]:
                 try:
                     setattr(self, attr, getattr(result, attr))
                 except AttributeError:
                     pass
             # Try:
             # self.mass_1 = result.mass_1
             # self.mass_2 = result.mass_2
             # self.spin_1z = result.spin_1z
             # self.spin_2z = result.spin_2z
             # self.marg_log_likelihood = result.marg_log_likelihood
             # ...
 
+    def __getitem__(self, key):
+        """
+        Get res.attr by res["attr"]
+        """
+        return getattr(self, key)
+
+    @property
+    def grid_coordinates(self):
+        """
+        List of grid coordinates in wich the grid points are rectilinear
+        """
+        iparam_search = self.config_info["intrinsic_param_to_search"]
+        distant_coord = self.config_info["distance_coordinates"]
+        if iparam_search == "[mass1,mass2]":
+            if distant_coord == "mchirp_q":
+                return ["chirp_mass", "mass_ratio"]
+            elif distant_coord == "mchirp_eta":
+                return ["chirp_mass", "symmetric_mass_ratio"]
+            else:
+                raise NotImplementedError("distance_coordinates not supported")
+        elif iparam_search == "[mass2,mass2,spin1z,spin2z]":
+            if distant_coord == "mchirp_q":
+                return ["chirp_mass", "mass_ratio", "chi_eff", "chi_a"]
+            elif distant_coord == "mchirp_eta":
+                return ["chirp_mass", "symmetric_mass_ratio", "chi_eff", "chi_a"]  # noqa E501
+            else:
+                raise NotImplementedError("distance_coordinates not supported")
+        else:
+            raise NotImplementedError("intrinsic_param_to_search not supported")  # noqa E501
+
     @cached_property
     def intrinsic_table(self):
         """
         Combine intrinsic tables to a single dict
         """
         # return pd.DataFrame({key: getattr(self, key) for key in self._keys})
         return {key: getattr(self, key) for key in self._keys}
@@ -174,15 +207,15 @@
             for attr in result._keys:
                 try:
                     setattr(result, attr, result.intrinsic_table[attr])
                 except KeyError:
                     pass
 
             # Load event_info and injection_info
-            for attr in ["event_info", "injection_info"]:
+            for attr in ["event_info", "injection_info", "config_info"]:
                 try:
                     x = dict_from_hdf_group(f[attr])
                     setattr(result, attr, x)
                 except KeyError:
                     pass
 
         return cls(result)
@@ -221,25 +254,37 @@
                                     use_numpy=use_numpy,
                                     use_ligolw=use_ligolw,
                                     extrinsic_table=extrinsic_table,
                                     parallel_n=parallel_n)
 
         event_info_dict_txt = run_dir / "event_info_dict.txt"
         injection_info_txt = run_dir / "injection_info.txt"
+        config_ini = run_dir/"Config.ini"
 
         try:
             result.event_info = load_event_info_dict_txt(event_info_dict_txt)
         except FileNotFoundError:
             pass
 
         try:
             result.injection_info = load_injection_info_txt(injection_info_txt)
         except FileNotFoundError:
             pass
 
+        try:
+            config = ConfigParser()
+            config.read(config_ini)
+            result.config_info = \
+                {"distance_coordinates":
+                    config["GridRefine"]["distance-coordinates"],
+                 "intrinsic_param_to_search":
+                    config["General"]["intrinsic_param_to_search"]}
+        except KeyError:
+            pass
+
         return result
 
     @classmethod
     def from_xml_array(cls,
                        xml_array,
                        use_numpy=True,
                        use_ligolw=True,
@@ -340,15 +385,15 @@
             result.symmetric_mass_ratio = x.symmetric_mass_ratio
             result.mass_ratio = x.mass_ratio
             result._keys.extend(
                 ["chirp_mass", "symmetric_mass_ratio", "mass_ratio"])
 
         return cls(result)
 
-    def to_hdf(self, hdf_filename, compression=None):
+    def to_hdf(self, hdf_filename, extrinsic_table=True, compression=None):
         """
         Save result to hdf file
 
         Parameters
         ----------
         filename : str
             The name of the hdf file
@@ -357,15 +402,15 @@
             The compression method, e.g., "gzip", "lzf".
         """
 
         with h5py.File(hdf_filename, 'w', track_order=True) as f:
 
             # Check if there is extrinsic_table
             gp = self.grid_points[0]
-            extrinsic_table = True if len(gp.extrinsic_table) > 0 else False
+            extrinsic_table &= len(gp.extrinsic_table) > 0
 
             # Create "grid_points" group to hold self.grid_points
             group_grid_points_raw = \
                 f.create_group("grid_points", track_order=True)
 
             for i, gp in enumerate(self.grid_points):
                 group_gp = group_grid_points_raw.create_group(str(i))
@@ -402,15 +447,15 @@
                 for i, gp in enumerate(gps.values()):
                     vsource = h5py.VirtualSource(gp["extrinsic_table"])
                     layout[n_samples[i]:n_samples[i+1]] = vsource
 
                 f.create_virtual_dataset('extrinsic_samples', layout)
 
             # Save event_info and injection_info
-            for attr in ["event_info", "injection_info"]:
+            for attr in ["event_info", "injection_info", "config_info"]:
                 try:
                     x = getattr(self, attr)
                     g = f.create_group(attr)
                     dict_to_hdf_group(x, g)
                 except AttributeError:
                     pass
 
@@ -437,14 +482,17 @@
 
         """
 
         _supported_methods = \
             'method= "cubic", "linear", "linear-scipy", "nearest-scipy",' \
             '"cubic-scipy", "gaussian", or "gaussian-renormalized"'
 
+        grid_coord_1 = self[self.grid_coordinates[0]]
+        grid_coord_2 = self[self.grid_coordinates[1]]
+
         if method == "gaussian" or method == "gaussian-renormalized":
             # def gaussian_log_likelihood(m1, m2):
             #     mc_arr, eta_arr = transform_m1m2_to_mceta(m1, m2)
             #     sigma_mc = grid_separation_min(self.chirp_mass) *\
             #         gaussian_sigma_to_grid_size_ratio
             #     sigma_eta = grid_separation_min(self.symmetric_mass_ratio) *\
             #         gaussian_sigma_to_grid_size_ratio
@@ -457,37 +505,39 @@
             #                 (-0.5/sigma_mc**2
             #                  * (mc_arr - self.chirp_mass[i])**2) +
             #                 (-0.5/sigma_eta**2
             #                  * (eta_arr - self.symmetric_mass_ratio[i])**2)
             #             )
             #     return np.log(likelihood)
             def gaussian_log_likelihood(m1, m2):
-                mc_arr, eta_arr = transform_m1m2_to_mceta(m1, m2)
+                x = Mass_Spin.from_m1m2(m1, m2)
+                x1 = x[self.grid_coordinates[0]]
+                x2 = x[self.grid_coordinates[1]]
 
                 grid_levels = np.unique(self.iteration)
-                sigma_mc = {}
-                sigma_eta = {}
+                sigma_1 = {}
+                sigma_2 = {}
 
                 for gl in grid_levels:
-                    sigma_mc[gl] = grid_separation_min(
-                            self.chirp_mass[self.iteration == gl]
+                    sigma_1[gl] = grid_separation_min(
+                            grid_coord_1[self.iteration == gl]
                         ) * gaussian_sigma_to_grid_size_ratio
-                    sigma_eta[gl] = grid_separation_min(
-                            self.symmetric_mass_ratio[self.iteration == gl]
+                    sigma_2[gl] = grid_separation_min(
+                            grid_coord_2[self.iteration == gl]
                         ) * gaussian_sigma_to_grid_size_ratio
 
-                likelihood = np.zeros_like(mc_arr)
-                for i in range(len(self.chirp_mass)):
+                likelihood = np.zeros_like(x1)
+                for i in range(len(grid_coord_1)):
                     likelihood += \
                         np.exp(self.marg_log_likelihood[i]) * \
                         np.exp(
-                            (-0.5/sigma_mc[self.iteration[i]]**2
-                             * (mc_arr - self.chirp_mass[i])**2) +
-                            (-0.5/sigma_eta[self.iteration[i]]**2
-                             * (eta_arr - self.symmetric_mass_ratio[i])**2)
+                            (-0.5/sigma_1[self.iteration[i]]**2
+                             * (x1 - grid_coord_1[i])**2) +
+                            (-0.5/sigma_2[self.iteration[i]]**2
+                             * (x2 - grid_coord_2[i])**2)
                         )
                 return np.log(likelihood)
             self.log_likelihood = gaussian_log_likelihood
 
             if method == "gaussian-renormalized":
                 # FIXME: The max point can shft if sigma is large.
 
@@ -538,43 +588,53 @@
                 return np.log(likelihood)
 
             self.log_likelihood = gaussian_log_likelihood
 
         elif "scipy" in method:
             if method == "linear-scipy":
                 f = LinearNDInterpolator(
-                    list(zip(self.chirp_mass, self.symmetric_mass_ratio)),
+                    # list(zip(self.chirp_mass, self.symmetric_mass_ratio)),
+                    list(zip(grid_coord_1, grid_coord_2)),
                     self.marg_log_likelihood,
                     rescale=True,
                     fill_value=-100  # FIXME: is -100 okay?
                     )
             elif method == "cubic-scipy":
                 f = CloughTocher2DInterpolator(
-                    list(zip(self.chirp_mass, self.symmetric_mass_ratio)),
+                    # list(zip(self.chirp_mass, self.symmetric_mass_ratio)),
+                    list(zip(grid_coord_1, grid_coord_2)),
                     self.marg_log_likelihood,
                     rescale=True,
                     fill_value=-100  # FIXME: is -100 okay?
                     )
             elif method == "nearest-scipy":
                 f = NearestNDInterpolator(
-                    list(zip(self.chirp_mass, self.symmetric_mass_ratio)),
+                    # list(zip(self.chirp_mass, self.symmetric_mass_ratio)),
+                    list(zip(grid_coord_1, grid_coord_2)),
                     self.marg_log_likelihood,
                     rescale=True
                     )
             else:
                 raise ValueError(_supported_methods)
 
             def log_likelihood(m1, m2):
-                mc, eta = transform_m1m2_to_mceta(m1, m2)
-                ll = f(mc, eta)
+                # mc, eta = transform_m1m2_to_mceta(m1, m2)
+                # ll = f(mc, eta)
+                x = Mass_Spin.from_m1m2(m1, m2)
+                x1 = x[self.grid_coordinates[0]]
+                x2 = x[self.grid_coordinates[1]]
+                ll = f(x1, x2)
                 return ll
 
             self.log_likelihood = log_likelihood
 
         else:
+            raise DeprecationWarning(_supported_methods)
+            # TODO: remove methods uisng matplotlib triangulation
+
             triangles = Triangulation(self.chirp_mass,
                                       self.symmetric_mass_ratio)
 
             if method == "cubic":
                 f = CubicTriInterpolator(triangles, self.marg_log_likelihood)
             elif method == "linear":
                 f = LinearTriInterpolator(triangles, self.marg_log_likelihood)
@@ -601,14 +661,15 @@
         if method == "gaussian":
             # Grid-level-independent covariance matrix (old method)
             #
             # sigma_mc = grid_separation_min(self.chirp_mass) * 0.5
             # sigma_eta = grid_separation_min(self.symmetric_mass_ratio) * 0.5
             # cov = np.diag([sigma_mc**2, sigma_eta**2])
 
+            # FIXME: use grid_coordinates instead of hard-coded
             # Compute covariance matrix for different grid levels
             grid_levels = np.unique(self.iteration)
             cov = {}
             for gl in grid_levels:
                 mask = self.iteration == gl
                 sigma_mc = grid_separation_min(
                     self.chirp_mass[mask]) * 0.5
```

### Comparing `read_rapidpe-0.5.2/PKG-INFO` & `read_rapidpe-0.5.3/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: read-rapidpe
-Version: 0.5.2
+Version: 0.5.3
 Summary: Read and analyse results generated by rapidpe-rift-pipe
 Home-page: https://github.com/c0rychu/read-rapidpe
 Author: Cory Chu
 Author-email: cory@gwlab.page
 Requires-Python: >=3.8,<4.0
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3.8
```

