# Comparing `tmp/kernel_tuner-0.4.4-py3-none-any.whl.zip` & `tmp/kernel_tuner-0.4.5-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,44 +1,59 @@
-Zip file size: 112235 bytes, number of entries: 42
--rw-r--r--  2.0 unx      156 b- defN 23-Mar-09 11:21 kernel_tuner/__init__.py
--rw-r--r--  2.0 unx    12646 b- defN 23-Mar-09 11:21 kernel_tuner/c.py
--rw-r--r--  2.0 unx    33856 b- defN 23-Mar-09 11:21 kernel_tuner/core.py
--rw-r--r--  2.0 unx     9777 b- defN 23-Mar-09 11:21 kernel_tuner/cupy.py
--rw-r--r--  2.0 unx     7891 b- defN 23-Mar-09 11:21 kernel_tuner/file_utils.py
--rw-r--r--  2.0 unx     3245 b- defN 23-Mar-09 11:21 kernel_tuner/hyper.py
--rw-r--r--  2.0 unx    17421 b- defN 23-Mar-09 11:21 kernel_tuner/integration.py
--rw-r--r--  2.0 unx    31042 b- defN 23-Mar-09 11:21 kernel_tuner/interface.py
--rw-r--r--  2.0 unx     4637 b- defN 23-Mar-09 11:21 kernel_tuner/kernelbuilder.py
--rw-r--r--  2.0 unx    14192 b- defN 23-Mar-09 11:21 kernel_tuner/nvcuda.py
--rw-r--r--  2.0 unx    21374 b- defN 23-Mar-09 11:21 kernel_tuner/nvml.py
--rw-r--r--  2.0 unx     3288 b- defN 23-Mar-09 11:21 kernel_tuner/observers.py
--rw-r--r--  2.0 unx     7517 b- defN 23-Mar-09 11:21 kernel_tuner/opencl.py
--rw-r--r--  2.0 unx    15532 b- defN 23-Mar-09 11:21 kernel_tuner/pycuda.py
--rw-r--r--  2.0 unx    19914 b- defN 23-Mar-09 11:21 kernel_tuner/searchspace.py
--rw-r--r--  2.0 unx    37666 b- defN 23-Mar-09 11:21 kernel_tuner/util.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-09 11:21 kernel_tuner/runners/__init__.py
--rw-r--r--  2.0 unx     5373 b- defN 23-Mar-09 11:21 kernel_tuner/runners/sequential.py
--rw-r--r--  2.0 unx     5769 b- defN 23-Mar-09 11:21 kernel_tuner/runners/simulation.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-09 11:21 kernel_tuner/strategies/__init__.py
--rw-r--r--  2.0 unx     1934 b- defN 23-Mar-09 11:21 kernel_tuner/strategies/basinhopping.py
--rw-r--r--  2.0 unx    46964 b- defN 23-Mar-09 11:21 kernel_tuner/strategies/bayes_opt.py
--rw-r--r--  2.0 unx      562 b- defN 23-Mar-09 11:21 kernel_tuner/strategies/brute_force.py
--rw-r--r--  2.0 unx     8597 b- defN 23-Mar-09 11:21 kernel_tuner/strategies/common.py
--rw-r--r--  2.0 unx     1917 b- defN 23-Mar-09 11:21 kernel_tuner/strategies/diff_evo.py
--rw-r--r--  2.0 unx     1723 b- defN 23-Mar-09 11:21 kernel_tuner/strategies/dual_annealing.py
--rw-r--r--  2.0 unx     4882 b- defN 23-Mar-09 11:21 kernel_tuner/strategies/firefly_algorithm.py
--rw-r--r--  2.0 unx     6844 b- defN 23-Mar-09 11:21 kernel_tuner/strategies/genetic_algorithm.py
--rw-r--r--  2.0 unx     3074 b- defN 23-Mar-09 11:21 kernel_tuner/strategies/greedy_ils.py
--rw-r--r--  2.0 unx     1974 b- defN 23-Mar-09 11:21 kernel_tuner/strategies/greedy_mls.py
--rw-r--r--  2.0 unx     4081 b- defN 23-Mar-09 11:21 kernel_tuner/strategies/hillclimbers.py
--rw-r--r--  2.0 unx     1769 b- defN 23-Mar-09 11:21 kernel_tuner/strategies/minimize.py
--rw-r--r--  2.0 unx     1291 b- defN 23-Mar-09 11:21 kernel_tuner/strategies/mls.py
--rw-r--r--  2.0 unx     1292 b- defN 23-Mar-09 11:21 kernel_tuner/strategies/ordered_greedy_mls.py
--rw-r--r--  2.0 unx     4111 b- defN 23-Mar-09 11:21 kernel_tuner/strategies/pso.py
--rw-r--r--  2.0 unx     1438 b- defN 23-Mar-09 11:21 kernel_tuner/strategies/random_sample.py
--rw-r--r--  2.0 unx     4540 b- defN 23-Mar-09 11:21 kernel_tuner/strategies/simulated_annealing.py
--rw-r--r--  2.0 unx    11358 b- defN 23-Mar-09 11:22 kernel_tuner-0.4.4.dist-info/LICENSE
--rw-r--r--  2.0 unx    11904 b- defN 23-Mar-09 11:22 kernel_tuner-0.4.4.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Mar-09 11:22 kernel_tuner-0.4.4.dist-info/WHEEL
--rw-r--r--  2.0 unx       13 b- defN 23-Mar-09 11:22 kernel_tuner-0.4.4.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     3675 b- defN 23-Mar-09 11:22 kernel_tuner-0.4.4.dist-info/RECORD
-42 files, 375331 bytes uncompressed, 106351 bytes compressed:  71.7%
+Zip file size: 122468 bytes, number of entries: 57
+-rw-r--r--  2.0 unx      156 b- defN 23-Jun-01 20:11 kernel_tuner/__init__.py
+-rw-r--r--  2.0 unx    33977 b- defN 23-Jun-01 20:11 kernel_tuner/core.py
+-rw-r--r--  2.0 unx     9879 b- defN 23-Jun-01 20:11 kernel_tuner/file_utils.py
+-rw-r--r--  2.0 unx     3245 b- defN 23-Jun-01 20:11 kernel_tuner/hyper.py
+-rw-r--r--  2.0 unx    17441 b- defN 23-Jun-01 20:11 kernel_tuner/integration.py
+-rw-r--r--  2.0 unx    31190 b- defN 23-Jun-01 20:11 kernel_tuner/interface.py
+-rw-r--r--  2.0 unx     4637 b- defN 23-Jun-01 20:11 kernel_tuner/kernelbuilder.py
+-rw-r--r--  2.0 unx    21695 b- defN 23-Jun-01 20:11 kernel_tuner/searchspace.py
+-rw-r--r--  2.0 unx    38230 b- defN 23-Jun-01 20:11 kernel_tuner/util.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-01 20:11 kernel_tuner/backends/__init__.py
+-rw-r--r--  2.0 unx     2704 b- defN 23-Jun-01 20:11 kernel_tuner/backends/backend.py
+-rw-r--r--  2.0 unx    12474 b- defN 23-Jun-01 20:11 kernel_tuner/backends/c.py
+-rw-r--r--  2.0 unx     9323 b- defN 23-Jun-01 20:11 kernel_tuner/backends/cupy.py
+-rw-r--r--  2.0 unx    13378 b- defN 23-Jun-01 20:11 kernel_tuner/backends/nvcuda.py
+-rw-r--r--  2.0 unx     7771 b- defN 23-Jun-01 20:11 kernel_tuner/backends/opencl.py
+-rw-r--r--  2.0 unx    15225 b- defN 23-Jun-01 20:11 kernel_tuner/backends/pycuda.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-01 20:11 kernel_tuner/energy/__init__.py
+-rw-r--r--  2.0 unx     8174 b- defN 23-Jun-01 20:11 kernel_tuner/energy/energy.py
+-rw-r--r--  2.0 unx       79 b- defN 23-Jun-01 20:11 kernel_tuner/observers/__init__.py
+-rw-r--r--  2.0 unx      615 b- defN 23-Jun-01 20:11 kernel_tuner/observers/c.py
+-rw-r--r--  2.0 unx      756 b- defN 23-Jun-01 20:11 kernel_tuner/observers/cupy.py
+-rw-r--r--  2.0 unx      849 b- defN 23-Jun-01 20:11 kernel_tuner/observers/nvcuda.py
+-rw-r--r--  2.0 unx    23237 b- defN 23-Jun-01 20:11 kernel_tuner/observers/nvml.py
+-rw-r--r--  2.0 unx     1218 b- defN 23-Jun-01 20:11 kernel_tuner/observers/observer.py
+-rw-r--r--  2.0 unx      612 b- defN 23-Jun-01 20:11 kernel_tuner/observers/opencl.py
+-rw-r--r--  2.0 unx     3173 b- defN 23-Jun-01 20:11 kernel_tuner/observers/pmt.py
+-rw-r--r--  2.0 unx     2189 b- defN 23-Jun-01 20:11 kernel_tuner/observers/powersensor.py
+-rw-r--r--  2.0 unx      661 b- defN 23-Jun-01 20:11 kernel_tuner/observers/pycuda.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-01 20:11 kernel_tuner/runners/__init__.py
+-rw-r--r--  2.0 unx      489 b- defN 23-Jun-01 20:11 kernel_tuner/runners/runner.py
+-rw-r--r--  2.0 unx     5254 b- defN 23-Jun-01 20:11 kernel_tuner/runners/sequential.py
+-rw-r--r--  2.0 unx     5540 b- defN 23-Jun-01 20:11 kernel_tuner/runners/simulation.py
+-rw-r--r--  2.0 unx     1494 b- defN 23-Jun-01 20:11 kernel_tuner/schema/T4/1.0.0/metadata-schema.json
+-rw-r--r--  2.0 unx     2780 b- defN 23-Jun-01 20:11 kernel_tuner/schema/T4/1.0.0/results-schema.json
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-01 20:11 kernel_tuner/strategies/__init__.py
+-rw-r--r--  2.0 unx     1850 b- defN 23-Jun-01 20:11 kernel_tuner/strategies/basinhopping.py
+-rw-r--r--  2.0 unx    46270 b- defN 23-Jun-01 20:11 kernel_tuner/strategies/bayes_opt.py
+-rw-r--r--  2.0 unx      405 b- defN 23-Jun-01 20:11 kernel_tuner/strategies/brute_force.py
+-rw-r--r--  2.0 unx     8906 b- defN 23-Jun-01 20:11 kernel_tuner/strategies/common.py
+-rw-r--r--  2.0 unx     1786 b- defN 23-Jun-01 20:11 kernel_tuner/strategies/diff_evo.py
+-rw-r--r--  2.0 unx     1645 b- defN 23-Jun-01 20:11 kernel_tuner/strategies/dual_annealing.py
+-rw-r--r--  2.0 unx     4575 b- defN 23-Jun-01 20:11 kernel_tuner/strategies/firefly_algorithm.py
+-rw-r--r--  2.0 unx     6702 b- defN 23-Jun-01 20:11 kernel_tuner/strategies/genetic_algorithm.py
+-rw-r--r--  2.0 unx     2837 b- defN 23-Jun-01 20:11 kernel_tuner/strategies/greedy_ils.py
+-rw-r--r--  2.0 unx     1850 b- defN 23-Jun-01 20:11 kernel_tuner/strategies/greedy_mls.py
+-rw-r--r--  2.0 unx     3622 b- defN 23-Jun-01 20:11 kernel_tuner/strategies/hillclimbers.py
+-rw-r--r--  2.0 unx     1690 b- defN 23-Jun-01 20:11 kernel_tuner/strategies/minimize.py
+-rw-r--r--  2.0 unx     1315 b- defN 23-Jun-01 20:11 kernel_tuner/strategies/mls.py
+-rw-r--r--  2.0 unx     1316 b- defN 23-Jun-01 20:11 kernel_tuner/strategies/ordered_greedy_mls.py
+-rw-r--r--  2.0 unx     3853 b- defN 23-Jun-01 20:11 kernel_tuner/strategies/pso.py
+-rw-r--r--  2.0 unx     1245 b- defN 23-Jun-01 20:11 kernel_tuner/strategies/random_sample.py
+-rw-r--r--  2.0 unx     4350 b- defN 23-Jun-01 20:11 kernel_tuner/strategies/simulated_annealing.py
+-rw-r--r--  2.0 unx    11358 b- defN 23-Jun-01 20:12 kernel_tuner-0.4.5.dist-info/LICENSE
+-rw-r--r--  2.0 unx    11904 b- defN 23-Jun-01 20:12 kernel_tuner-0.4.5.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jun-01 20:12 kernel_tuner-0.4.5.dist-info/WHEEL
+-rw-r--r--  2.0 unx       13 b- defN 23-Jun-01 20:12 kernel_tuner-0.4.5.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     5085 b- defN 23-Jun-01 20:12 kernel_tuner-0.4.5.dist-info/RECORD
+57 files, 401114 bytes uncompressed, 114306 bytes compressed:  71.5%
```

## zipnote {}

```diff
@@ -1,19 +1,13 @@
 Filename: kernel_tuner/__init__.py
 Comment: 
 
-Filename: kernel_tuner/c.py
-Comment: 
-
 Filename: kernel_tuner/core.py
 Comment: 
 
-Filename: kernel_tuner/cupy.py
-Comment: 
-
 Filename: kernel_tuner/file_utils.py
 Comment: 
 
 Filename: kernel_tuner/hyper.py
 Comment: 
 
 Filename: kernel_tuner/integration.py
@@ -21,44 +15,95 @@
 
 Filename: kernel_tuner/interface.py
 Comment: 
 
 Filename: kernel_tuner/kernelbuilder.py
 Comment: 
 
-Filename: kernel_tuner/nvcuda.py
+Filename: kernel_tuner/searchspace.py
 Comment: 
 
-Filename: kernel_tuner/nvml.py
+Filename: kernel_tuner/util.py
 Comment: 
 
-Filename: kernel_tuner/observers.py
+Filename: kernel_tuner/backends/__init__.py
 Comment: 
 
-Filename: kernel_tuner/opencl.py
+Filename: kernel_tuner/backends/backend.py
 Comment: 
 
-Filename: kernel_tuner/pycuda.py
+Filename: kernel_tuner/backends/c.py
 Comment: 
 
-Filename: kernel_tuner/searchspace.py
+Filename: kernel_tuner/backends/cupy.py
 Comment: 
 
-Filename: kernel_tuner/util.py
+Filename: kernel_tuner/backends/nvcuda.py
+Comment: 
+
+Filename: kernel_tuner/backends/opencl.py
+Comment: 
+
+Filename: kernel_tuner/backends/pycuda.py
+Comment: 
+
+Filename: kernel_tuner/energy/__init__.py
+Comment: 
+
+Filename: kernel_tuner/energy/energy.py
+Comment: 
+
+Filename: kernel_tuner/observers/__init__.py
+Comment: 
+
+Filename: kernel_tuner/observers/c.py
+Comment: 
+
+Filename: kernel_tuner/observers/cupy.py
+Comment: 
+
+Filename: kernel_tuner/observers/nvcuda.py
+Comment: 
+
+Filename: kernel_tuner/observers/nvml.py
+Comment: 
+
+Filename: kernel_tuner/observers/observer.py
+Comment: 
+
+Filename: kernel_tuner/observers/opencl.py
+Comment: 
+
+Filename: kernel_tuner/observers/pmt.py
+Comment: 
+
+Filename: kernel_tuner/observers/powersensor.py
+Comment: 
+
+Filename: kernel_tuner/observers/pycuda.py
 Comment: 
 
 Filename: kernel_tuner/runners/__init__.py
 Comment: 
 
+Filename: kernel_tuner/runners/runner.py
+Comment: 
+
 Filename: kernel_tuner/runners/sequential.py
 Comment: 
 
 Filename: kernel_tuner/runners/simulation.py
 Comment: 
 
+Filename: kernel_tuner/schema/T4/1.0.0/metadata-schema.json
+Comment: 
+
+Filename: kernel_tuner/schema/T4/1.0.0/results-schema.json
+Comment: 
+
 Filename: kernel_tuner/strategies/__init__.py
 Comment: 
 
 Filename: kernel_tuner/strategies/basinhopping.py
 Comment: 
 
 Filename: kernel_tuner/strategies/bayes_opt.py
@@ -105,23 +150,23 @@
 
 Filename: kernel_tuner/strategies/random_sample.py
 Comment: 
 
 Filename: kernel_tuner/strategies/simulated_annealing.py
 Comment: 
 
-Filename: kernel_tuner-0.4.4.dist-info/LICENSE
+Filename: kernel_tuner-0.4.5.dist-info/LICENSE
 Comment: 
 
-Filename: kernel_tuner-0.4.4.dist-info/METADATA
+Filename: kernel_tuner-0.4.5.dist-info/METADATA
 Comment: 
 
-Filename: kernel_tuner-0.4.4.dist-info/WHEEL
+Filename: kernel_tuner-0.4.5.dist-info/WHEEL
 Comment: 
 
-Filename: kernel_tuner-0.4.4.dist-info/top_level.txt
+Filename: kernel_tuner-0.4.5.dist-info/top_level.txt
 Comment: 
 
-Filename: kernel_tuner-0.4.4.dist-info/RECORD
+Filename: kernel_tuner-0.4.5.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## kernel_tuner/__init__.py

```diff
@@ -1,4 +1,4 @@
 from kernel_tuner.integration import store_results, create_device_targets
 from kernel_tuner.interface import tune_kernel, run_kernel
 
-__version__ = "0.4.4"
+__version__ = "0.4.5"
```

## kernel_tuner/core.py

```diff
@@ -7,21 +7,22 @@
 import numpy as np
 
 try:
     import cupy as cp
 except ImportError:
     cp = np
 
-from kernel_tuner.cupy import CupyFunctions
-from kernel_tuner.pycuda import PyCudaFunctions
-from kernel_tuner.nvcuda import CudaFunctions
-from kernel_tuner.c import CFunctions
-from kernel_tuner.nvml import NVMLObserver
-from kernel_tuner.observers import ContinuousObserver
-from kernel_tuner.opencl import OpenCLFunctions
+from kernel_tuner.observers.nvml import NVMLObserver
+from kernel_tuner.observers.observer import ContinuousObserver
+from kernel_tuner.backends.cupy import CupyFunctions
+from kernel_tuner.backends.pycuda import PyCudaFunctions
+from kernel_tuner.backends.nvcuda import CudaFunctions
+from kernel_tuner.backends.opencl import OpenCLFunctions
+from kernel_tuner.backends.c import CFunctions
+from kernel_tuner.backends.opencl import OpenCLFunctions
 import kernel_tuner.util as util
 
 try:
     import torch
 except ImportError:
     torch = util.TorchPlaceHolder()
```

## kernel_tuner/file_utils.py

```diff
@@ -1,218 +1,252 @@
 """ This module contains utility functions for operations on files, mostly JSON cache files """
 
 import os
 import json
 import subprocess
 import xmltodict
+from sys import platform
+from pathlib import Path
 
 from importlib.metadata import requires, version, PackageNotFoundError
 from packaging.requirements import Requirement
 
 from kernel_tuner import util
 
 schema_dir = os.path.dirname(os.path.realpath(__file__)) + "/schema"
 
 
 def output_file_schema(target):
-    """ Get the requested JSON schema and the version number
+    """Get the requested JSON schema and the version number
 
     :param target: Name of the T4 schema to return, should be any of ['output', 'metadata']
     :type target: string
 
     :returns: the current version of the T4 schemas and the JSON string of the target schema
     :rtype: string, string
 
     """
     current_version = "1.0.0"
     output_file = schema_dir + f"/T4/{current_version}/{target}-schema.json"
-    with open(output_file, 'r') as fh:
+    with open(output_file, "r") as fh:
         json_string = json.load(fh)
     return current_version, json_string
 
 
 def get_configuration_validity(objective) -> str:
-    """ Convert internal Kernel Tuner error to string """
+    """Convert internal Kernel Tuner error to string"""
     errorstring: str
     if not isinstance(objective, util.ErrorConfig):
         errorstring = "correct"
     else:
         if isinstance(objective, util.CompilationFailedConfig):
             errorstring = "compile"
         elif isinstance(objective, util.RuntimeFailedConfig):
             errorstring = "runtime"
-        else:
+        elif isinstance(objective, util.InvalidConfig):
             errorstring = "constraints"
+        else:
+            raise ValueError(f"Unkown objective type {type(objective)}, value {objective}")
     return errorstring
 
 
 def filename_ensure_json_extension(filename: str) -> str:
-    """ Check if the filename has a .json extension, if not, add it """
+    """Check if the filename has a .json extension, if not, add it"""
     if filename[-5:] != ".json":
         filename += ".json"
     return filename
 
 
-def store_output_file(output_filename, results, tune_params, objective="time"):
-    """ Store the obtained auto-tuning results in a JSON output file
+def make_filenamepath(filenamepath: Path):
+    """Create the given path to a filename if the path does not yet exist"""
+    filepath = filenamepath.parents[0]
+    if not filepath.exists():
+        filepath.mkdir()
+
+
+def store_output_file(output_filename: str, results, tune_params, objective="time"):
+    """Store the obtained auto-tuning results in a JSON output file
 
     This function produces a JSON file that adheres to the T4 auto-tuning output JSON schema.
 
-    :param output_filename: Name of the to be created output file
+    :param output_filename: Name or 'path / name' of the to be created output file
     :type output_filename: string
 
     :param results: Results list as return by tune_kernel
     :type results: list of dicts
 
     :param tune_params: Tunable parameters as passed to tune_kernel
     :type tune_params: OrderedDict
 
     :param objective: The objective used during auto-tuning, default is 'time'.
     :type objective: string
 
     """
-    output_filename = filename_ensure_json_extension(output_filename)
+    output_filenamepath = Path(filename_ensure_json_extension(output_filename))
+    make_filenamepath(output_filenamepath)
 
-    timing_keys = [
-        "compile_time", "benchmark_time", "framework_time", "strategy_time",
-        "verification_time"
-    ]
-    not_measurement_keys = list(
-        tune_params.keys()) + timing_keys + ["timestamp"] + ["times"]
+    timing_keys = ["compile_time", "benchmark_time", "framework_time", "strategy_time", "verification_time"]
+    not_measurement_keys = list(tune_params.keys()) + timing_keys + ["timestamp"] + ["times"]
 
     output_data = []
 
     for result in results:
-
         out = {}
 
-        out["timestamp"] = result["timestamp"]
-        out["configuration"] = {
-            k: v
-            for k, v in result.items() if k in tune_params
-        }
+        if "timestamp" in result:
+            out["timestamp"] = result["timestamp"]
+        out["configuration"] = {k: v for k, v in result.items() if k in tune_params}
 
         # collect configuration specific timings
         timings = dict()
         timings["compilation"] = result["compile_time"]
         timings["benchmark"] = result["benchmark_time"]
         timings["framework"] = result["framework_time"]
         timings["search_algorithm"] = result["strategy_time"]
         timings["validation"] = result["verification_time"]
-        timings["runtimes"] = result["times"]
+        if "times" in result:
+            timings["runtimes"] = result["times"]
         out["times"] = timings
 
         # encode the validity of the configuration
         out["invalidity"] = get_configuration_validity(result[objective])
 
         # Kernel Tuner does not support producing results of configs that fail the correctness check
         # therefore correctness is always 1
         out["correctness"] = 1
 
         # measurements gathers everything that was measured
         measurements = []
         for key, value in result.items():
             if key not in not_measurement_keys:
-                measurements.append(
-                    dict(name=key,
-                         value=value,
-                         unit="ms" if key.startswith("time") else ""))
+                measurements.append(dict(name=key, value=value, unit="ms" if key.startswith("time") else ""))
         out["measurements"] = measurements
 
         # objectives
         # In Kernel Tuner we currently support only one objective at a time, this can be a user-defined
         # metric that combines scores from multiple different quantities into a single value to support
         # multi-objective tuning however.
         out["objectives"] = [objective]
 
         # append to output
         output_data.append(out)
 
     # write output_data to a JSON file
     version, _ = output_file_schema("results")
     output_json = dict(results=output_data, schema_version=version)
-    with open(output_filename, 'w+') as fh:
-        json.dump(output_json, fh)
+    with open(output_filenamepath, "w+") as fh:
+        json.dump(output_json, fh, cls=util.NpEncoder)
 
 
-def get_dependencies(package='kernel_tuner'):
-    """ Get the Python dependencies of Kernel Tuner currently installed and their version numbers """
+def get_dependencies(package="kernel_tuner"):
+    """Get the Python dependencies of Kernel Tuner currently installed and their version numbers"""
     requirements = requires(package)
     deps = [Requirement(req).name for req in requirements]
     depends = []
     for dep in deps:
         try:
             depends.append(f"{dep}=={version(dep)}")
         except PackageNotFoundError:
             # uninstalled packages can not have been used to produce these results
             # so it is safe to ignore
             pass
     return depends
 
 
 def get_device_query(target):
-    """ Get the information about GPUs in the current system, target is any of ['nvidia', 'amd'] """
+    """Get the information about GPUs in the current system, target is any of ['nvidia', 'amd']"""
     if target == "nvidia":
-        nvidia_smi_out = subprocess.run(["nvidia-smi", "--query", "-x"],
-                                        capture_output=True)
+        nvidia_smi_out = subprocess.run(["nvidia-smi", "--query", "-x"], capture_output=True)
         nvidia_smi = xmltodict.parse(nvidia_smi_out.stdout)
         gpu_info = nvidia_smi["nvidia_smi_log"]["gpu"]
         del_key = "processes"
         # on multi-GPU systems gpu_info is a list
         if isinstance(gpu_info, list):
             for gpu in gpu_info:
                 del gpu[del_key]
         elif isinstance(gpu_info, dict) and del_key in gpu_info:
-                del gpu_info[del_key]
+            del gpu_info[del_key]
         return nvidia_smi
     elif target == "amd":
-        rocm_smi_out = subprocess.run(["rocm-smi", "--showallinfo", "--json"],
-                                      capture_output=True)
+        rocm_smi_out = subprocess.run(["rocm-smi", "--showallinfo", "--json"], capture_output=True)
         return json.loads(rocm_smi_out.stdout)
     else:
         raise ValueError("get_device_query target not supported")
 
 
-def store_metadata_file(metadata_filename):
-    """ Store the metadata about the current hardware and software environment in a JSON output file
+def store_metadata_file(metadata_filename: str):
+    """Store the metadata about the current hardware and software environment in a JSON output file
 
     This function produces a JSON file that adheres to the T4 auto-tuning metadata JSON schema.
 
-    :param metadata_filename: Name of the to be created metadata file
+    :param metadata_filename: Name or 'path / name' of the to be created metadata file
     :type metadata_filename: string
 
     """
-    metadata_filename = filename_ensure_json_extension(metadata_filename)
+    metadata_filenamepath = Path(filename_ensure_json_extension(metadata_filename))
+    make_filenamepath(metadata_filenamepath)
     metadata = {}
+    supported_operating_systems = ["linux", "win32", "darwin"]
 
-    # lshw only works on Linux, this intentionally raises a FileNotFoundError when ran on systems that do not have it
-    lshw_out = subprocess.run(["lshw", "-json"], capture_output=True)
+    if all(platform != supported for supported in supported_operating_systems):
+        raise ValueError(f"Platform {platform} not supported for metadata collection")
 
-    # sometimes lshw outputs a list of length 1, sometimes just as a dict, schema wants a list
-    lshw_string = lshw_out.stdout.decode('utf-8').strip()
-    if lshw_string[0] == '{' and lshw_string[-1] == '}':
-        lshw_string = '[' + lshw_string + ']'
+    try:
+        # differentiate between OSes, possible values: https://docs.python.org/3/library/sys.html#sys.platform
+        if platform == "linux":
+            os_string = "Linux"
+            hardware_description_out = subprocess.run(["lshw", "-json"], capture_output=True)
+        elif platform == "win32":
+            os_string = "Windows"
+            raise NotImplementedError("Hardware specification not yet implemented for Windows")
+        elif platform == "darwin":
+            os_string = "Mac"
+            hardware_description_out = subprocess.run(
+                [
+                    "system_profiler",
+                    "-json",
+                    "-detailLevel",
+                    "mini",
+                    "SPSoftwareDataType",
+                    "SPHardwareDataType",
+                    "SPiBridgeDataType",
+                    "SPPCIDataType",
+                    "SPMemoryDataType",
+                    "SPNVMeDataType",
+                ],
+                capture_output=True,
+            )
+        else:
+            raise ValueError("This code is supposed to be unreachable, the supported platform check has failed")
 
-    metadata["hardware"] = dict(lshw=json.loads(lshw_string))
+        # process the hardware description output
+        hardware_description_string = hardware_description_out.stdout.decode("utf-8").strip()
+        if hardware_description_string[0] == "{" and hardware_description_string[-1] == "}":
+            # sometimes lshw outputs a list of length 1, sometimes just as a dict, schema wants a list
+            hardware_description_string = "[" + hardware_description_string + "]"
+        metadata["operating_system"] = os_string
+    except:
+        hardware_description_string = "[error retrieving hardware description]"
+        metadata["operating_system"] = "unidentified OS"
+    metadata["hardware"] = dict(hardware_description=json.loads(hardware_description_string))
 
     # attempts to use nvidia-smi or rocm-smi if present
     device_query = {}
     try:
-        device_query['nvidia-smi'] = get_device_query("nvidia")
+        device_query["nvidia-smi"] = get_device_query("nvidia")
     except FileNotFoundError:
         # ignore if nvidia-smi is not found
         pass
 
     try:
-        device_query['rocm-smi'] = get_device_query("amd")
+        device_query["rocm-smi"] = get_device_query("amd")
     except FileNotFoundError:
         # ignore if rocm-smi is not found
         pass
 
-    metadata["environment"] = dict(device_query=device_query,
-                                   requirements=get_dependencies())
+    metadata["environment"] = dict(device_query=device_query, requirements=get_dependencies())
 
     # write metadata to JSON file
     version, _ = output_file_schema("metadata")
     metadata_json = dict(metadata=metadata, schema_version=version)
-    with open(metadata_filename, 'w+') as fh:
+    with open(metadata_filenamepath, "w+") as fh:
         json.dump(metadata_json, fh, indent="  ")
```

## kernel_tuner/integration.py

```diff
@@ -20,18 +20,19 @@
     "GFLOP/J": True,
     "TFLOP/J": True
 }
 
 def get_objective_defaults(objective, objective_higher_is_better):
     """ Uses time as default objective and attempts to lookup objective_higher_is_better for known objectives """
     objective = objective or "time"
-    if objective_higher_is_better is None and objective in objective_default_map:
-        objective_higher_is_better = objective_default_map[objective]
-    else:
-        raise ValueError(f"Please specify objective_higher_is_better for objective {objective}")
+    if objective_higher_is_better is None:
+        if objective in objective_default_map:
+            objective_higher_is_better = objective_default_map[objective]
+        else:
+            raise ValueError(f"Please specify objective_higher_is_better for objective {objective}")
     return objective, objective_higher_is_better
 
 schema_v1_0 = {
     "$schema": "https://json-schema.org/draft-07/schema#",
     "type": "object",
     "properties": {
         "version_number": {"type": "string"},
```

## kernel_tuner/interface.py

```diff
@@ -33,14 +33,15 @@
 from kernel_tuner.integration import get_objective_defaults
 
 import kernel_tuner.util as util
 import kernel_tuner.core as core
 
 from kernel_tuner.runners.sequential import SequentialRunner
 from kernel_tuner.runners.simulation import SimulationRunner
+from kernel_tuner.searchspace import Searchspace
 
 try:
     import torch
 except ImportError:
     torch = util.TorchPlaceHolder()
 
 from kernel_tuner.strategies import (
@@ -597,15 +598,14 @@
         raise ValueError("Iterations should be at least one!")
 
     # sort all the options into separate dicts
     opts = locals()
     kernel_options = Options([(k, opts[k]) for k in _kernel_options.keys()])
     tuning_options = Options([(k, opts[k]) for k in _tuning_options.keys()])
     device_options = Options([(k, opts[k]) for k in _device_options.keys()])
-    tuning_options["snap"] = True
     tuning_options["unique_results"] = {}
     if strategy_options and "max_fevals" in strategy_options:
         tuning_options["max_fevals"] = strategy_options["max_fevals"]
     if strategy_options and "time_limit" in strategy_options:
         tuning_options["time_limit"] = strategy_options["time_limit"]
 
     logging.debug("tune_kernel called")
@@ -658,17 +658,21 @@
             cache += ".json"
 
         util.process_cache(cache, kernel_options, tuning_options, runner)
     else:
         tuning_options.cache = {}
         tuning_options.cachefile = None
 
+    # create search space
+    searchspace = Searchspace(tune_params, restrictions, runner.dev.max_threads)
+
     # call the strategy to execute the tuning process
     tuning_options["start_time"] = perf_counter()
-    results, env = strategy.tune(runner, kernel_options, device_options, tuning_options)
+    results = strategy.tune(searchspace, runner, tuning_options)
+    env = runner.get_environment(tuning_options)
 
     # finished iterating over search space
     if not device_options.quiet:
         if results:    # checks if results is not empty
             best_config = util.get_best_config(results, objective, objective_higher_is_better)
             units = getattr(runner, "units", None)
             print("best performing configuration:")
```

## kernel_tuner/searchspace.py

```diff
@@ -12,100 +12,112 @@
 
 
 class Searchspace:
     """Class that offers the search space to strategies"""
 
     def __init__(
         self,
-        tuning_options: dict,
+        tune_params: dict,
+        restrictions,
         max_threads: int,
+        block_size_names=default_block_size_names,
         build_neighbors_index=False,
         neighbor_method=None,
-        sort=False,
-        sort_last_param_first=False,
     ) -> None:
         """Build a searchspace using the variables and constraints.
         Optionally build the neighbors index - only faster if you repeatedly look up neighbors. Methods:
             strictly-adjacent: differs +1 or -1 parameter index value for each parameter
             adjacent: picks closest parameter value in both directions for each parameter
             Hamming: any parameter config with 1 different parameter value is a neighbor
         Optionally sort the searchspace by the order in which the parameter values were specified. By default, sort goes from first to last parameter, to reverse this use sort_last_param_first.
         """
-        self.tuning_options = tuning_options
-        self.restrictions = tuning_options.restrictions
-        self.tune_params = tuning_options.tune_params
-        self.max_threads = max_threads
+        self.tune_params = tune_params
+        self.restrictions = restrictions
         self.param_names = list(self.tune_params.keys())
-        self.params_values = tuple(tuple(param_vals) for param_vals in self.tune_params.values())
+        self.params_values = tuple(
+            tuple(param_vals) for param_vals in self.tune_params.values()
+        )
         self.params_values_indices = None
         self.build_neighbors_index = build_neighbors_index
         self.__neighbor_cache = dict()
         self.neighbor_method = neighbor_method
-        if (neighbor_method is not None or build_neighbors_index) and neighbor_method not in supported_neighbor_methods:
-            raise ValueError(f"Neighbor method is {neighbor_method}, must be one of {supported_neighbor_methods}")
+        if (
+            neighbor_method is not None or build_neighbors_index
+        ) and neighbor_method not in supported_neighbor_methods:
+            raise ValueError(
+                f"Neighbor method is {neighbor_method}, must be one of {supported_neighbor_methods}"
+            )
 
-        self.list, self.__numpy, self.__dict, self.size = self.__build_searchspace(sort, sort_last_param_first)
+        self.list, self.__numpy, self.__dict, self.size = self.__build_searchspace(
+            block_size_names, max_threads
+        )
         self.num_params = len(self.tune_params)
         self.indices = np.arange(self.size)
         if neighbor_method is not None and neighbor_method != "Hamming":
             self.__prepare_neighbors_index()
         if build_neighbors_index:
             self.neighbors_index = self.__build_neighbors_index(neighbor_method)
 
-    def __build_searchspace(self, sort: bool, sort_last_param_first: bool) -> Tuple[List[tuple], np.ndarray, dict, int]:
+    def __build_searchspace(
+        self, block_size_names: list, max_threads: int
+    ) -> Tuple[List[tuple], np.ndarray, dict, int]:
         """compute valid configurations in a search space based on restrictions and max_threads, returns the searchspace, a dict of the searchspace for fast lookups and the size"""
 
         # instantiate the parameter space with all the variables
         parameter_space = Problem()
         for param_name, param_values in self.tune_params.items():
             parameter_space.addVariable(param_name, param_values)
 
         # add the user-specified restrictions as constraints on the parameter space
         parameter_space = self.__add_restrictions(parameter_space)
 
         # add the default blocksize threads restrictions last, because it is unlikely to reduce the parameter space by much
-        block_size_names = self.tuning_options.get("block_size_names", default_block_size_names)
-        block_size_names = list(block_size_name for block_size_name in block_size_names if block_size_name in self.param_names)
-        if len(block_size_names) > 0:
-            parameter_space.addConstraint(MaxProdConstraint(self.max_threads), block_size_names)
+        valid_block_size_names = list(
+            block_size_name
+            for block_size_name in block_size_names
+            if block_size_name in self.param_names
+        )
+        if len(valid_block_size_names) > 0:
+            parameter_space.addConstraint(
+                MaxProdConstraint(max_threads), valid_block_size_names
+            )
 
         # construct the parameter space with the constraints applied
         parameter_space = parameter_space.getSolutions()
 
         # form the parameter tuples in the order specified by tune_params.keys()
-        parameter_space_list = list((tuple(params[param_name] for param_name in self.param_names)) for params in parameter_space)
-
-        # sort the parameter space on the order of parameters and their values as specified
-        if sort is True:
-            params_values_indices = list(self.get_param_indices(param_config) for param_config in parameter_space_list)
-            params_values_indices_dict = dict(zip(params_values_indices, list(range(len(params_values_indices)))))
-
-            # Python's built-in sort will sort starting in front, so if we want to vary the first parameter the tuple needs to be reversed
-            params_values_indices.sort(key=lambda t: tuple(reversed(t))) if sort_last_param_first else params_values_indices.sort()
-
-            # find the index of the parameter configuration for each parameter value index, using a dict to do it in constant time
-            new_order = [params_values_indices_dict.get(param_values_indices) for param_values_indices in params_values_indices]
-            # apply the new order
-            parameter_space_list = [parameter_space_list[i] for i in new_order]
+        parameter_space_list = list(
+            (tuple(params[param_name] for param_name in self.param_names))
+            for params in parameter_space
+        )
 
         # create a numpy array of the search space
         # in order to have the tuples as tuples in numpy, the types are set with a string, but this will make the type np.void
         # type_string = ",".join(list(type(param).__name__ for param in parameter_space_list[0]))
         parameter_space_numpy = np.array(parameter_space_list)
 
         # create a dictionary with the hashed parameter configurations as keys and indices as values for fast lookups
-        parameter_space_dict = dict(zip(parameter_space_list, list(range(parameter_space_numpy.size))))
+        parameter_space_dict = dict(
+            zip(parameter_space_list, list(range(parameter_space_numpy.size)))
+        )
 
         # check for duplicates
         size_list = len(parameter_space_list)
         size_dict = len(parameter_space_dict.keys())
         if size_list != size_dict:
-            raise ValueError(f"{size_list - size_dict} duplicate parameter configurations in the searchspace, this should not happen")
+            raise ValueError(
+                f"{size_list - size_dict} duplicate parameter configurations in the searchspace, this should not happen"
+            )
 
-        return parameter_space_list, parameter_space_numpy, parameter_space_dict, size_list
+        return (
+            parameter_space_list,
+            parameter_space_numpy,
+            parameter_space_dict,
+            size_list,
+        )
 
     def __add_restrictions(self, parameter_space: Problem) -> Problem:
         """add the user-specified restrictions as constraints on the parameter space"""
         if isinstance(self.restrictions, list):
             for restriction in self.restrictions:
                 if callable(restriction) and not isinstance(restriction, Constraint):
                     restriction = FunctionConstraint(restriction)
@@ -114,194 +126,326 @@
                 elif isinstance(restriction, Constraint):
                     parameter_space.addConstraint(restriction)
                 else:
                     raise ValueError(f"Unrecognized restriction {restriction}")
 
         # if the restrictions are the old monolithic function, apply them directly (only for backwards compatibility, likely slower than well-specified constraints!)
         elif callable(self.restrictions):
-            restrictions_wrapper = lambda *args: check_instance_restrictions(self.restrictions, dict(zip(self.param_names, args)), False)
+            restrictions_wrapper = lambda *args: check_instance_restrictions(
+                self.restrictions, dict(zip(self.param_names, args)), False
+            )
             parameter_space.addConstraint(restrictions_wrapper, self.param_names)
         elif self.restrictions is not None:
-            raise ValueError(f"The restrictions are of unsupported type {type(self.restrictions)}")
+            raise ValueError(
+                f"The restrictions are of unsupported type {type(self.restrictions)}"
+            )
         return parameter_space
 
+    def sorted_list(self, sort_last_param_first=False):
+        """returns list of parameter configs sorted based on the order in which the parameter values were specified
+
+        :param sort_last_param_first: By default, sort goes from first to last parameter, to reverse this use sort_last_param_first
+        """
+        params_values_indices = list(
+            self.get_param_indices(param_config) for param_config in self.list
+        )
+        params_values_indices_dict = dict(
+            zip(params_values_indices, list(range(len(params_values_indices))))
+        )
+
+        # Python's built-in sort will sort starting in front, so if we want to vary the first parameter the tuple needs to be reversed
+        if sort_last_param_first:
+            params_values_indices.sort(key=lambda t: tuple(reversed(t)))
+        else:
+            params_values_indices.sort()
+
+        # find the index of the parameter configuration for each parameter value index, using a dict to do it in constant time
+        new_order = [
+            params_values_indices_dict.get(param_values_indices)
+            for param_values_indices in params_values_indices
+        ]
+
+        # apply the new order
+        return [self.list[i] for i in new_order]
+
     def is_param_config_valid(self, param_config: tuple) -> bool:
         """returns whether the parameter config is valid (i.e. is in the searchspace after restrictions)"""
         return self.get_param_config_index(param_config) is not None
 
     def get_list_dict(self) -> dict:
         """get the internal dictionary"""
         return self.__dict
 
     def get_param_indices(self, param_config: tuple) -> tuple:
         """for each parameter value in the param config, find the index in the tunable parameters"""
-        return tuple(self.params_values[index].index(param_value) for index, param_value in enumerate(param_config))
+        return tuple(
+            self.params_values[index].index(param_value)
+            for index, param_value in enumerate(param_config)
+        )
 
     def get_param_configs_at_indices(self, indices: List[int]) -> List[tuple]:
         """Get the param configs at the given indices"""
         # map(get) is ~40% faster than numpy[indices] (average based on six searchspaces with 10000, 100000 and 1000000 configs and 10 or 100 random indices)
         return list(map(self.list.__getitem__, indices))
 
     def get_param_config_index(self, param_config: tuple):
         """Lookup the index for a parameter configuration, returns None if not found"""
         # constant time O(1) access - much faster than any other method, but needs a shadow dict of the search space
         return self.__dict.get(param_config, None)
 
     def __prepare_neighbors_index(self):
         """prepare by calculating the indices for the individual parameters"""
-        self.params_values_indices = np.array(list(self.get_param_indices(param_config) for param_config in self.list))
+        self.params_values_indices = np.array(
+            list(self.get_param_indices(param_config) for param_config in self.list)
+        )
 
     def __get_neighbors_indices_hamming(self, param_config: tuple) -> List[int]:
         """get the neighbors using Hamming distance from the parameter configuration"""
         num_matching_params = np.count_nonzero(self.__numpy == param_config, -1)
         matching_indices = (num_matching_params == self.num_params - 1).nonzero()[0]
         return matching_indices
 
-    def __get_neighbors_indices_strictlyadjacent(self, param_config_index: int = None, param_config: tuple = None) -> List[int]:
+    def __get_neighbors_indices_strictlyadjacent(
+        self, param_config_index: int = None, param_config: tuple = None
+    ) -> List[int]:
         """get the neighbors using strictly adjacent distance from the parameter configuration (parameter index absolute difference == 1)"""
-        param_config_value_indices = self.get_param_indices(param_config) if param_config_index is None else self.params_values_indices[param_config_index]
+        param_config_value_indices = (
+            self.get_param_indices(param_config)
+            if param_config_index is None
+            else self.params_values_indices[param_config_index]
+        )
         # calculate the absolute difference between the parameter value indices
-        abs_index_difference = np.abs(self.params_values_indices - param_config_value_indices)
+        abs_index_difference = np.abs(
+            self.params_values_indices - param_config_value_indices
+        )
         # get the param config indices where the difference is one or less for each position
         matching_indices = (np.max(abs_index_difference, axis=1) <= 1).nonzero()[0]
         # as the selected param config does not differ anywhere, remove it from the matches
         if param_config_index is not None:
-            matching_indices = np.setdiff1d(matching_indices, [param_config_index], assume_unique=False)
+            matching_indices = np.setdiff1d(
+                matching_indices, [param_config_index], assume_unique=False
+            )
         return matching_indices
 
-    def __get_neighbors_indices_adjacent(self, param_config_index: int = None, param_config: tuple = None) -> List[int]:
+    def __get_neighbors_indices_adjacent(
+        self, param_config_index: int = None, param_config: tuple = None
+    ) -> List[int]:
         """get the neighbors using adjacent distance from the parameter configuration (parameter index absolute difference >= 1)"""
-        param_config_value_indices = self.get_param_indices(param_config) if param_config_index is None else self.params_values_indices[param_config_index]
+        param_config_value_indices = (
+            self.get_param_indices(param_config)
+            if param_config_index is None
+            else self.params_values_indices[param_config_index]
+        )
         # calculate the difference between the parameter value indices
         index_difference = self.params_values_indices - param_config_value_indices
         # transpose to get the param indices difference per parameter instead of per param config
         index_difference_transposed = index_difference.transpose()
         # for each parameter get the closest upper and lower parameter (absolute index difference >= 1)
         # np.PINF has been replaced by 1e12 here, as on some systems np.PINF becomes np.NINF
         upper_bound = tuple(
-            np.min(index_difference_transposed[p][(index_difference_transposed[p] > 0).nonzero()], initial=1e12) for p in range(self.num_params))
+            np.min(
+                index_difference_transposed[p][
+                    (index_difference_transposed[p] > 0).nonzero()
+                ],
+                initial=1e12,
+            )
+            for p in range(self.num_params)
+        )
         lower_bound = tuple(
-            np.max(index_difference_transposed[p][(index_difference_transposed[p] < 0).nonzero()], initial=-1e12) for p in range(self.num_params))
+            np.max(
+                index_difference_transposed[p][
+                    (index_difference_transposed[p] < 0).nonzero()
+                ],
+                initial=-1e12,
+            )
+            for p in range(self.num_params)
+        )
         # return the indices where each parameter is within bounds
-        matching_indices = np.logical_and(index_difference <= upper_bound, index_difference >= lower_bound).all(axis=1).nonzero()[0]
+        matching_indices = (
+            np.logical_and(
+                index_difference <= upper_bound, index_difference >= lower_bound
+            )
+            .all(axis=1)
+            .nonzero()[0]
+        )
         # as the selected param config does not differ anywhere, remove it from the matches
         if param_config_index is not None:
-            matching_indices = np.setdiff1d(matching_indices, [param_config_index], assume_unique=False)
+            matching_indices = np.setdiff1d(
+                matching_indices, [param_config_index], assume_unique=False
+            )
         return matching_indices
 
-    def __build_neighbors_index(self, neighbor_method) -> np.ndarray:
+    def __build_neighbors_index(self, neighbor_method) -> List[List[int]]:
         """build an index of the neighbors for each parameter configuration"""
         # for Hamming no preperation is necessary, find the neighboring parameter configurations
         if neighbor_method == "Hamming":
-            return np.array(list(self.__get_neighbors_indices_hamming(param_config) for param_config in self.list))
+            return list(
+                self.__get_neighbors_indices_hamming(param_config)
+                for param_config in self.list
+            )
 
         # for each parameter configuration, find the neighboring parameter configurations
         if self.params_values_indices is None:
             self.__prepare_neighbors_index()
         if neighbor_method == "strictly-adjacent":
-            return np.array(
-                list(
-                    self.__get_neighbors_indices_strictlyadjacent(param_config_index, param_config)
-                    for param_config_index, param_config in enumerate(self.list)))
+            return list(
+                self.__get_neighbors_indices_strictlyadjacent(
+                    param_config_index, param_config
+                )
+                for param_config_index, param_config in enumerate(self.list)
+            )
+
         if neighbor_method == "adjacent":
-            return np.array(
-                list(self.__get_neighbors_indices_adjacent(param_config_index, param_config) for param_config_index, param_config in enumerate(self.list)))
-        raise NotImplementedError()
+            return list(
+                self.__get_neighbors_indices_adjacent(param_config_index, param_config)
+                for param_config_index, param_config in enumerate(self.list)
+            )
+
+        raise NotImplementedError(
+            f"The neighbor method {neighbor_method} is not implemented"
+        )
 
     def get_random_sample_indices(self, num_samples: int) -> np.ndarray:
         """Get the list indices for a random, non-conflicting sample"""
         if num_samples > self.size:
-            raise ValueError(f"The number of samples requested ({num_samples}) is greater than the searchspace size ({self.size})")
+            raise ValueError(
+                f"The number of samples requested ({num_samples}) is greater than the searchspace size ({self.size})"
+            )
         return np.random.choice(self.indices, size=num_samples, replace=False)
 
     def get_random_sample(self, num_samples: int) -> List[tuple]:
         """Get the parameter configurations for a random, non-conflicting sample (caution: not unique in consecutive calls)"""
-        return self.get_param_configs_at_indices(self.get_random_sample_indices(num_samples))
-
-    def get_neighbors_indices_no_cache(self, param_config: tuple, neighbor_method=None) -> List[int]:
+        return self.get_param_configs_at_indices(
+            self.get_random_sample_indices(num_samples)
+        )
+
+    def get_neighbors_indices_no_cache(
+        self, param_config: tuple, neighbor_method=None
+    ) -> List[int]:
         """Get the neighbors indices for a parameter configuration (does not check running cache, useful when mixing neighbor methods)"""
         param_config_index = self.get_param_config_index(param_config)
 
         # this is the simplest case, just return the cached value
         if self.build_neighbors_index and param_config_index is not None:
             if neighbor_method is not None and neighbor_method != self.neighbor_method:
-                raise ValueError(f"The neighbor method {neighbor_method} differs from the neighbor method {self.neighbor_method} initially used for indexing")
+                raise ValueError(
+                    f"The neighbor method {neighbor_method} differs from the neighbor method {self.neighbor_method} initially used for indexing"
+                )
             return self.neighbors_index[param_config_index]
 
         # check if there is a neighbor method to use
         if neighbor_method is None:
             if self.neighbor_method is None:
-                raise ValueError("Neither the neighbor_method argument nor self.neighbor_method was set")
+                raise ValueError(
+                    "Neither the neighbor_method argument nor self.neighbor_method was set"
+                )
             neighbor_method = self.neighbor_method
 
         if neighbor_method == "Hamming":
             return self.__get_neighbors_indices_hamming(param_config)
 
         # prepare the indices if necessary
         if self.params_values_indices is None:
             self.__prepare_neighbors_index()
 
         # if the passed param_config is fictious, we can not use the pre-calculated neighbors index
         if neighbor_method == "strictly-adjacent":
-            return self.__get_neighbors_indices_strictlyadjacent(param_config_index, param_config)
+            return self.__get_neighbors_indices_strictlyadjacent(
+                param_config_index, param_config
+            )
         if neighbor_method == "adjacent":
-            return self.__get_neighbors_indices_adjacent(param_config_index, param_config)
-        raise ValueError(f"The neighbor method {neighbor_method} is not in {supported_neighbor_methods}")
-
-    def get_neighbors_indices(self, param_config: tuple, neighbor_method=None) -> List[int]:
+            return self.__get_neighbors_indices_adjacent(
+                param_config_index, param_config
+            )
+        raise ValueError(
+            f"The neighbor method {neighbor_method} is not in {supported_neighbor_methods}"
+        )
+
+    def get_neighbors_indices(
+        self, param_config: tuple, neighbor_method=None
+    ) -> List[int]:
         """Get the neighbors indices for a parameter configuration, possibly cached"""
         neighbors = self.__neighbor_cache.get(param_config, None)
         # if there are no cached neighbors, compute them
         if neighbors is None:
-            neighbors = self.get_neighbors_indices_no_cache(param_config, neighbor_method)
+            neighbors = self.get_neighbors_indices_no_cache(
+                param_config, neighbor_method
+            )
             self.__neighbor_cache[param_config] = neighbors
         # if the neighbors were cached but the specified neighbor method was different than the one initially used to build the cache, throw an error
-        elif self.neighbor_method is not None and neighbor_method is not None and self.neighbor_method != neighbor_method:
+        elif (
+            self.neighbor_method is not None
+            and neighbor_method is not None
+            and self.neighbor_method != neighbor_method
+        ):
             raise ValueError(
                 f"The neighbor method {neighbor_method} differs from the intially set {self.neighbor_method}, can not use cached neighbors. Use 'get_neighbors_no_cache()' when mixing neighbor methods to avoid this."
             )
         return neighbors
 
     def are_neighbors_indices_cached(self, param_config: tuple) -> bool:
         """Returns true if the neighbor indices are in the cache, false otherwise"""
         return param_config in self.__neighbor_cache
 
-    def get_neighbors_no_cache(self, param_config: tuple, neighbor_method=None) -> List[tuple]:
+    def get_neighbors_no_cache(
+        self, param_config: tuple, neighbor_method=None
+    ) -> List[tuple]:
         """Get the neighbors for a parameter configuration (does not check running cache, useful when mixing neighbor methods)"""
-        return self.get_param_configs_at_indices(self.get_neighbors_indices_no_cache(param_config, neighbor_method))
+        return self.get_param_configs_at_indices(
+            self.get_neighbors_indices_no_cache(param_config, neighbor_method)
+        )
 
     def get_neighbors(self, param_config: tuple, neighbor_method=None) -> List[tuple]:
         """Get the neighbors for a parameter configuration"""
-        return self.get_param_configs_at_indices(self.get_neighbors_indices(param_config, neighbor_method))
-
-    def get_param_neighbors(self, param_config: tuple, index: int, neighbor_method: str, randomize: bool) -> list:
+        return self.get_param_configs_at_indices(
+            self.get_neighbors_indices(param_config, neighbor_method)
+        )
+
+    def get_param_neighbors(
+        self, param_config: tuple, index: int, neighbor_method: str, randomize: bool
+    ) -> list:
         """Get the neighboring parameters at an index"""
         original_value = param_config[index]
-        params = list(set(neighbor[index] for neighbor in self.get_neighbors(param_config, neighbor_method) if neighbor[index] != original_value))
+        params = list(
+            set(
+                neighbor[index]
+                for neighbor in self.get_neighbors(param_config, neighbor_method)
+                if neighbor[index] != original_value
+            )
+        )
         if randomize:
             shuffle(params)
         return params
 
-    def order_param_configs(self, param_configs: List[tuple], order: List[int], randomize_in_params=True) -> List[tuple]:
+    def order_param_configs(
+        self, param_configs: List[tuple], order: List[int], randomize_in_params=True
+    ) -> List[tuple]:
         """Order a list of parameter configurations based on the indices of the parameters given, starting at 0. If randomize_params is true, the order within parameters is shuffled."""
         if len(order) != self.num_params:
-            raise ValueError(f"The length of the order ({len(order)}) must be equal to the number of parameters ({self.num_params})")
+            raise ValueError(
+                f"The length of the order ({len(order)}) must be equal to the number of parameters ({self.num_params})"
+            )
         for i in range(self.num_params):
             if i not in order:
-                raise ValueError(f"order needs to be a list of the parameter indices, but index {i} is missing")
+                raise ValueError(
+                    f"order needs to be a list of the parameter indices, but index {i} is missing"
+                )
 
         # choose the comparison basis and add it as the first in the order
         base_comparison = choice(param_configs)
         ordered_param_configs = list([base_comparison])
 
         # move through the parameters in order, if a configuration does not match the base comparison add it to the list
         for param_index in order:
             sublist = list()
             for param_config in param_configs:
-                if param_config[param_index] != base_comparison[param_index] and param_config not in ordered_param_configs:
+                if (
+                    param_config[param_index] != base_comparison[param_index]
+                    and param_config not in ordered_param_configs
+                ):
                     ordered_param_configs.append(param_config)
             # randomize the order within the parameters
             if randomize_in_params:
                 shuffle(sublist)
             # append to the ordered list
             ordered_param_configs += sublist
```

## kernel_tuner/util.py

```diff
@@ -14,16 +14,20 @@
 
 import numpy as np
 from constraint import Constraint, AllDifferentConstraint, AllEqualConstraint, MaxSumConstraint, ExactSumConstraint, MinSumConstraint, InSetConstraint, NotInSetConstraint, SomeInSetConstraint, SomeNotInSetConstraint, FunctionConstraint
 try:
     import cupy as cp
 except ImportError:
     cp = np
+try:
+    from cuda import cuda, cudart, nvrtc
+except ImportError:
+    cuda = None
 
-from kernel_tuner.nvml import NVMLObserver
+from kernel_tuner.observers.nvml import NVMLObserver
 
 # number of special values to insert when a configuration cannot be measured
 
 
 class ErrorConfig(str):
 
     def __str__(self):
@@ -41,14 +45,27 @@
     pass
 
 
 class RuntimeFailedConfig(ErrorConfig):
     pass
 
 
+class NpEncoder(json.JSONEncoder):
+    """ Class we use for dumping Numpy objects to JSON """
+
+    def default(self, obj):
+        if isinstance(obj, np.integer):
+            return int(obj)
+        if isinstance(obj, np.floating):
+            return float(obj)
+        if isinstance(obj, np.ndarray):
+            return obj.tolist()
+        return super(NpEncoder, self).default(obj)
+
+
 class TorchPlaceHolder():
 
     def __init__(self):
         self.Tensor = Exception    #using Exception here as a type that will never be among kernel arguments
 
 
 class SkippableFailure(Exception):
@@ -717,26 +734,14 @@
 
     # actually compile
     code_object = compile(parsed_restrictions, '<string>', 'exec')
     func = FunctionType(code_object.co_consts[0], globals())
     return func
 
 
-class NpEncoder(json.JSONEncoder):
-
-    def default(self, obj):
-        if isinstance(obj, np.integer):
-            return int(obj)
-        if isinstance(obj, np.floating):
-            return float(obj)
-        if isinstance(obj, np.ndarray):
-            return obj.tolist()
-        return super(NpEncoder, self).default(obj)
-
-
 def process_cache(cache, kernel_options, tuning_options, runner):
     """cache file for storing tuned configurations
 
     the cache file is stored using JSON and uses the following format:
 
     .. code-block:: python
 
@@ -863,38 +868,28 @@
         with open(cache, "w") as fh:
             fh.write(contents[:-1] + "}\n}")
 
 
 def store_cache(key, params, tuning_options):
     """ stores a new entry (key, params) to the cachefile """
 
-    # create converter for dumping numpy objects to JSON
-    def JSONconverter(obj):
-        if isinstance(obj, np.integer):
-            return int(obj)
-        if isinstance(obj, np.floating):
-            return float(obj)
-        if isinstance(obj, np.ndarray):
-            return obj.tolist()
-        return obj.__str__()
-
-    logging.debug('store_cache called, cache=%s, cachefile=%s' % (tuning_options.cache, tuning_options.cachefile))
+    #logging.debug('store_cache called, cache=%s, cachefile=%s' % (tuning_options.cache, tuning_options.cachefile))
     if isinstance(tuning_options.cache, dict):
         if not key in tuning_options.cache:
             tuning_options.cache[key] = params
 
             # Convert ErrorConfig objects to string, wanted to do this inside the JSONconverter but couldn't get it to work
             output_params = params.copy()
             for k, v in output_params.items():
                 if isinstance(v, ErrorConfig):
                     output_params[k] = str(v)
 
             if tuning_options.cachefile:
                 with open(tuning_options.cachefile, "a") as cachefile:
-                    cachefile.write("\n" + json.dumps({ key: output_params }, default=JSONconverter)[1:-1] + ",")
+                    cachefile.write("\n" + json.dumps({ key: output_params }, cls=NpEncoder)[1:-1] + ",")
 
 
 def dump_cache(obj: str, tuning_options):
     """ dumps a string in the cache, this omits the several checks of store_cache() to speed up the process - with great power comes great responsibility! """
     if isinstance(tuning_options.cache, dict) and tuning_options.cachefile:
         with open(tuning_options.cachefile, "a") as cachefile:
             cachefile.write(obj)
@@ -938,7 +933,22 @@
                     domain = domains[variable]
                     for value in domain[:]:
                         if prod * value > maxprod:
                             domain.hideValue(value)
                     if not domain:
                         return False
         return True
+
+def cuda_error_check(error):
+    """ Checking the status of CUDA calls using the NVIDIA cuda-python backend """
+    if isinstance(error, cuda.CUresult):
+        if error != cuda.CUresult.CUDA_SUCCESS:
+            _, name = cuda.cuGetErrorName(error)
+            raise RuntimeError(f"CUDA error: {name.decode()}")
+    elif isinstance(error, cudart.cudaError_t):
+        if error != cudart.cudaError_t.cudaSuccess:
+            _, name = cudart.getErrorName(error)
+            raise RuntimeError(f"CUDART error: {name.decode()}")
+    elif isinstance(error, nvrtc.nvrtcResult):
+        if error != nvrtc.nvrtcResult.NVRTC_SUCCESS:
+            _, desc = nvrtc.nvrtcGetErrorString(error)
+            raise RuntimeError(f"NVRTC error: {desc.decode()}")
```

## kernel_tuner/runners/sequential.py

```diff
@@ -3,17 +3,18 @@
 from collections import OrderedDict
 from datetime import datetime, timezone
 from time import perf_counter
 
 from kernel_tuner.core import DeviceInterface
 from kernel_tuner.util import (ErrorConfig, print_config_output,
                                process_metrics, store_cache)
+from kernel_tuner.runners.runner import Runner
 
 
-class SequentialRunner:
+class SequentialRunner(Runner):
     """ SequentialRunner is used for tuning with a single process/thread """
 
     def __init__(self, kernel_source, kernel_options, device_options, iterations, observers):
         """ Instantiate the SequentialRunner
 
         :param kernel_source: The kernel source
         :type kernel_source: kernel_tuner.core.KernelSource
@@ -37,38 +38,38 @@
         self.quiet = device_options.quiet
         self.kernel_source = kernel_source
         self.warmed_up = False
         self.simulation_mode = False
         self.start_time = perf_counter()
         self.last_strategy_start_time = self.start_time
         self.last_strategy_time = 0
+        self.kernel_options = kernel_options
 
         #move data to the GPU
         self.gpu_args = self.dev.ready_argument_list(kernel_options.arguments)
 
-    def run(self, parameter_space, kernel_options, tuning_options):
+    def get_environment(self, tuning_options):
+        return self.dev.get_environment()
+
+    def run(self, parameter_space, tuning_options):
         """ Iterate through the entire parameter space using a single Python process
 
         :param parameter_space: The parameter space as an iterable.
         :type parameter_space: iterable
 
-        :param kernel_options: A dictionary with all options for the kernel.
-        :type kernel_options: kernel_tuner.interface.Options
-
         :param tuning_options: A dictionary with all options regarding the tuning
             process.
         :type tuning_options: kernel_tuner.iterface.Options
 
         :returns: A list of dictionaries for executed kernel configurations and their
-            execution times. And a dictionary that contains information
-            about the hardware/software environment on which the tuning took place.
-        :rtype: list(dict()), dict()
+            execution times.
+        :rtype: dict())
 
         """
-        logging.debug('sequential runner started for ' + kernel_options.kernel_name)
+        logging.debug('sequential runner started for ' + self.kernel_options.kernel_name)
 
         results = []
 
         # iterate over parameter space
         for element in parameter_space:
             params = OrderedDict(zip(tuning_options.tune_params.keys(), element))
 
@@ -79,23 +80,22 @@
             x_int = ",".join([str(i) for i in element])
             if tuning_options.cache and x_int in tuning_options.cache:
                 params.update(tuning_options.cache[x_int])
                 params['compile_time'] = 0
                 params['verification_time'] = 0
                 params['benchmark_time'] = 0
             else:
-
                 # attempt to warmup the GPU by running the first config in the parameter space and ignoring the result
                 if not self.warmed_up:
                     warmup_time = perf_counter()
-                    self.dev.compile_and_benchmark(self.kernel_source, self.gpu_args, params, kernel_options, tuning_options)
+                    self.dev.compile_and_benchmark(self.kernel_source, self.gpu_args, params, self.kernel_options, tuning_options)
                     self.warmed_up = True
                     warmup_time = 1e3 * (perf_counter() - warmup_time)
 
-                result = self.dev.compile_and_benchmark(self.kernel_source, self.gpu_args, params, kernel_options, tuning_options)
+                result = self.dev.compile_and_benchmark(self.kernel_source, self.gpu_args, params, self.kernel_options, tuning_options)
 
                 params.update(result)
 
                 # only compute metrics on configs that have not errored
                 if tuning_options.objective in result and isinstance(result[tuning_options.objective], ErrorConfig):
                     logging.debug('kernel configuration was skipped silently due to compile or runtime failure')
                 elif tuning_options.metrics:
@@ -113,8 +113,8 @@
 
             if result:
                 store_cache(x_int, params, tuning_options)
 
             # all visited configurations are added to results to provide a trace for optimization strategies
             results.append(params)
 
-        return results, self.dev.get_environment()
+        return results
```

## kernel_tuner/runners/simulation.py

```diff
@@ -1,13 +1,14 @@
 """ The simulation runner for sequentially tuning the parameter space based on cached data """
 import logging
 from collections import namedtuple
 from time import perf_counter
 
 from kernel_tuner import util
+from kernel_tuner.runners.runner import Runner
 
 _SimulationDevice = namedtuple("_SimulationDevice", ["max_threads", "env", "quiet"])
 
 
 class SimulationDevice(_SimulationDevice):
     """ Simulated device used by simulation runner """
 
@@ -21,15 +22,15 @@
         if not self.quiet:
             print("Simulating: " + value)
 
     def get_environment(self):
         return self.env
 
 
-class SimulationRunner:
+class SimulationRunner(Runner):
     """ SimulationRunner is used for tuning with a single process/thread """
 
     def __init__(self, kernel_source, kernel_options, device_options, iterations, observers):
         """ Instantiate the SimulationRunner
 
         :param kernel_source: The kernel source
         :type kernel_source: kernel_tuner.core.KernelSource
@@ -47,46 +48,43 @@
         """
 
         self.quiet = device_options.quiet
         self.dev = SimulationDevice(1024, dict(device_name="Simulation"), self.quiet)
 
         self.kernel_source = kernel_source
         self.simulation_mode = True
+        self.kernel_options = kernel_options
 
         self.start_time = perf_counter()
         self.last_strategy_start_time = self.start_time
         self.last_strategy_time = 0
         self.units = {}
 
     def get_environment(self, tuning_options):
         env = self.dev.get_environment()
         env["simulation"] = True
         env["simulated_time"] = tuning_options.simulated_time
         return env
 
-    def run(self, parameter_space, kernel_options, tuning_options):
+    def run(self, parameter_space, tuning_options):
         """ Iterate through the entire parameter space using a single Python process
 
         :param parameter_space: The parameter space as an iterable.
         :type parameter_space: iterable
 
-        :param kernel_options: A dictionary with all options for the kernel.
-        :type kernel_options: kernel_tuner.interface.Options
-
         :param tuning_options: A dictionary with all options regarding the tuning
             process.
         :type tuning_options: kernel_tuner.iterface.Options
 
         :returns: A list of dictionaries for executed kernel configurations and their
-            execution times. And a dictionary that contains information
-            about the hardware/software environment on which the tuning took place.
-        :rtype: list(dict()), dict()
+            execution times.
+        :rtype: dict()
 
         """
-        logging.debug('simulation runner started for ' + kernel_options.kernel_name)
+        logging.debug('simulation runner started for ' + self.kernel_options.kernel_name)
 
         results = []
 
         # iterate over parameter space
         for element in parameter_space:
 
             # check if element is in the cache
@@ -131,8 +129,8 @@
                 results.append(result)
                 continue
 
             # if the element is not in the cache, raise an error
             logging.debug(f"kernel configuration {element} not in cache")
             raise ValueError(f"Kernel configuration {element} not in cache - in simulation mode, all configurations must be present in the cache")
 
-        return results, self.get_environment(tuning_options)
+        return results
```

## kernel_tuner/strategies/basinhopping.py

```diff
@@ -1,50 +1,50 @@
 """ The strategy that uses the basinhopping global optimization method """
 from collections import OrderedDict
 
 import scipy.optimize
 from kernel_tuner import util
+from kernel_tuner.searchspace import Searchspace
 from kernel_tuner.strategies import common
-from kernel_tuner.strategies.common import (_cost_func, get_bounds_x0_eps,
+from kernel_tuner.strategies.common import (CostFunc,
                                             setup_method_arguments,
                                             setup_method_options)
 
 supported_methods = ["Nelder-Mead", "Powell", "CG", "BFGS", "L-BFGS-B", "TNC", "COBYLA", "SLSQP"]
 
 _options = OrderedDict(method=(f"Local optimization algorithm to use, choose any from {supported_methods}", "L-BFGS-B"),
                        T=("Temperature parameter for the accept or reject criterion", 1.0))
 
-def tune(runner, kernel_options, device_options, tuning_options):
+def tune(searchspace: Searchspace, runner, tuning_options):
 
     results = []
 
     method, T = common.get_options(tuning_options.strategy_options, _options)
 
     # scale variables in x to make 'eps' relevant for multiple variables
-    tuning_options["scaling"] = True
+    cost_func = CostFunc(searchspace, tuning_options, runner, scaling=True)
 
-    bounds, x0, eps = get_bounds_x0_eps(tuning_options, runner.dev.max_threads)
+    bounds, x0, eps = cost_func.get_bounds_x0_eps()
 
     kwargs = setup_method_arguments(method, bounds)
     options = setup_method_options(method, tuning_options)
     kwargs['options'] = options
 
-    args = (kernel_options, tuning_options, runner, results)
 
     minimizer_kwargs = dict(**kwargs)
     minimizer_kwargs["method"] = method
-    minimizer_kwargs["args"] = args
 
     opt_result = None
     try:
-        opt_result = scipy.optimize.basinhopping(_cost_func, x0, T=T, stepsize=eps,
+        opt_result = scipy.optimize.basinhopping(cost_func, x0, T=T, stepsize=eps,
                                              minimizer_kwargs=minimizer_kwargs, disp=tuning_options.verbose)
     except util.StopCriterionReached as e:
         if tuning_options.verbose:
             print(e)
 
     if opt_result and tuning_options.verbose:
         print(opt_result.message)
 
-    return results, runner.dev.get_environment()
+    return cost_func.results
+
 
 tune.__doc__ = common.get_strategy_docstring("basin hopping", _options)
```

## kernel_tuner/strategies/bayes_opt.py

```diff
@@ -6,14 +6,17 @@
 from random import randint, shuffle
 from typing import Tuple
 
 import numpy as np
 from scipy.stats import norm
 
 # BO imports
+from kernel_tuner.searchspace import Searchspace
+from kernel_tuner.strategies.common import CostFunc
+
 try:
     from sklearn.exceptions import ConvergenceWarning
     from sklearn.gaussian_process import GaussianProcessRegressor
     from sklearn.gaussian_process.kernels import RBF, ConstantKernel, Matern
     from skopt.sampler import Lhs
     bayes_opt_present = True
 except ImportError:
@@ -62,53 +65,45 @@
             removed_tune_params.append(normalized)
     if 'verbose' in tuning_options and tuning_options.verbose is True and len(tune_params.keys()) != sum(pruned_tune_params_mask):
         print(f"Number of parameters (dimensions): {len(tune_params.keys())}, after pruning: {sum(pruned_tune_params_mask)}")
     parameter_space = list(tuple(itertools.compress(param_config, pruned_tune_params_mask)) for param_config in parameter_space)
     return parameter_space, removed_tune_params
 
 
-def tune(runner, kernel_options, device_options, tuning_options):
+def tune(searchspace: Searchspace, runner, tuning_options):
     """ Find the best performing kernel configuration in the parameter space
 
     :params runner: A runner from kernel_tuner.runners
     :type runner: kernel_tuner.runner
 
-    :param kernel_options: A dictionary with all options for the kernel.
-    :type kernel_options: kernel_tuner.interface.Options
-
-    :param device_options: A dictionary with all options for the device
-        on which the kernel should be tuned.
-    :type device_options: kernel_tuner.interface.Options
-
     :param tuning_options: A dictionary with all options regarding the tuning
         process. Allows setting hyperparameters via the strategy_options key.
     :type tuning_options: kernel_tuner.interface.Options
 
     :returns: A list of dictionaries for executed kernel configurations and their
-        execution times. And a dictionary that contains a information
-        about the hardware/software environment on which the tuning took place.
+        execution times.
     :rtype: list(dict()), dict()
 
     """
 
     max_fevals = tuning_options.strategy_options.get("max_fevals", 100)
     prune_parameterspace = tuning_options.strategy_options.get("pruneparameterspace", True)
     if not bayes_opt_present:
         raise ImportError("Error: optional dependencies for Bayesian Optimization not installed, please install scikit-learn and scikit-optimize")
 
     # epsilon for scaling should be the evenly spaced distance between the largest set of parameter options in an interval [0,1]
-    tune_params = tuning_options.tune_params
-    tuning_options["scaling"] = True
-    _, _, eps = common.get_bounds_x0_eps(tuning_options, runner.dev.max_threads)
+    tune_params = searchspace.tune_params
+    cost_func = CostFunc(searchspace, tuning_options, runner, scaling=True)
+    _, _, eps = cost_func.get_bounds_x0_eps()
 
     # compute cartesian product of all tunable parameters
     parameter_space = itertools.product(*tune_params.values())
 
     # check for search space restrictions
-    if tuning_options.restrictions is not None:
+    if searchspace.restrictions is not None:
         tuning_options.verbose = False
     parameter_space = filter(lambda p: util.config_valid(p, tuning_options, runner.dev.max_threads), parameter_space)
     parameter_space = list(parameter_space)
     if len(parameter_space) < 1:
         raise ValueError("Empty parameterspace after restrictionscheck. Restrictionscheck is possibly too strict.")
     if len(parameter_space) == 1:
         raise ValueError(f"Only one configuration after restrictionscheck. Restrictionscheck is possibly too strict. Configuration: {parameter_space[0]}")
@@ -122,39 +117,39 @@
         parameter_space, removed_tune_params = prune_parameter_space(parameter_space, tuning_options, tune_params, normalize_dict)
     else:
         parameter_space = list(parameter_space)
         removed_tune_params = [None] * len(tune_params.keys())
 
     # initialize and optimize
     try:
-        bo = BayesianOptimization(parameter_space, removed_tune_params, kernel_options, tuning_options, normalize_dict, denormalize_dict, runner)
+        bo = BayesianOptimization(parameter_space, removed_tune_params, tuning_options, normalize_dict, denormalize_dict, cost_func)
     except util.StopCriterionReached as e:
         print(f"Stop criterion reached during initialization, was popsize (default 20) greater than max_fevals or the alotted time?")
         raise e
     try:
         if max_fevals - bo.fevals <= 0:
             raise ValueError(f"No function evaluations left for optimization after sampling")
         bo.optimize(max_fevals)
     except util.StopCriterionReached as e:
         if tuning_options.verbose:
             print(e)
 
-    return bo.results, runner.dev.get_environment()
+    return cost_func.results
 
 # _options dict is used for generating documentation, but is not used to check for unsupported strategy_options in bayes_opt
 _options = dict(covariancekernel=('The Covariance kernel to use, choose any from "constantrbf", "rbf", "matern32", "matern52"', "matern32"),
                 covariancelengthscale=("The covariance length scale", 1.5),
                 method=("The Bayesian Optimization method to use, choose any from " + ", ".join(supported_methods), "multi-advanced"),
                 samplingmethod=("Method used for initial sampling the parameter space, either random or lhs", "lhs"),
                 popsize=("Number of initial samples", 20))
 
-class BayesianOptimization():
 
-    def __init__(self, searchspace: list, removed_tune_params: list, kernel_options: dict, tuning_options: dict, normalize_dict: dict, denormalize_dict: dict,
-                 runner, opt_direction='min'):
+class BayesianOptimization():
+    def __init__(self, searchspace: list, removed_tune_params: list, tuning_options: dict, normalize_dict: dict, denormalize_dict: dict,
+                 cost_func: CostFunc, opt_direction='min'):
         time_start = time.perf_counter_ns()
 
         # supported hyperparameter values
         self.supported_cov_kernels = ["constantrbf", "rbf", "matern32", "matern52"]
         self.supported_methods = supported_methods
         self.supported_sampling_methods = ["random", "lhs"]
         self.supported_sampling_criterion = ["correlation", "ratio", "maximin", None]
@@ -186,22 +181,21 @@
             acq_params['explorationfactor'] = 'CV'
         if 'zeta' not in acq_params:
             acq_params['zeta'] = 1
         if 'skip_duplicate_after' not in acq_params:
             acq_params['skip_duplicate_after'] = 5
 
         # set arguments
-        self.kernel_options = kernel_options
         self.tuning_options = tuning_options
         self.tune_params = tuning_options.tune_params
         self.param_names = list(self.tune_params.keys())
         self.normalized_dict = normalize_dict
         self.denormalized_dict = denormalize_dict
-        self.runner = runner
-        self.max_threads = runner.dev.max_threads
+        self.cost_func = cost_func
+        self.max_threads = cost_func.runner.dev.max_threads
         self.log_timings = False
 
         # set optimization constants
         self.invalid_value = 1e20
         self.opt_direction = opt_direction
         if opt_direction == 'min':
             self.worst_value = np.PINF
@@ -217,15 +211,14 @@
         self.af_name = acquisition_function
         self.af_params = acq_params
         self.multi_afs = list(self.get_af_by_name(af_name) for af_name in multi_af_names)
         self.set_acquisition_function(acquisition_function)
         self.set_surrogate_model(cov_kernel_name, cov_kernel_lengthscale)
 
         # set remaining values
-        self.results = []
         self.__searchspace = searchspace
         self.removed_tune_params = removed_tune_params
         self.searchspace_size = len(self.searchspace)
         self.num_dimensions = len(self.dimensions())
         self.__current_optimum = self.worst_value
         self.cv_norm_maximum = None
         self.fevals = 0
@@ -404,15 +397,15 @@
 
     def evaluate_objective_function(self, param_config: tuple) -> float:
         """ Evaluates the objective function """
         param_config = self.unprune_param_config(param_config)
         denormalized_param_config = self.denormalize_param_config(param_config)
         if not util.config_valid(denormalized_param_config, self.tuning_options, self.max_threads):
             return self.invalid_value
-        val = common._cost_func(param_config, self.kernel_options, self.tuning_options, self.runner, self.results)
+        val = self.cost_func(param_config)
         self.fevals += 1
         return val
 
     def dimensions(self) -> list:
         """ List of parameter values per parameter """
         return self.tune_params.values()
 
@@ -511,15 +504,14 @@
             # afterwards select the best AF value
             best_af = self.argopt(list_of_acquisition_values)
             candidate_params = self.unvisited_cache[best_af]
             candidate_index = self.find_param_config_index(candidate_params)
             observation = self.evaluate_objective_function(candidate_params)
             self.update_after_evaluation(observation, candidate_index, candidate_params)
             self.fit_observations_to_model()
-        return self.results
 
     def __optimize_multi(self, max_fevals):
         """ Optimize with a portfolio of multiple acquisition functions. Predictions are always only taken once. Skips AFs if they suggest X/max_evals duplicates in a row, prefers AF with best discounted average. """
         if self.opt_direction != 'min':
             raise ValueError(f"Optimization direction must be minimization ('min'), is {self.opt_direction}")
         # calculate how many times an AF can suggest a duplicate candidate before the AF is skipped
         # skip_duplicates_fraction = self.af_params['skip_duplicates_fraction']
@@ -622,15 +614,14 @@
                 time_taken_afs = round(time_afs - time_predictions, 3) / 1000
                 time_taken_eval = round(time_eval - time_afs, 3) / 1000
                 time_taken_af_selection = round(time_af_selection - time_eval, 3) / 1000
                 time_taken_total = round(time_af_selection - time_start, 3) / 1000
                 print(
                     f"({self.fevals}/{max_fevals}) Total time: {time_taken_total} | Predictions: {time_taken_predictions} | AFs: {time_taken_afs} | Eval: {time_taken_eval} | AF selection: {time_taken_af_selection}",
                     flush=True)
-        return self.results
 
     def __optimize_multi_advanced(self, max_fevals, increase_precision=False):
         """ Optimize with a portfolio of multiple acquisition functions. Predictions are only taken once, unless increase_precision is true. Skips AFs if they are consistently worse than the mean of discounted observations, promotes AFs if they are consistently better than this mean. """
         if self.opt_direction != 'min':
             raise ValueError(f"Optimization direction must be minimization ('min'), is {self.opt_direction}")
         aqfs = self.multi_afs
         discount_factor = self.multi_afs_discount_factor
@@ -723,16 +714,14 @@
                                                                                       or discounted_obs[af_index] < discounted_obs[af_index_best]):
                             af_index_best = af_index
                 # make the best AF single
                 if af_index_best > -1:
                     single_af = True
                     self.__af = aqfs[af_index_best]
 
-        return self.results
-
     def __optimize_multi_fast(self, max_fevals):
         """ Optimize with a portfolio of multiple acquisition functions. Predictions are only taken once. """
         while self.fevals < max_fevals:
             aqfs = self.multi_afs
             # if we take the prediction only once, we want to go from most exploiting to most exploring, because the more exploiting an AF is, the more it relies on non-stale information from the model
             predictions, _, std = self.predict_list(self.unvisited_cache)
             hyperparam = self.contextual_variance(std)
@@ -745,15 +734,14 @@
                 best_af = self.argopt(list_of_acquisition_values)
                 del predictions[best_af]    # to avoid going out of bounds
                 candidate_params = self.unvisited_cache[best_af]
                 candidate_index = self.find_param_config_index(candidate_params)
                 observation = self.evaluate_objective_function(candidate_params)
                 self.update_after_evaluation(observation, candidate_index, candidate_params)
             self.fit_observations_to_model()
-        return self.results
 
     def af_random(self, predictions=None, hyperparam=None) -> list:
         """ Acquisition function returning a randomly shuffled list for comparison """
         list_random = range(len(self.unvisited_cache))
         shuffle(list_random)
         return list_random
 
@@ -838,15 +826,15 @@
         """ Visualize the model after the optimization """
         print(self.__model.kernel_.get_params())
         print(self.__model.log_marginal_likelihood())
         import matplotlib.pyplot as plt
         _, mu, std = self.predict_list(self.searchspace)
         brute_force_observations = list()
         for param_config in self.searchspace:
-            obs = common._cost_func(param_config, self.kernel_options, self.tuning_options, self.runner, self.results)
+            obs = self.cost_func(param_config)
             if obs == self.invalid_value:
                 obs = None
             brute_force_observations.append(obs)
         x_axis = range(len(mu))
         plt.fill_between(x_axis, mu - std, mu + std, alpha=0.2, antialiased=True)
         plt.plot(x_axis, mu, label="predictions", linestyle=' ', marker='.')
         plt.plot(x_axis, brute_force_observations, label="actual", linestyle=' ', marker='.')
```

## kernel_tuner/strategies/brute_force.py

```diff
@@ -1,18 +1,13 @@
 """ The default strategy that iterates through the whole parameter space """
 from kernel_tuner.searchspace import Searchspace
 from kernel_tuner.strategies import common
 
 _options = {}
 
-def tune(runner, kernel_options, device_options, tuning_options):
-
-    # create the searchspace
-    searchspace = Searchspace(tuning_options, runner.dev.max_threads, sort=True)
+def tune(searchspace: Searchspace, runner, tuning_options):
 
     # call the runner
-    results, env = runner.run(searchspace.list, kernel_options, tuning_options)
-
-    return results, env
+    return runner.run(searchspace.sorted_list(), tuning_options)
 
 
 tune.__doc__ = common.get_strategy_docstring("Brute Force", _options)
```

## kernel_tuner/strategies/common.py

```diff
@@ -12,21 +12,14 @@
     This $NAME$ strategy supports the following strategy_options:
 
 $STRAT_OPT$
 
     :params runner: A runner from kernel_tuner.runners
     :type runner: kernel_tuner.runner
 
-    :param kernel_options: A dictionary with all options for the kernel.
-    :type kernel_options: kernel_tuner.interface.Options
-
-    :param device_options: A dictionary with all options for the device
-        on which the kernel should be tuned.
-    :type device_options: kernel_tuner.interface.Options
-
     :param tuning_options: A dictionary with all options regarding the tuning
         process.
     :type tuning_options: kernel_tuner.interface.Options
 
     :returns: A list of dictionaries for executed kernel configurations and their
         execution times. And a dictionary that contains information
         about the hardware/software environment on which the tuning took place.
@@ -55,114 +48,122 @@
     for key in strategy_options:
         if key not in accepted:
             raise ValueError(f"Unrecognized option {key} in strategy_options")
     assert isinstance(options, OrderedDict)
     return [strategy_options.get(opt, default) for opt, (_, default) in options.items()]
 
 
-def _cost_func(x, kernel_options, tuning_options, runner, results, check_restrictions=True):
-    """ Cost function used by almost all strategies """
-    runner.last_strategy_time = 1000 * (perf_counter() - runner.last_strategy_start_time)
-
-    # error value to return for numeric optimizers that need a numerical value
-    logging.debug('_cost_func called')
-    logging.debug('x: ' + str(x))
-
-    # check if max_fevals is reached or time limit is exceeded
-    util.check_stop_criterion(tuning_options)
-
-    # snap values in x to nearest actual value for each parameter unscale x if needed
-    if tuning_options.snap:
-        if tuning_options.scaling:
-            params = unscale_and_snap_to_nearest(x, tuning_options.tune_params, tuning_options.eps)
+class CostFunc:
+    def __init__(self, searchspace: Searchspace, tuning_options, runner, *, scaling=False, snap=True):
+        self.runner = runner
+        self.tuning_options = tuning_options
+        self.snap = snap
+        self.scaling = scaling
+        self.searchspace = searchspace
+        self.results = []
+
+    def __call__(self, x, check_restrictions=True):
+        """ Cost function used by almost all strategies """
+        self.runner.last_strategy_time = 1000 * (perf_counter() - self.runner.last_strategy_start_time)
+
+        # error value to return for numeric optimizers that need a numerical value
+        logging.debug('_cost_func called')
+        logging.debug('x: ' + str(x))
+
+        # check if max_fevals is reached or time limit is exceeded
+        util.check_stop_criterion(self.tuning_options)
+
+        # snap values in x to nearest actual value for each parameter unscale x if needed
+        if self.snap:
+            if self.scaling:
+                params = unscale_and_snap_to_nearest(x, self.searchspace.tune_params, self.tuning_options.eps)
+            else:
+                params = snap_to_nearest_config(x, self.searchspace.tune_params)
         else:
-            params = snap_to_nearest_config(x, tuning_options.tune_params)
-    else:
-        params = x
-    logging.debug('params ' + str(params))
-
-    legal = True
-    result = {}
-    x_int = ",".join([str(i) for i in params])
-
-    # else check if this is a legal (non-restricted) configuration
-    if check_restrictions and tuning_options.restrictions:
-        params_dict = OrderedDict(zip(tuning_options.tune_params.keys(), params))
-        legal = util.check_restrictions(tuning_options.restrictions, params_dict, tuning_options.verbose)
-        if not legal:
-            result = params_dict
-            result[tuning_options.objective] = util.InvalidConfig()
-
-    # compile and benchmark this instance
-    if not result:
-        res, _ = runner.run([params], kernel_options, tuning_options)
-        result = res[0]
-
-    # append to tuning results
-    if x_int not in tuning_options.unique_results:
-        tuning_options.unique_results[x_int] = result
-
-    results.append(result)
-
-    # get numerical return value, taking optimization direction into account
-    return_value = result[tuning_options.objective] or sys.float_info.max
-    return_value = return_value if not tuning_options.objective_higher_is_better else -return_value
-
-    # upon returning from this function control will be given back to the strategy, so reset the start time
-    runner.last_strategy_start_time = perf_counter()
-    return return_value
-
-
-def get_bounds_x0_eps(tuning_options, max_threads):
-    """compute bounds, x0 (the initial guess), and eps"""
-    values = list(tuning_options.tune_params.values())
-
-    if "x0" in tuning_options.strategy_options:
-        x0 = tuning_options.strategy_options.x0
-    else:
-        x0 = None
-
-    if tuning_options.scaling:
-        eps = np.amin([1.0 / len(v) for v in values])
-
-        # reducing interval from [0, 1] to [0, eps*len(v)]
-        bounds = [(0, eps * len(v)) for v in values]
-        if x0:
-            # x0 has been supplied by the user, map x0 into [0, eps*len(v)]
-            x0 = scale_from_params(x0, tuning_options, eps)
+            params = x
+        logging.debug('params ' + str(params))
+
+        legal = True
+        result = {}
+        x_int = ",".join([str(i) for i in params])
+
+        # else check if this is a legal (non-restricted) configuration
+        if check_restrictions and self.searchspace.restrictions:
+            params_dict = OrderedDict(zip(self.searchspace.tune_params.keys(), params))
+            legal = util.check_restrictions(self.searchspace.restrictions, params_dict, self.tuning_options.verbose)
+            if not legal:
+                result = params_dict
+                result[self.tuning_options.objective] = util.InvalidConfig()
+
+        if legal:
+            # compile and benchmark this instance
+            res = self.runner.run([params], self.tuning_options)
+            result = res[0]
+
+            # append to tuning results
+            if x_int not in self.tuning_options.unique_results:
+                self.tuning_options.unique_results[x_int] = result
+
+            self.results.append(result)
+
+            # upon returning from this function control will be given back to the strategy, so reset the start time
+            self.runner.last_strategy_start_time = perf_counter()
+
+        # get numerical return value, taking optimization direction into account
+        return_value = result[self.tuning_options.objective] or sys.float_info.max
+        return_value = return_value if not self.tuning_options.objective_higher_is_better else -return_value
+
+        return return_value
+
+    def get_bounds_x0_eps(self):
+        """compute bounds, x0 (the initial guess), and eps"""
+        values = list(self.searchspace.tune_params.values())
+
+        if "x0" in self.tuning_options.strategy_options:
+            x0 = self.tuning_options.strategy_options.x0
+        else:
+            x0 = None
+
+        if self.scaling:
+            eps = np.amin([1.0 / len(v) for v in values])
+
+            # reducing interval from [0, 1] to [0, eps*len(v)]
+            bounds = [(0, eps * len(v)) for v in values]
+            if x0:
+                # x0 has been supplied by the user, map x0 into [0, eps*len(v)]
+                x0 = scale_from_params(x0, self.tuning_options, eps)
+            else:
+                # get a valid x0
+                pos = list(self.searchspace.get_random_sample(1)[0])
+                x0 = scale_from_params(pos, self.searchspace.tune_params, eps)
         else:
-            # get a valid x0
-            searchspace = Searchspace(tuning_options, max_threads)
-            pos = list(searchspace.get_random_sample(1)[0])
-            x0 = scale_from_params(pos, tuning_options.tune_params, eps)
-    else:
-        bounds = get_bounds(tuning_options.tune_params)
-        if not x0:
-            x0 = [(min_v + max_v) / 2.0 for (min_v, max_v) in bounds]
-        eps = 1e9
-        for v_list in values:
-            vals = np.sort(v_list)
-            eps = min(eps, np.amin(np.gradient(vals)))
-
-    tuning_options["eps"] = eps
-    logging.debug('get_bounds_x0_eps called')
-    logging.debug('bounds ' + str(bounds))
-    logging.debug('x0 ' + str(x0))
-    logging.debug('eps ' + str(eps))
-
-    return bounds, x0, eps
-
-
-def get_bounds(tune_params):
-    """ create a bounds array from the tunable parameters """
-    bounds = []
-    for values in tune_params.values():
-        sorted_values = np.sort(values)
-        bounds.append((sorted_values[0], sorted_values[-1]))
-    return bounds
+            bounds = self.get_bounds()
+            if not x0:
+                x0 = [(min_v + max_v) / 2.0 for (min_v, max_v) in bounds]
+            eps = 1e9
+            for v_list in values:
+                if len(v_list) > 1:
+                    vals = np.sort(v_list)
+                    eps = min(eps, np.amin(np.gradient(vals)))
+
+        self.tuning_options["eps"] = eps
+        logging.debug('get_bounds_x0_eps called')
+        logging.debug('bounds ' + str(bounds))
+        logging.debug('x0 ' + str(x0))
+        logging.debug('eps ' + str(eps))
+
+        return bounds, x0, eps
+
+    def get_bounds(self):
+        """ create a bounds array from the tunable parameters """
+        bounds = []
+        for values in self.searchspace.tune_params.values():
+            sorted_values = np.sort(values)
+            bounds.append((sorted_values[0], sorted_values[-1]))
+        return bounds
 
 
 def setup_method_arguments(method, bounds):
     """ prepare method specific arguments """
     kwargs = {}
     # pass bounds to methods that support it
     if method in ["L-BFGS-B", "TNC", "SLSQP"]:
```

## kernel_tuner/strategies/diff_evo.py

```diff
@@ -1,47 +1,45 @@
 """ The differential evolution strategy that optimizes the search through the parameter space """
 from collections import OrderedDict
 
 from kernel_tuner import util
 from kernel_tuner.searchspace import Searchspace
 from kernel_tuner.strategies import common
-from kernel_tuner.strategies.common import _cost_func, get_bounds
+from kernel_tuner.strategies.common import CostFunc
 from scipy.optimize import differential_evolution
 
 supported_methods = ["best1bin", "best1exp", "rand1exp", "randtobest1exp", "best2exp", "rand2exp", "randtobest1bin", "best2bin", "rand2bin", "rand1bin"]
 
 _options = OrderedDict(method=(f"Creation method for new population, any of {supported_methods}", "best1bin"),
                        popsize=("Population size", 20),
                        maxiter=("Number of generations", 100))
 
 
-def tune(runner, kernel_options, device_options, tuning_options):
+def tune(searchspace: Searchspace, runner, tuning_options):
 
     results = []
 
     method, popsize, maxiter = common.get_options(tuning_options.strategy_options, _options)
 
-    tuning_options["scaling"] = False
     # build a bounds array as needed for the optimizer
-    bounds = get_bounds(tuning_options.tune_params)
-
-    args = (kernel_options, tuning_options, runner, results)
+    cost_func = CostFunc(searchspace, tuning_options, runner)
+    bounds = cost_func.get_bounds()
 
     # ensure particles start from legal points
-    searchspace = Searchspace(tuning_options, runner.dev.max_threads)
     population = list(list(p) for p in searchspace.get_random_sample(popsize))
 
     # call the differential evolution optimizer
     opt_result = None
     try:
-        opt_result = differential_evolution(_cost_func, bounds, args, maxiter=maxiter, popsize=popsize, init=population, polish=False, strategy=method, disp=tuning_options.verbose)
+        opt_result = differential_evolution(cost_func, bounds, maxiter=maxiter, popsize=popsize, init=population,
+                                        polish=False, strategy=method, disp=tuning_options.verbose)
     except util.StopCriterionReached as e:
         if tuning_options.verbose:
             print(e)
 
     if opt_result and tuning_options.verbose:
         print(opt_result.message)
 
-    return results, runner.dev.get_environment()
+    return cost_func.results
 
 
 tune.__doc__ = common.get_strategy_docstring("Differential Evolution", _options)
```

## kernel_tuner/strategies/dual_annealing.py

```diff
@@ -1,48 +1,46 @@
 """ The strategy that uses the dual annealing optimization method """
 from collections import OrderedDict
 
 import scipy.optimize
 from kernel_tuner import util
+from kernel_tuner.searchspace import Searchspace
 from kernel_tuner.strategies import common
-from kernel_tuner.strategies.common import (_cost_func, get_bounds_x0_eps,
+from kernel_tuner.strategies.common import (CostFunc,
                                             setup_method_arguments,
                                             setup_method_options)
 
 supported_methods = ['COBYLA', 'L-BFGS-B', 'SLSQP', 'CG', 'Powell', 'Nelder-Mead', 'BFGS', 'trust-constr']
 
 _options = OrderedDict(method=(f"Local optimization method to use, choose any from {supported_methods}", "Powell"))
 
-def tune(runner, kernel_options, device_options, tuning_options):
-
-    results = []
+def tune(searchspace: Searchspace, runner, tuning_options):
 
     method = common.get_options(tuning_options.strategy_options, _options)[0]
 
     #scale variables in x to make 'eps' relevant for multiple variables
-    tuning_options["scaling"] = True
+    cost_func = CostFunc(searchspace, tuning_options, runner, scaling=True)
 
-    bounds, x0, _ = get_bounds_x0_eps(tuning_options, runner.dev.max_threads)
+    bounds, x0, _ = cost_func.get_bounds_x0_eps()
 
     kwargs = setup_method_arguments(method, bounds)
     options = setup_method_options(method, tuning_options)
     kwargs['options'] = options
 
-    args = (kernel_options, tuning_options, runner, results)
 
     minimizer_kwargs = {}
     minimizer_kwargs["method"] = method
 
     opt_result = None
     try:
-        opt_result = scipy.optimize.dual_annealing(_cost_func, bounds, args=args, minimizer_kwargs=minimizer_kwargs, x0=x0)
+        opt_result = scipy.optimize.dual_annealing(cost_func, bounds, minimizer_kwargs=minimizer_kwargs, x0=x0)
     except util.StopCriterionReached as e:
         if tuning_options.verbose:
             print(e)
 
     if opt_result and tuning_options.verbose:
         print(opt_result.message)
 
-    return results, runner.dev.get_environment()
+    return cost_func.results
 
 
 tune.__doc__ = common.get_strategy_docstring("Dual Annealing", _options)
```

## kernel_tuner/strategies/firefly_algorithm.py

```diff
@@ -2,60 +2,54 @@
 import sys
 from collections import OrderedDict
 
 import numpy as np
 from kernel_tuner import util
 from kernel_tuner.searchspace import Searchspace
 from kernel_tuner.strategies import common
-from kernel_tuner.strategies.common import (_cost_func, get_bounds_x0_eps,
-                                            scale_from_params)
+from kernel_tuner.strategies.common import (CostFunc, scale_from_params)
 from kernel_tuner.strategies.pso import Particle
 
 _options = OrderedDict(popsize=("Population size", 20),
                        maxiter=("Maximum number of iterations", 100),
                        B0=("Maximum attractiveness", 1.0),
                        gamma=("Light absorption coefficient", 1.0),
                        alpha=("Randomization parameter", 0.2))
 
-def tune(runner, kernel_options, device_options, tuning_options):
-
-    results = []
+def tune(searchspace: Searchspace, runner, tuning_options):
 
     # scale variables in x because PSO works with velocities to visit different configurations
-    tuning_options["scaling"] = True
+    cost_func = CostFunc(searchspace, tuning_options, runner, scaling=True)
 
     # using this instead of get_bounds because scaling is used
-    bounds, _, eps = get_bounds_x0_eps(tuning_options, runner.dev.max_threads)
-
-    args = (kernel_options, tuning_options, runner, results)
+    bounds, _, eps = cost_func.get_bounds_x0_eps()
 
     num_particles, maxiter, B0, gamma, alpha = common.get_options(tuning_options.strategy_options, _options)
 
     best_score_global = sys.float_info.max
     best_position_global = []
 
     # init particle swarm
     swarm = []
     for i in range(0, num_particles):
-        swarm.append(Firefly(bounds, args))
+        swarm.append(Firefly(bounds))
 
     # ensure particles start from legal points
-    searchspace = Searchspace(tuning_options, runner.dev.max_threads)
     population = list(list(p) for p in searchspace.get_random_sample(num_particles))
     for i, particle in enumerate(swarm):
-        particle.position = scale_from_params(population[i], tuning_options.tune_params, eps)
+        particle.position = scale_from_params(population[i], searchspace.tune_params, eps)
 
     # compute initial intensities
     for j in range(num_particles):
         try:
-            swarm[j].compute_intensity(_cost_func)
+            swarm[j].compute_intensity(cost_func)
         except util.StopCriterionReached as e:
             if tuning_options.verbose:
                 print(e)
-            return results, runner.dev.get_environment()
+            return cost_func.results
         if swarm[j].score <= best_score_global:
             best_position_global = swarm[j].position
             best_score_global = swarm[j].score
 
     for c in range(maxiter):
         if tuning_options.verbose:
             print("start iteration ", c, "best score global", best_score_global)
@@ -66,53 +60,53 @@
 
                 if swarm[i].intensity < swarm[j].intensity:
                     dist = swarm[i].distance_to(swarm[j])
                     beta = B0 * np.exp(-gamma * dist * dist)
 
                     swarm[i].move_towards(swarm[j], beta, alpha)
                     try:
-                        swarm[i].compute_intensity(_cost_func)
+                        swarm[i].compute_intensity(cost_func)
                     except util.StopCriterionReached as e:
                         if tuning_options.verbose:
                             print(e)
-                        return results, runner.dev.get_environment()
+                        return cost_func.results
 
                     # update global best if needed, actually only used for printing
                     if swarm[i].score <= best_score_global:
                         best_position_global = swarm[i].position
                         best_score_global = swarm[i].score
 
         swarm.sort(key=lambda x: x.score)
 
     if tuning_options.verbose:
         print('Final result:')
         print(best_position_global)
         print(best_score_global)
 
-    return results, runner.dev.get_environment()
+    return cost_func.results
 
 
 tune.__doc__ = common.get_strategy_docstring("firefly algorithm", _options)
 
 class Firefly(Particle):
     """Firefly object for use in the Firefly Algorithm"""
 
-    def __init__(self, bounds, args):
+    def __init__(self, bounds):
         """Create Firefly at random position within bounds"""
-        super().__init__(bounds, args)
+        super().__init__(bounds)
         self.bounds = bounds
         self.intensity = 1 / self.score
 
     def distance_to(self, other):
         """Return Euclidian distance between self and other Firefly"""
         return np.linalg.norm(self.position-other.position)
 
-    def compute_intensity(self, _cost_func):
+    def compute_intensity(self, fun):
         """Evaluate cost function and compute intensity at this position"""
-        self.evaluate(_cost_func)
+        self.evaluate(fun)
         if self.score == sys.float_info.max:
             self.intensity = -sys.float_info.max
         else:
             self.intensity = 1 / self.score
 
     def move_towards(self, other, beta, alpha):
         """Move firefly towards another given beta and alpha values"""
```

## kernel_tuner/strategies/genetic_algorithm.py

```diff
@@ -2,58 +2,55 @@
 import random
 from collections import OrderedDict
 
 import numpy as np
 from kernel_tuner import util
 from kernel_tuner.searchspace import Searchspace
 from kernel_tuner.strategies import common
-from kernel_tuner.strategies.common import _cost_func
+from kernel_tuner.strategies.common import CostFunc
 
 _options = OrderedDict(
     popsize=("population size", 20),
     maxiter=("maximum number of generations", 100),
     method=("crossover method to use, choose any from single_point, two_point, uniform, disruptive_uniform", "uniform"),
     mutation_chance=("chance to mutate is 1 in mutation_chance", 10),
 )
 
 
-def tune(runner, kernel_options, device_options, tuning_options):
+def tune(searchspace: Searchspace, runner, tuning_options):
 
     options = tuning_options.strategy_options
     pop_size, generations, method, mutation_chance = common.get_options(options, _options)
     crossover = supported_methods[method]
 
-    tuning_options["scaling"] = False
-
     best_score = 1e20
-    results = []
+    cost_func = CostFunc(searchspace, tuning_options, runner)
 
-    searchspace = Searchspace(tuning_options, runner.dev.max_threads)
     population = list(list(p) for p in searchspace.get_random_sample(pop_size))
 
     for generation in range(generations):
 
         # determine fitness of population members
         weighted_population = []
         for dna in population:
             try:
-                time = _cost_func(dna, kernel_options, tuning_options, runner, results, check_restrictions=False)
+                time = cost_func(dna, check_restrictions=False)
             except util.StopCriterionReached as e:
                 if tuning_options.verbose:
                     print(e)
-                return results, runner.dev.get_environment()
+                return cost_func.results
 
             weighted_population.append((dna, time))
 
         # population is sorted such that better configs have higher chance of reproducing
         weighted_population.sort(key=lambda x: x[1])
 
         # 'best_score' is used only for printing
-        if tuning_options.verbose and results:
-            best_score = util.get_best_config(results, tuning_options.objective, tuning_options.objective_higher_is_better)[tuning_options.objective]
+        if tuning_options.verbose and cost_func.results:
+            best_score = util.get_best_config(cost_func.results, tuning_options.objective, tuning_options.objective_higher_is_better)[tuning_options.objective]
 
         if tuning_options.verbose:
             print("Generation %d, best_score %f" % (generation, best_score))
 
         population = []
 
         # crossover and mutate
@@ -69,15 +66,15 @@
                     population.append(child)
 
                 if len(population) >= pop_size:
                     break
 
         # could combine old + new generation here and do a selection
 
-    return results, runner.dev.get_environment()
+    return cost_func.results
 
 
 tune.__doc__ = common.get_strategy_docstring("Genetic Algorithm", _options)
 
 
 def weighted_choice(population, n):
     """Randomly select n unique individuals from a weighted population, fitness determines probability of being selected"""
```

## kernel_tuner/strategies/greedy_ils.py

```diff
@@ -1,68 +1,65 @@
 """ A simple greedy iterative local search algorithm for parameter search """
 from collections import OrderedDict
 
 from kernel_tuner import util
 from kernel_tuner.searchspace import Searchspace
 from kernel_tuner.strategies import common
-from kernel_tuner.strategies.common import _cost_func
+from kernel_tuner.strategies.common import CostFunc
 from kernel_tuner.strategies.genetic_algorithm import mutate
 from kernel_tuner.strategies.hillclimbers import base_hillclimb
 
 _options = OrderedDict(neighbor=("Method for selecting neighboring nodes, choose from Hamming or adjacent", "Hamming"),
                        restart=("controls greedyness, i.e. whether to restart from a position as soon as an improvement is found", True),
                        no_improvement=("number of evaluations to exceed without improvement before restarting", 50),
                        random_walk=("controls greedyness, i.e. whether to restart from a position as soon as an improvement is found", 0.3))
 
-def tune(runner, kernel_options, device_options, tuning_options):
+def tune(searchspace: Searchspace, runner, tuning_options):
 
-    dna_size = len(tuning_options.tune_params.keys())
+    dna_size = len(searchspace.tune_params.keys())
 
     options = tuning_options.strategy_options
 
     neighbor, restart, no_improvement, randomwalk = common.get_options(options, _options)
 
     perm_size = int(randomwalk * dna_size)
     if perm_size == 0:
         perm_size = 1
     max_fevals = options.get("max_fevals", 100)
 
-    tuning_options["scaling"] = False
-
     # limit max_fevals to max size of the parameter space
-    searchspace = Searchspace(tuning_options, runner.dev.max_threads)
     max_fevals = min(searchspace.size, max_fevals)
 
     fevals = 0
-    results = []
+    cost_func = CostFunc(searchspace, tuning_options, runner)
 
     #while searching
     candidate = searchspace.get_random_sample(1)[0]
-    best_score = _cost_func(candidate, kernel_options, tuning_options, runner, results, check_restrictions=False)
+    best_score = cost_func(candidate, check_restrictions=False)
 
     last_improvement = 0
     while fevals < max_fevals:
 
         try:
-            candidate = base_hillclimb(candidate, neighbor, max_fevals, searchspace, results, kernel_options, tuning_options, runner, restart=restart, randomize=True)
-            new_score = _cost_func(candidate, kernel_options, tuning_options, runner, results, check_restrictions=False)
+            candidate = base_hillclimb(candidate, neighbor, max_fevals, searchspace, tuning_options, cost_func, restart=restart, randomize=True)
+            new_score = cost_func(candidate, check_restrictions=False)
         except util.StopCriterionReached as e:
             if tuning_options.verbose:
                 print(e)
-            return results, runner.dev.get_environment()
+            return cost_func.results
 
         fevals = len(tuning_options.unique_results)
         if new_score < best_score:
             last_improvement = 0
         else:
             last_improvement += 1
 
         # Instead of full restart, permute the starting candidate
         candidate = random_walk(candidate, perm_size, no_improvement, last_improvement, searchspace)
-    return results, runner.dev.get_environment()
+    return cost_func.results
 
 
 tune.__doc__ = common.get_strategy_docstring("Greedy Iterative Local Search (ILS)", _options)
 
 def random_walk(indiv, permutation_size, no_improve, last_improve, searchspace: Searchspace):
     if last_improve >= no_improve:
         return searchspace.get_random_sample(1)[0]
```

## kernel_tuner/strategies/greedy_mls.py

```diff
@@ -7,41 +7,39 @@
 from kernel_tuner.strategies.hillclimbers import base_hillclimb
 
 _options = OrderedDict(neighbor=("Method for selecting neighboring nodes, choose from Hamming or adjacent", "Hamming"),
                        restart=("controls greedyness, i.e. whether to restart from a position as soon as an improvement is found", True),
                        order=("set a user-specified order to search among dimensions while hillclimbing", None),
                        randomize=("use a random order to search among dimensions while hillclimbing", True))
 
-def tune(runner, kernel_options, device_options, tuning_options):
+def tune(searchspace: Searchspace, runner, tuning_options):
 
     # retrieve options with defaults
     options = tuning_options.strategy_options
     neighbor, restart, order, randomize = common.get_options(options, _options)
 
     max_fevals = options.get("max_fevals", 100)
 
-    tuning_options["scaling"] = False
+    cost_func = common.CostFunc(searchspace, tuning_options, runner)
 
     # limit max_fevals to max size of the parameter space
-    searchspace = Searchspace(tuning_options, runner.dev.max_threads)
     max_fevals = min(searchspace.size, max_fevals)
 
     fevals = 0
-    results = []
 
     #while searching
     while fevals < max_fevals:
         candidate = searchspace.get_random_sample(1)[0]
 
         try:
-            base_hillclimb(candidate, neighbor, max_fevals, searchspace, results, kernel_options, tuning_options, runner, restart=restart, randomize=randomize, order=order)
+            base_hillclimb(candidate, neighbor, max_fevals, searchspace, tuning_options, cost_func, restart=restart, randomize=randomize, order=order)
         except util.StopCriterionReached as e:
             if tuning_options.verbose:
                 print(e)
-            return results, runner.dev.get_environment()
+            return cost_func.results
 
         fevals = len(tuning_options.unique_results)
 
-    return results, runner.dev.get_environment()
+    return cost_func.results
 
 
 tune.__doc__ = common.get_strategy_docstring("Greedy Multi-start Local Search (MLS)", _options)
```

## kernel_tuner/strategies/hillclimbers.py

```diff
@@ -1,15 +1,16 @@
 import random
 
 from kernel_tuner import util
 from kernel_tuner.searchspace import Searchspace
-from kernel_tuner.strategies.common import _cost_func
+from kernel_tuner.strategies.common import CostFunc
 
 
-def base_hillclimb(base_sol: tuple, neighbor_method: str, max_fevals: int, searchspace: Searchspace, all_results, kernel_options, tuning_options, runner, restart=True, randomize=True, order=None):
+def base_hillclimb(base_sol: tuple, neighbor_method: str, max_fevals: int, searchspace: Searchspace, tuning_options,
+                   cost_func: CostFunc, restart=True, randomize=True, order=None):
     """ Hillclimbing search until max_fevals is reached or no improvement is found
 
     Base hillclimber that evaluates neighbouring solutions in a random or fixed order
     and possibly immediately moves to the neighbour if it is an improvement.
 
     :params base_sol: Starting position for hillclimbing
     :type base_sol: list
@@ -21,26 +22,20 @@
     :params max_fevals: Maximum number of unique function evaluations that is allowed
          during the search.
     :type max_fevals: int
 
     :params searchspace: The searchspace object.
     :type searchspace: Seachspace
 
-    :params all_results: List of dictionaries with all benchmarked configurations
-    :type all_results: list(dict)
-
-    :param kernel_options: A dictionary with all options for the kernel.
-    :type kernel_options: dict
-
     :param tuning_options: A dictionary with all options regarding the tuning
         process.
     :type tuning_options: dict
 
-    :params runner: A runner from kernel_tuner.runners
-    :type runner: kernel_tuner.runner
+    :param cost_func: An instance of `kernel_tuner.strategies.common.CostFunc`
+    :type runner: kernel_tuner.strategies.common.CostFunc
 
     :params restart: Boolean that controls whether to greedely restart hillclimbing
         from a new position as soon as an improved position is found. True by default.
     :type restart: bool
 
     :params randomize: Boolean that controls whether the dimensions of the tunable
         parameters are randomized.
@@ -53,26 +48,24 @@
     :returns: The final position that was reached when hillclimbing halted.
     :rtype: list
 
     """
     if randomize and order:
         raise ValueError("Using a preset order and randomize at the same time is not supported.")
 
-    tune_params = tuning_options.tune_params
+    tune_params = searchspace.tune_params
 
     # measure start point score
-    best_score = _cost_func(base_sol, kernel_options, tuning_options, runner, all_results, check_restrictions=False)
+    best_score = cost_func(base_sol, check_restrictions=False)
 
     found_improved = True
     while found_improved:
         child = list(base_sol[:])
         found_improved = False
 
-        current_results = []
-
         vals = list(tune_params.values())
         if order is None:
             indices = list(range(len(vals)))
         else:
             indices = order
         if randomize:
             random.shuffle(indices)
@@ -83,29 +76,27 @@
 
             # for each value in this dimension
             for val in neighbors:
                 orig_val = child[index]
                 child[index] = val
 
                 # get score for this position
-                score = _cost_func(child, kernel_options, tuning_options, runner, current_results, check_restrictions=False)
+                score = cost_func(child, check_restrictions=False)
 
                 # generalize this to other tuning objectives
                 if score < best_score:
                     best_score = score
                     base_sol = child[:]
                     found_improved = True
                     if restart:
                         break
                 else:
                     child[index] = orig_val
 
                 fevals = len(tuning_options.unique_results)
                 if fevals >= max_fevals:
-                    all_results += current_results
                     return base_sol
+
             if found_improved and restart:
                 break
 
-        # append current_results to all_results
-        all_results += current_results
     return base_sol
```

## kernel_tuner/strategies/minimize.py

```diff
@@ -3,46 +3,43 @@
 import sys
 from collections import OrderedDict
 from time import perf_counter
 
 import numpy as np
 import scipy.optimize
 from kernel_tuner import util
-from kernel_tuner.strategies.common import (_cost_func, get_bounds_x0_eps,
+from kernel_tuner.searchspace import Searchspace
+from kernel_tuner.strategies.common import (CostFunc,
                                             get_options,
                                             get_strategy_docstring,
                                             setup_method_arguments,
                                             setup_method_options)
 
 supported_methods = ["Nelder-Mead", "Powell", "CG", "BFGS", "L-BFGS-B", "TNC", "COBYLA", "SLSQP"]
 
 _options = OrderedDict(method=(f"Local optimization algorithm to use, choose any from {supported_methods}", "L-BFGS-B"))
 
-def tune(runner, kernel_options, device_options, tuning_options):
-
-    results = []
+def tune(searchspace: Searchspace, runner, tuning_options):
 
     method = get_options(tuning_options.strategy_options, _options)[0]
 
     # scale variables in x to make 'eps' relevant for multiple variables
-    tuning_options["scaling"] = True
+    cost_func = CostFunc(searchspace, tuning_options, runner, scaling=True)
 
-    bounds, x0, _ = get_bounds_x0_eps(tuning_options, runner.dev.max_threads)
+    bounds, x0, _ = cost_func.get_bounds_x0_eps()
     kwargs = setup_method_arguments(method, bounds)
     options = setup_method_options(method, tuning_options)
 
-    args = (kernel_options, tuning_options, runner, results)
-
     opt_result = None
     try:
-        opt_result = scipy.optimize.minimize(_cost_func, x0, args=args, method=method, options=options, **kwargs)
+        opt_result = scipy.optimize.minimize(cost_func, x0, method=method, options=options, **kwargs)
     except util.StopCriterionReached as e:
         if tuning_options.verbose:
             print(e)
 
     if opt_result and tuning_options.verbose:
         print(opt_result.message)
 
-    return results, runner.dev.get_environment()
+    return cost_func.results
 
 
 tune.__doc__ = get_strategy_docstring("Minimize", _options)
```

## kernel_tuner/strategies/mls.py

```diff
@@ -1,22 +1,23 @@
 """ The strategy that uses multi-start local search """
 from collections import OrderedDict
 
+from kernel_tuner.searchspace import Searchspace
 from kernel_tuner.strategies import common
 from kernel_tuner.strategies.greedy_mls import tune as mls_tune
 
 _options = OrderedDict(neighbor=("Method for selecting neighboring nodes, choose from Hamming or adjacent", "Hamming"),
                        restart=("controls greedyness, i.e. whether to restart from a position as soon as an improvement is found", False),
                        order=("set a user-specified order to search among dimensions while hillclimbing", None),
                        randomize=("use a random order to search among dimensions while hillclimbing", True))
 
-def tune(runner, kernel_options, device_options, tuning_options):
+def tune(searchspace: Searchspace, runner, tuning_options):
 
     # Default MLS uses 'best improvement' hillclimbing, so greedy hillclimbing is disabled with restart defaulting to False
     _, restart, _, _ = common.get_options(tuning_options.strategy_options, _options)
 
     # Delegate to greedy_mls.tune() but make sure restart uses our default, if not overwritten by the user
     tuning_options.strategy_options["restart"] = restart
-    return mls_tune(runner, kernel_options, device_options, tuning_options)
+    return mls_tune(searchspace, runner, tuning_options)
 
 
 tune.__doc__ = common.get_strategy_docstring("Multi-start Local Search (MLS)", _options)
```

## kernel_tuner/strategies/ordered_greedy_mls.py

```diff
@@ -1,22 +1,23 @@
 """ A greedy multi-start local search algorithm for parameter search that traverses variables in order."""
 from collections import OrderedDict
 
+from kernel_tuner.searchspace import Searchspace
 from kernel_tuner.strategies import common
 from kernel_tuner.strategies.greedy_mls import tune as mls_tune
 
 _options = OrderedDict(neighbor=("Method for selecting neighboring nodes, choose from Hamming or adjacent", "Hamming"),
                        restart=("controls greedyness, i.e. whether to restart from a position as soon as an improvement is found", True),
                        order=("set a user-specified order to search among dimensions while hillclimbing", None),
                        randomize=("use a random order to search among dimensions while hillclimbing", False))
 
-def tune(runner, kernel_options, device_options, tuning_options):
+def tune(searchspace: Searchspace, runner, tuning_options):
 
     _, restart, _, randomize = common.get_options(tuning_options.strategy_options, _options)
 
     # Delegate to Greedy MLS, but make sure our defaults are used if not overwritten by the user
     tuning_options.strategy_options["restart"] = restart
     tuning_options.strategy_options["randomize"] = randomize
-    return mls_tune(runner, kernel_options, device_options, tuning_options)
+    return mls_tune(searchspace, runner, tuning_options)
 
 
 tune.__doc__ = common.get_strategy_docstring("Ordered Greedy Multi-start Local Search (MLS)", _options)
```

## kernel_tuner/strategies/pso.py

```diff
@@ -3,64 +3,60 @@
 import sys
 from collections import OrderedDict
 
 import numpy as np
 from kernel_tuner import util
 from kernel_tuner.searchspace import Searchspace
 from kernel_tuner.strategies import common
-from kernel_tuner.strategies.common import (_cost_func, get_bounds_x0_eps,
+from kernel_tuner.strategies.common import (CostFunc,
                                             scale_from_params)
 
 _options = OrderedDict(popsize=("Population size", 20),
                        maxiter=("Maximum number of iterations", 100),
                        w=("Inertia weight constant", 0.5),
                        c1=("Cognitive constant", 2.0),
                        c2=("Social constant", 1.0))
 
-def tune(runner, kernel_options, device_options, tuning_options):
-
-    results = []
+def tune(searchspace: Searchspace, runner, tuning_options):
 
     #scale variables in x because PSO works with velocities to visit different configurations
-    tuning_options["scaling"] = True
+    cost_func = CostFunc(searchspace, tuning_options, runner, scaling=True)
 
     #using this instead of get_bounds because scaling is used
-    bounds, _, eps = get_bounds_x0_eps(tuning_options, runner.dev.max_threads)
+    bounds, _, eps = cost_func.get_bounds_x0_eps()
 
-    args = (kernel_options, tuning_options, runner, results)
 
     num_particles, maxiter, w, c1, c2 = common.get_options(tuning_options.strategy_options, _options)
 
     best_score_global = sys.float_info.max
     best_position_global = []
 
     # init particle swarm
     swarm = []
     for i in range(0, num_particles):
-        swarm.append(Particle(bounds, args))
+        swarm.append(Particle(bounds))
 
     # ensure particles start from legal points
-    searchspace = Searchspace(tuning_options, runner.dev.max_threads)
     population = list(list(p) for p in searchspace.get_random_sample(num_particles))
     for i, particle in enumerate(swarm):
-        particle.position = scale_from_params(population[i], tuning_options.tune_params, eps)
+        particle.position = scale_from_params(population[i], searchspace.tune_params, eps)
 
     # start optimization
     for i in range(maxiter):
         if tuning_options.verbose:
             print("start iteration ", i, "best time global", best_score_global)
 
         # evaluate particle positions
         for j in range(num_particles):
             try:
-                swarm[j].evaluate(_cost_func)
+                swarm[j].evaluate(cost_func)
             except util.StopCriterionReached as e:
                 if tuning_options.verbose:
                     print(e)
-                return results, runner.dev.get_environment()
+                return cost_func.results
 
             # update global best if needed
             if swarm[j].score <= best_score_global:
                 best_position_global = swarm[j].position
                 best_score_global = swarm[j].score
 
         # update particle velocities and positions
@@ -69,32 +65,31 @@
             swarm[j].update_position(bounds)
 
     if tuning_options.verbose:
         print('Final result:')
         print(best_position_global)
         print(best_score_global)
 
-    return results, runner.dev.get_environment()
+    return cost_func.results
 
 
 tune.__doc__ = common.get_strategy_docstring("Particle Swarm Optimization (PSO)", _options)
 
 class Particle:
-    def __init__(self, bounds, args):
+    def __init__(self, bounds):
         self.ndim = len(bounds)
-        self.args = args
 
         self.velocity = np.random.uniform(-1, 1, self.ndim)
         self.position = np.random.uniform([b[0] for b in bounds], [b[1] for b in bounds])
         self.best_pos = self.position
         self.best_score = sys.float_info.max
         self.score = sys.float_info.max
 
     def evaluate(self, cost_func):
-        self.score = cost_func(self.position, *self.args)
+        self.score = cost_func(self.position)
         # update best_pos if needed
         if self.score < self.best_score:
             self.best_pos = self.position
             self.best_score = self.score
 
     def update_velocity(self, best_position_global, w, c1, c2):
         r1 = random.random()
```

## kernel_tuner/strategies/random_sample.py

```diff
@@ -1,43 +1,38 @@
 """ Iterate over a random sample of the parameter space """
 from collections import OrderedDict
 
 import numpy as np
 from kernel_tuner import util
 from kernel_tuner.searchspace import Searchspace
 from kernel_tuner.strategies import common
-from kernel_tuner.strategies.common import _cost_func
+from kernel_tuner.strategies.common import CostFunc
 
 _options = OrderedDict(fraction=("Fraction of the search space to cover value in [0, 1]", 0.1))
 
-def tune(runner, kernel_options, device_options, tuning_options):
-
-    tuning_options["scaling"] = False
-
-    # create the search space
-    searchspace = Searchspace(tuning_options, runner.dev.max_threads)
 
+def tune(searchspace: Searchspace, runner, tuning_options):
     # get the samples
     fraction = common.get_options(tuning_options.strategy_options, _options)[0]
     assert 0 <= fraction <= 1.0
     num_samples = int(np.ceil(searchspace.size * fraction))
 
     # override if max_fevals is specified
     if "max_fevals" in tuning_options:
         num_samples = tuning_options.max_fevals
 
     samples = searchspace.get_random_sample(num_samples)
 
-    results = []
+    cost_func = CostFunc(searchspace, tuning_options, runner)
 
     for sample in samples:
         try:
-            _cost_func(sample, kernel_options, tuning_options, runner, results, check_restrictions=False)
+            cost_func(sample, check_restrictions=False)
         except util.StopCriterionReached as e:
             if tuning_options.verbose:
                 print(e)
-            return results, runner.dev.get_environment()
+            return cost_func.results
 
-    return results, runner.dev.get_environment()
+    return cost_func.results
 
 
 tune.__doc__ = common.get_strategy_docstring("Random Sampling", _options)
```

## kernel_tuner/strategies/simulated_annealing.py

```diff
@@ -3,44 +3,39 @@
 import sys
 from collections import OrderedDict
 
 import numpy as np
 from kernel_tuner import util
 from kernel_tuner.searchspace import Searchspace
 from kernel_tuner.strategies import common
-from kernel_tuner.strategies.common import _cost_func
+from kernel_tuner.strategies.common import CostFunc
 
 _options = OrderedDict(T=("Starting temperature", 1.0),
                        T_min=("End temperature", 0.001),
                        alpha=("Alpha parameter", 0.995),
                        maxiter=("Number of iterations within each annealing step", 1))
 
-def tune(runner, kernel_options, device_options, tuning_options):
-
-    results = []
-
+def tune(searchspace: Searchspace, runner, tuning_options):
     # SA works with real parameter values and does not need scaling
-    tuning_options["scaling"] = False
-    args = (kernel_options, tuning_options, runner, results)
-    searchspace = Searchspace(tuning_options, runner.dev.max_threads)
+    cost_func = CostFunc(searchspace, tuning_options, runner)
 
     # optimization parameters
     T, T_min, alpha, niter = common.get_options(tuning_options.strategy_options, _options)
     T_start = T
 
     # compute how many iterations would be needed to complete the annealing schedule
     max_iter = int(np.ceil(np.log(T_min)/np.log(alpha)))
 
     # if user supplied max_fevals that is lower then max_iter we will
     # scale the annealing schedule to fit max_fevals
     max_feval = tuning_options.strategy_options.get("max_fevals", max_iter)
 
     # get random starting point and evaluate cost
     pos = list(searchspace.get_random_sample(1)[0])
-    old_cost = _cost_func(pos, *args, check_restrictions=False)
+    old_cost = cost_func(pos, check_restrictions=False)
 
     # main optimization loop
     stuck = 0
     iteration = 0
     c = 0
     c_old = 0
 
@@ -49,19 +44,19 @@
             print("iteration: ", iteration, "T", T, "cost: ", old_cost)
             iteration += 1
 
         for _ in range(niter):
 
             new_pos = neighbor(pos, searchspace)
             try:
-                new_cost = _cost_func(new_pos, *args, check_restrictions=False)
+                new_cost = cost_func(new_pos, check_restrictions=False)
             except util.StopCriterionReached as e:
                 if tuning_options.verbose:
                     print(e)
-                return results, runner.dev.get_environment()
+                return cost_func.results
 
             ap = acceptance_prob(old_cost, new_cost, T, tuning_options)
             r = random.random()
 
             if ap > r:
                 if tuning_options.verbose:
                     print("new position accepted", new_pos, new_cost, 'old:', pos, old_cost, 'ap', ap, 'r', r, 'T', T)
@@ -81,15 +76,15 @@
             pos = list(searchspace.get_random_sample(1)[0])
             stuck = 0
 
         # safeguard
         if iteration > 10*max_iter:
             break
 
-    return results, runner.dev.get_environment()
+    return cost_func.results
 
 
 tune.__doc__ = common.get_strategy_docstring("Simulated Annealing", _options)
 
 def acceptance_prob(old_cost, new_cost, T, tuning_options):
     """annealing equation, with modifications to work towards a lower value"""
     error_val = sys.float_info.max if not tuning_options.objective_higher_is_better else -sys.float_info.max
```

## Comparing `kernel_tuner/c.py` & `kernel_tuner/backends/c.py`

 * *Files 17% similar despite different names*

```diff
@@ -8,88 +8,78 @@
 import logging
 import ctypes as C
 import _ctypes
 
 import numpy as np
 import numpy.ctypeslib
 
-from kernel_tuner.observers import BenchmarkObserver
-from kernel_tuner.util import get_temp_filename, delete_temp_file, write_file, SkippableFailure
-
-dtype_map = {"int8": C.c_int8,
-             "int16": C.c_int16,
-             "int32": C.c_int32,
-             "int64": C.c_int64,
-             "uint8": C.c_uint8,
-             "uint16": C.c_uint16,
-             "uint32": C.c_uint32,
-             "uint64": C.c_uint64,
-             "float32": C.c_float,
-             "float64": C.c_double}
+from kernel_tuner.backends.backend import CompilerBackend
+from kernel_tuner.observers.c import CRuntimeObserver
+from kernel_tuner.util import (
+    get_temp_filename,
+    delete_temp_file,
+    write_file,
+    SkippableFailure,
+)
+
+dtype_map = {
+    "int8": C.c_int8,
+    "int16": C.c_int16,
+    "int32": C.c_int32,
+    "int64": C.c_int64,
+    "uint8": C.c_uint8,
+    "uint16": C.c_uint16,
+    "uint32": C.c_uint32,
+    "uint64": C.c_uint64,
+    "float32": C.c_float,
+    "float64": C.c_double,
+}
 
 # This represents an individual kernel argument.
 # It contains a numpy object (ndarray or number) and a ctypes object with a copy
 # of the argument data. For an ndarray, the ctypes object is a wrapper for the ndarray's data.
 Argument = namedtuple("Argument", ["numpy", "ctypes"])
 
-class CRuntimeObserver(BenchmarkObserver):
-    """ Observer that collects results returned by benchmarking function """
-
-    def __init__(self, dev):
-        self.dev = dev
-        self.objective = "time"
-        self.times = []
-
-    def after_finish(self):
-        self.times.append(self.dev.last_result)
-
-    def get_results(self):
-        results = {
-            self.objective: np.average(self.times),
-            self.objective + "s": self.times.copy()
-        }
-        self.times = []
-        return results
 
-
-class CFunctions(object):
+class CFunctions(CompilerBackend):
     """Class that groups the code for running and compiling C functions"""
 
     def __init__(self, iterations=7, compiler_options=None, compiler=None):
         """instantiate CFunctions object used for interacting with C code
 
         :param iterations: Number of iterations used while benchmarking a kernel, 7 by default.
         :type iterations: int
         """
         self.iterations = iterations
         self.max_threads = 1024
         self.compiler_options = compiler_options
-        self.compiler = compiler or "g++"  # use gcc by default
+        # if no compiler is specified, use g++ by default
+        self.compiler = compiler or "g++"
         self.lib = None
         self.using_openmp = False
         self.observers = [CRuntimeObserver(self)]
         self.last_result = None
 
         try:
             cc_version = str(subprocess.check_output([self.compiler, "--version"]))
             cc_version = cc_version.splitlines()[0].split(" ")[-1]
         except OSError as e:
             raise e
 
-        #check if nvcc is available
+        # check if nvcc is available
         self.nvcc_available = False
         try:
             nvcc_version = str(subprocess.check_output(["nvcc", "--version"]))
             nvcc_version = nvcc_version.splitlines()[-1].split(" ")[-1]
             self.nvcc_available = True
         except OSError as e:
             if e.errno != errno.ENOENT:
                 raise e
 
-        #environment info
+        # environment info
         env = dict()
         env["CC Version"] = cc_version
         if self.nvcc_available:
             env["NVCC Version"] = nvcc_version
         env["iterations"] = self.iterations
         env["compiler_options"] = compiler_options
         self.env = env
@@ -102,27 +92,29 @@
             The order should match the argument list on the C function.
             Allowed values are np.ndarray, and/or np.int32, np.float32, and so on.
         :type arguments: list(numpy objects)
 
         :returns: A list of arguments that can be passed to the C function.
         :rtype: list(Argument)
         """
-        ctype_args = [ None for _ in arguments]
+        ctype_args = [None for _ in arguments]
 
         for i, arg in enumerate(arguments):
             if not isinstance(arg, (np.ndarray, np.number)):
-                raise TypeError("Argument is not numpy ndarray or numpy scalar %s" % type(arg))
+                raise TypeError(
+                    "Argument is not numpy ndarray or numpy scalar %s" % type(arg)
+                )
             dtype_str = str(arg.dtype)
             if isinstance(arg, np.ndarray):
                 if dtype_str in dtype_map.keys():
                     # In numpy <= 1.15, ndarray.ctypes.data_as does not itself keep a reference
                     # to its underlying array, so we need to store a reference to arg.copy()
                     # in the Argument object manually to avoid it being deleted.
                     # (This changed in numpy > 1.15.)
-                    #data_ctypes = data.ctypes.data_as(C.POINTER(dtype_map[dtype_str]))
+                    # data_ctypes = data.ctypes.data_as(C.POINTER(dtype_map[dtype_str]))
                     data_ctypes = arg.ctypes.data_as(C.POINTER(dtype_map[dtype_str]))
                 else:
                     raise TypeError("unknown dtype for ndarray")
             elif isinstance(arg, np.generic):
                 data_ctypes = dtype_map[dtype_str](arg)
             ctype_args[i] = Argument(numpy=arg, ctypes=data_ctypes)
         return ctype_args
@@ -133,137 +125,153 @@
         :param kernel_instance: An object representing the specific instance of the tunable kernel
             in the parameter space.
         :type kernel_instance: kernel_tuner.core.KernelInstance
 
         :returns: An ctypes function that can be called directly.
         :rtype: ctypes._FuncPtr
         """
-        logging.debug('compiling ' + kernel_instance.name)
+        logging.debug("compiling " + kernel_instance.name)
 
         kernel_string = kernel_instance.kernel_string
         kernel_name = kernel_instance.name
 
         if self.lib is not None:
             self.cleanup_lib()
 
         compiler_options = ["-fPIC"]
 
-        #detect openmp
+        # detect openmp
         if "#include <omp.h>" in kernel_string or "use omp_lib" in kernel_string:
-            logging.debug('set using_openmp to true')
+            logging.debug("set using_openmp to true")
             self.using_openmp = True
             if self.compiler == "pgfortran":
                 compiler_options.append("-mp")
             else:
                 if "#pragma acc" in kernel_string or "!$acc" in kernel_string:
                     compiler_options.append("-fopenacc")
                 else:
                     compiler_options.append("-fopenmp")
 
-        #if filename is known, use that one
+        # if filename is known, use that one
         suffix = kernel_instance.kernel_source.get_user_suffix()
 
-        #if code contains device code, suffix .cu is required
+        # if code contains device code, suffix .cu is required
         device_code_signals = ["__global", "__syncthreads()", "threadIdx"]
         if any([snippet in kernel_string for snippet in device_code_signals]):
             suffix = ".cu"
 
-        #detect whether to use nvcc as default instead of g++, may overrule an explicitly passed g++
-        if ((suffix == ".cu") or ("#include <cuda" in kernel_string) or ("cudaMemcpy" in kernel_string)) and self.compiler == "g++" and self.nvcc_available:
+        # detect whether to use nvcc as default instead of g++, may overrule an explicitly passed g++
+        if (
+            (
+                (suffix == ".cu")
+                or ("#include <cuda" in kernel_string)
+                or ("cudaMemcpy" in kernel_string)
+            )
+            and self.compiler == "g++"
+            and self.nvcc_available
+        ):
             self.compiler = "nvcc"
 
         if suffix is None:
-            #select right suffix based on compiler
+            # select right suffix based on compiler
             suffix = ".cc"
 
             if self.compiler in ["gfortran", "pgfortran", "ftn", "ifort"]:
                 suffix = ".F90"
 
         if self.compiler == "nvcc":
             compiler_options = ["-Xcompiler=" + c for c in compiler_options]
 
-        #this basically checks if we aren't compiling Fortran
-        #at the moment any C, C++, or CUDA code is assumed to use extern "C" linkage
-        if ".c" in suffix and "extern \"C\"" not in kernel_string:
-            kernel_string = "extern \"C\" {\n" + kernel_string + "\n}"
+        # this basically checks if we aren't compiling Fortran
+        # at the moment any C, C++, or CUDA code is assumed to use extern "C" linkage
+        if ".c" in suffix and 'extern "C"' not in kernel_string:
+            kernel_string = 'extern "C" {\n' + kernel_string + "\n}"
 
-        #copy user specified compiler options to current list
+        # copy user specified compiler options to current list
         if self.compiler_options:
             compiler_options += self.compiler_options
 
         lib_args = []
         if "CL/cl.h" in kernel_string:
             lib_args = ["-lOpenCL"]
 
-        logging.debug('using compiler ' + self.compiler)
-        logging.debug('compiler_options ' + " ".join(compiler_options))
-        logging.debug('lib_args ' + " ".join(lib_args))
+        logging.debug("using compiler " + self.compiler)
+        logging.debug("compiler_options " + " ".join(compiler_options))
+        logging.debug("lib_args " + " ".join(lib_args))
 
         source_file = get_temp_filename(suffix=suffix)
         filename = ".".join(source_file.split(".")[:-1])
 
-        #detect Fortran modules
+        # detect Fortran modules
         match = re.search(r"\s*module\s+([a-zA-Z_]*)", kernel_string)
         if match:
             if self.compiler == "gfortran":
                 kernel_name = "__" + match.group(1) + "_MOD_" + kernel_name
             elif self.compiler in ["ftn", "ifort"]:
                 kernel_name = match.group(1) + "_mp_" + kernel_name + "_"
             elif self.compiler == "pgfortran":
                 kernel_name = match.group(1) + "_" + kernel_name + "_"
         else:
-            #for functions outside of modules
+            # for functions outside of modules
             if self.compiler in ["gfortran", "ftn", "ifort", "pgfortran"]:
                 kernel_name = kernel_name + "_"
 
         try:
             write_file(source_file, kernel_string)
 
             lib_extension = ".so"
             if platform.system() == "Darwin":
                 lib_extension = ".dylib"
 
-            subprocess.check_call([self.compiler, "-c", source_file] + compiler_options + ["-o", filename + ".o"])
-            subprocess.check_call([self.compiler, filename + ".o"] + compiler_options + ["-shared", "-o", filename + lib_extension] + lib_args)
+            subprocess.check_call(
+                [self.compiler, "-c", source_file]
+                + compiler_options
+                + ["-o", filename + ".o"]
+            )
+            subprocess.check_call(
+                [self.compiler, filename + ".o"]
+                + compiler_options
+                + ["-shared", "-o", filename + lib_extension]
+                + lib_args
+            )
 
-            self.lib = np.ctypeslib.load_library(filename, '.')
+            self.lib = np.ctypeslib.load_library(filename, ".")
             func = getattr(self.lib, kernel_name)
             func.restype = C.c_float
 
         finally:
             delete_temp_file(source_file)
-            delete_temp_file(filename+".o")
-            delete_temp_file(filename+".so")
-            delete_temp_file(filename+".dylib")
+            delete_temp_file(filename + ".o")
+            delete_temp_file(filename + ".so")
+            delete_temp_file(filename + ".dylib")
 
         return func
 
-
     def start_event(self):
-        """ Records the event that marks the start of a measurement
+        """Records the event that marks the start of a measurement
 
-        C backend does not use events """
+        C backend does not use events"""
         pass
 
     def stop_event(self):
-        """ Records the event that marks the end of a measurement
+        """Records the event that marks the end of a measurement
 
-        C backend does not use events """
+        C backend does not use events"""
         pass
 
     def kernel_finished(self):
-        """ Returns True if the kernel has finished, False otherwise
+        """Returns True if the kernel has finished, False otherwise
 
-        C backend does not support asynchronous launches """
+        C backend does not support asynchronous launches"""
         return True
 
     def synchronize(self):
-        """ Halts execution until device has finished its tasks
+        """Halts execution until device has finished its tasks
 
-        C backend does not support asynchronous launches """
+        C backend does not support asynchronous launches"""
         pass
 
     def run_kernel(self, func, c_args, threads, grid):
         """runs the kernel once, returns whatever the kernel returns
 
         :param func: A C function compiled for this specific configuration
         :type func: ctypes._FuncPtr
@@ -325,15 +333,15 @@
 
         :param src: A numpy array containing the source data
         :type src: np.ndarray
         """
         dest.numpy[:] = src
 
     def cleanup_lib(self):
-        """ unload the previously loaded shared library """
+        """unload the previously loaded shared library"""
         if not self.using_openmp:
-            #this if statement is necessary because shared libraries that use
-            #OpenMP will core dump when unloaded, this is a well-known issue with OpenMP
-            logging.debug('unloading shared library')
+            # this if statement is necessary because shared libraries that use
+            # OpenMP will core dump when unloaded, this is a well-known issue with OpenMP
+            logging.debug("unloading shared library")
             _ctypes.dlclose(self.lib._handle)
 
     units = {}
```

## Comparing `kernel_tuner/cupy.py` & `kernel_tuner/backends/cupy.py`

 * *Files 7% similar despite different names*

```diff
@@ -2,43 +2,27 @@
 from __future__ import print_function
 
 
 import logging
 import time
 import numpy as np
 
-from kernel_tuner.observers import BenchmarkObserver
+from kernel_tuner.backends.backend import GPUBackend
+from kernel_tuner.observers.cupy import CupyRuntimeObserver
 
-#embedded in try block to be able to generate documentation
-#and run tests without cupy installed
+
+# embedded in try block to be able to generate documentation
+# and run tests without cupy installed
 try:
     import cupy as cp
 except ImportError:
     cp = None
 
 
-class CupyRuntimeObserver(BenchmarkObserver):
-    """ Observer that measures time using CUDA events during benchmarking """
-    def __init__(self, dev):
-        self.dev = dev
-        self.stream = dev.stream
-        self.start = dev.start
-        self.end = dev.end
-        self.times = []
-
-    def after_finish(self):
-        self.times.append(cp.cuda.get_elapsed_time(self.start, self.end)) #ms
-
-    def get_results(self):
-        results = {"time": np.average(self.times), "times": self.times.copy()}
-        self.times = []
-        return results
-
-
-class CupyFunctions:
+class CupyFunctions(GPUBackend):
     """Class that groups the Cupy functions on maintains state about the device"""
 
     def __init__(self, device=0, iterations=7, compiler_options=None, observers=None):
         """instantiate CupyFunctions object used for interacting with the CUDA device
 
         Instantiating this object will inspect and store certain device properties at
         runtime, which are used during compilation and/or execution of kernels by the
@@ -50,50 +34,54 @@
 
         :param iterations: Number of iterations used while benchmarking a kernel, 7 by default.
         :type iterations: int
         """
         self.allocations = []
         self.texrefs = []
         if not cp:
-            raise ImportError("Error: cupy not installed, please install e.g. " +
-                            "using 'pip install cupy', please check https://github.com/cupy/cupy.")
+            raise ImportError(
+                "Error: cupy not installed, please install e.g. "
+                + "using 'pip install cupy', please check https://github.com/cupy/cupy."
+            )
 
-        #select device
+        # select device
         self.dev = dev = cp.cuda.Device(device)
         dev.use()
 
-        #inspect device properties
+        # inspect device properties
         self.devprops = dev.attributes
         self.cc = dev.compute_capability
-        self.max_threads = self.devprops['MaxThreadsPerBlock']
+        self.max_threads = self.devprops["MaxThreadsPerBlock"]
 
         self.iterations = iterations
         self.current_module = None
         self.func = None
         self.compiler_options = compiler_options or []
 
-        #create a stream and events
+        # create a stream and events
         self.stream = cp.cuda.Stream()
         self.start = cp.cuda.Event()
         self.end = cp.cuda.Event()
 
-        #default dynamically allocated shared memory size, can be overwritten using smem_args
+        # default dynamically allocated shared memory size, can be overwritten using smem_args
         self.smem_size = 0
 
-        #setup observers
+        # setup observers
         self.observers = observers or []
         self.observers.append(CupyRuntimeObserver(self))
         for obs in self.observers:
             obs.register_device(self)
 
-        #collect environment information
+        # collect environment information
         env = dict()
         cupy_info = str(cp._cupyx.get_runtime_info()).split("\n")[:-1]
-        info_dict = {s.split(":")[0].strip():s.split(":")[1].strip() for s in cupy_info}
-        env["device_name"] = info_dict[f'Device {device} Name']
+        info_dict = {
+            s.split(":")[0].strip(): s.split(":")[1].strip() for s in cupy_info
+        }
+        env["device_name"] = info_dict[f"Device {device} Name"]
 
         env["cuda_version"] = cp.cuda.runtime.driverGetVersion()
         env["compute_capability"] = self.cc
         env["iterations"] = self.iterations
         env["compiler_options"] = compiler_options
         env["device_properties"] = self.devprops
         self.env = env
@@ -113,19 +101,19 @@
         gpu_args = []
         for arg in arguments:
             # if arg i is a numpy array copy to device
             if isinstance(arg, np.ndarray):
                 alloc = cp.array(arg)
                 self.allocations.append(alloc)
                 gpu_args.append(alloc)
-            else: # if not a numpy array, just pass argument along
+            # if not a numpy array, just pass argument along
+            else:
                 gpu_args.append(arg)
         return gpu_args
 
-
     def compile(self, kernel_instance):
         """call the CUDA compiler to compile the kernel, return the device function
 
         :param kernel_name: The name of the kernel to be compiled, used to lookup the
             function after compilation.
         :type kernel_name: string
 
@@ -135,70 +123,69 @@
         :returns: An CUDA kernel that can be called directly.
         :rtype: cupy.RawKernel
         """
         kernel_string = kernel_instance.kernel_string
         kernel_name = kernel_instance.name
 
         compiler_options = self.compiler_options
-        if not any(['-std=' in opt for opt in self.compiler_options]):
-            compiler_options = ['--std=c++11'] + self.compiler_options
+        if not any(["-std=" in opt for opt in self.compiler_options]):
+            compiler_options = ["--std=c++11"] + self.compiler_options
 
         options = tuple(compiler_options)
 
-        self.current_module = cp.RawModule(code=kernel_string, options=options,
-                                           name_expressions=[kernel_name])
+        self.current_module = cp.RawModule(
+            code=kernel_string, options=options, name_expressions=[kernel_name]
+        )
 
         self.func = self.current_module.get_function(kernel_name)
         return self.func
 
-
     def start_event(self):
-        """ Records the event that marks the start of a measurement """
+        """Records the event that marks the start of a measurement"""
         self.start.record(stream=self.stream)
 
     def stop_event(self):
-        """ Records the event that marks the end of a measurement """
+        """Records the event that marks the end of a measurement"""
         self.end.record(stream=self.stream)
 
     def kernel_finished(self):
-        """ Returns True if the kernel has finished, False otherwise """
+        """Returns True if the kernel has finished, False otherwise"""
         return self.end.done
 
     def synchronize(self):
-        """ Halts execution until device has finished its tasks """
+        """Halts execution until device has finished its tasks"""
         self.dev.synchronize()
 
-
     def copy_constant_memory_args(self, cmem_args):
         """adds constant memory arguments to the most recently compiled module
 
         :param cmem_args: A dictionary containing the data to be passed to the
             device constant memory. The format to be used is as follows: A
             string key is used to name the constant memory symbol to which the
             value needs to be copied. Similar to regular arguments, these need
             to be numpy objects, such as numpy.ndarray or numpy.int32, and so on.
         :type cmem_args: dict( string: numpy.ndarray, ... )
         """
         for k, v in cmem_args.items():
             symbol = self.current_module.get_global(k)
-            constant_mem = cp.ndarray(v.shape,v.dtype,symbol)
+            constant_mem = cp.ndarray(v.shape, v.dtype, symbol)
             constant_mem[:] = cp.asarray(v)
 
     def copy_shared_memory_args(self, smem_args):
         """add shared memory arguments to the kernel"""
         self.smem_size = smem_args["size"]
 
     def copy_texture_memory_args(self, texmem_args):
         """adds texture memory arguments to the most recently compiled module
 
         :param texmem_args: A dictionary containing the data to be passed to the
             device texture memory. See tune_kernel().
         :type texmem_args: dict
         """
-        raise NotImplementedError('CuPy backend does not yet support texture memory')
+        raise NotImplementedError("CuPy backend does not support texture memory")
 
     def run_kernel(self, func, gpu_args, threads, grid, stream=None):
         """runs the CUDA kernel passed as 'func'
 
         :param func: A cupy kernel compiled for this specific kernel configuration
         :type func: cupy.RawKernel
 
@@ -258,8 +245,8 @@
         :param src: A numpy array in host memory to store the data
         :type src: numpy.ndarray
         """
         if isinstance(src, np.ndarray):
             src = cp.asarray(src)
         cp.copyto(dest, src)
 
-    units = {'time': 'ms'}
+    units = {"time": "ms"}
```

## Comparing `kernel_tuner/nvcuda.py` & `kernel_tuner/backends/nvcuda.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,62 +1,23 @@
 """This module contains all NVIDIA cuda-python specific kernel_tuner functions"""
 import numpy as np
 
-from kernel_tuner.observers import BenchmarkObserver
-from kernel_tuner.util import SkippableFailure
+from kernel_tuner.backends.backend import GPUBackend
+from kernel_tuner.observers.nvcuda import CudaRuntimeObserver
+from kernel_tuner.util import SkippableFailure, cuda_error_check
 
-#embedded in try block to be able to generate documentation
-#and run tests without cuda-python installed
+# embedded in try block to be able to generate documentation
+# and run tests without cuda-python installed
 try:
     from cuda import cuda, cudart, nvrtc
 except ImportError:
     cuda = None
 
 
-def error_check(error):
-    """ Checking the status of CUDA calls """
-    if isinstance(error, cuda.CUresult):
-        if error != cuda.CUresult.CUDA_SUCCESS:
-            _, name = cuda.cuGetErrorName(error)
-            raise RuntimeError(f"CUDA error: {name.decode()}")
-    elif isinstance(error, cudart.cudaError_t):
-        if error != cudart.cudaError_t.cudaSuccess:
-            _, name = cudart.getErrorName(error)
-            raise RuntimeError(f"CUDART error: {name.decode()}")
-    elif isinstance(error, nvrtc.nvrtcResult):
-        if error != nvrtc.nvrtcResult.NVRTC_SUCCESS:
-            _, desc = nvrtc.nvrtcGetErrorString(error)
-            raise RuntimeError(f"NVRTC error: {desc.decode()}")
-
-
-class CudaRuntimeObserver(BenchmarkObserver):
-    """ Observer that measures time using CUDA events during benchmarking """
-    def __init__(self, dev):
-        self.dev = dev
-        self.stream = dev.stream
-        self.start = dev.start
-        self.end = dev.end
-        self.times = []
-
-    def after_finish(self):
-        # time in ms
-        err, time = cudart.cudaEventElapsedTime(self.start, self.end)
-        error_check(err)
-        self.times.append(time)
-
-    def get_results(self):
-        results = {
-            "time": np.average(self.times),
-            "times": self.times.copy()
-        }
-        self.times = []
-        return results
-
-
-class CudaFunctions(object):
+class CudaFunctions(GPUBackend):
     """Class that groups the Cuda functions on maintains state about the device"""
 
     def __init__(self, device=0, iterations=7, compiler_options=None, observers=None):
         """instantiate CudaFunctions object used for interacting with the CUDA device
 
         Instantiating this object will inspect and store certain device properties at
         runtime, which are used during compilation and/or execution of kernels by the
@@ -72,80 +33,88 @@
         :param compiler_options: Compiler options for the CUDA runtime compiler
 
         :param observers: List of Observer type objects
         """
         self.allocations = []
         self.texrefs = []
         if not cuda:
-            raise ImportError("Error: cuda-python not installed, please install e.g. " +
-                              "using 'pip install cuda-python', please check https://github.com/NVIDIA/cuda-python.")
+            raise ImportError(
+                "Error: cuda-python not installed, please install e.g. "
+                + "using 'pip install cuda-python', please check https://github.com/NVIDIA/cuda-python."
+            )
 
         # initialize and select device
         err = cuda.cuInit(0)
-        error_check(err)
+        cuda_error_check(err)
         err, self.device = cuda.cuDeviceGet(device)
-        error_check(err)
+        cuda_error_check(err)
         err, self.context = cuda.cuDevicePrimaryCtxRetain(device)
-        error_check(err)
+        cuda_error_check(err)
         if CudaFunctions.last_selected_device != device:
             err = cuda.cuCtxSetCurrent(self.context)
-            error_check(err)
+            cuda_error_check(err)
             CudaFunctions.last_selected_device = device
 
         # compute capabilities and device properties
-        err, major = cudart.cudaDeviceGetAttribute(cudart.cudaDeviceAttr.cudaDevAttrComputeCapabilityMajor, device)
-        error_check(err)
-        err, minor = cudart.cudaDeviceGetAttribute(cudart.cudaDeviceAttr.cudaDevAttrComputeCapabilityMinor, device)
-        error_check(err)
-        err, self.max_threads = cudart.cudaDeviceGetAttribute(cudart.cudaDeviceAttr.cudaDevAttrMaxThreadsPerBlock, device)
-        error_check(err)
+        err, major = cudart.cudaDeviceGetAttribute(
+            cudart.cudaDeviceAttr.cudaDevAttrComputeCapabilityMajor, device
+        )
+        cuda_error_check(err)
+        err, minor = cudart.cudaDeviceGetAttribute(
+            cudart.cudaDeviceAttr.cudaDevAttrComputeCapabilityMinor, device
+        )
+        cuda_error_check(err)
+        err, self.max_threads = cudart.cudaDeviceGetAttribute(
+            cudart.cudaDeviceAttr.cudaDevAttrMaxThreadsPerBlock, device
+        )
+        cuda_error_check(err)
         self.cc = f"{major}{minor}"
         self.iterations = iterations
         self.current_module = None
         self.func = None
         self.compiler_options = compiler_options or []
         self.compiler_options_bytes = []
         for option in self.compiler_options:
             self.compiler_options_bytes.append(str(option).encode("UTF-8"))
 
         # create a stream and events
         err, self.stream = cuda.cuStreamCreate(0)
-        error_check(err)
+        cuda_error_check(err)
         err, self.start = cuda.cuEventCreate(0)
-        error_check(err)
+        cuda_error_check(err)
         err, self.end = cuda.cuEventCreate(0)
-        error_check(err)
+        cuda_error_check(err)
 
         # default dynamically allocated shared memory size, can be overwritten using smem_args
         self.smem_size = 0
 
         # setup observers
         self.observers = observers or []
         self.observers.append(CudaRuntimeObserver(self))
         for observer in self.observers:
             observer.register_device(self)
 
         # collect environment information
         err, device_properties = cudart.cudaGetDeviceProperties(device)
-        error_check(err)
+        cuda_error_check(err)
         env = dict()
         env["device_name"] = device_properties.name.decode()
         env["cuda_version"] = cuda.CUDA_VERSION
         env["compute_capability"] = self.cc
         env["iterations"] = self.iterations
         env["compiler_options"] = self.compiler_options
         env["device_properties"] = str(device_properties).replace("\n", ", ")
         self.env = env
         self.name = env["device_name"]
 
     def __del__(self):
         for device_memory in self.allocations:
             if isinstance(device_memory, cuda.CUdeviceptr):
                 err = cuda.cuMemFree(device_memory)
-                error_check(err)
+                cuda_error_check(err)
 
     def ready_argument_list(self, arguments):
         """ready argument list to be passed to the kernel, allocates gpu mem
 
         :param arguments: List of arguments to be passed to the kernel.
             The order should match the argument list on the CUDA kernel.
             Allowed values are numpy.ndarray, and/or numpy.int32, numpy.float32, and so on.
@@ -155,15 +124,15 @@
         :rtype: list( pycuda.driver.DeviceAllocation, numpy.int32, ... )
         """
         gpu_args = []
         for arg in arguments:
             # if arg is a numpy array copy it to device
             if isinstance(arg, np.ndarray):
                 err, device_memory = cuda.cuMemAlloc(arg.nbytes)
-                error_check(err)
+                cuda_error_check(err)
                 self.allocations.append(device_memory)
                 gpu_args.append(device_memory)
                 self.memcpy_htod(device_memory, arg)
             # if not array, just pass along
             else:
                 gpu_args.append(arg)
         return gpu_args
@@ -180,108 +149,115 @@
 
         :returns: A kernel that can be launched by the CUDA runtime
         :rtype:
         """
         kernel_string = kernel_instance.kernel_string
         kernel_name = kernel_instance.name
 
-        #mimic pycuda behavior to wrap kernel_string in extern "C" if not in kernel_string already
+        # mimic pycuda behavior to wrap kernel_string in extern "C" if not in kernel_string already
         if 'extern "C"' not in kernel_string:
-            kernel_string = 'extern "C" {\n' + kernel_string + '\n}'
+            kernel_string = 'extern "C" {\n' + kernel_string + "\n}"
 
         compiler_options = self.compiler_options_bytes
         if not any([b"--std=" in opt for opt in compiler_options]):
             compiler_options.append(b"--std=c++11")
         if not any(["--std=" in opt for opt in self.compiler_options]):
             self.compiler_options.append("--std=c++11")
         if not any([b"--gpu-architecture=" in opt for opt in compiler_options]):
-            compiler_options.append(f"--gpu-architecture=compute_{self.cc}".encode("UTF-8"))
+            compiler_options.append(
+                f"--gpu-architecture=compute_{self.cc}".encode("UTF-8")
+            )
         if not any(["--gpu-architecture=" in opt for opt in self.compiler_options]):
             self.compiler_options.append(f"--gpu-architecture=compute_{self.cc}")
 
-        err, program = nvrtc.nvrtcCreateProgram(str.encode(kernel_string), b"CUDAProgram", 0, [], [])
+        err, program = nvrtc.nvrtcCreateProgram(
+            str.encode(kernel_string), b"CUDAProgram", 0, [], []
+        )
         try:
-            error_check(err)
-            err = nvrtc.nvrtcCompileProgram(program, len(compiler_options), compiler_options)
-            error_check(err)
+            cuda_error_check(err)
+            err = nvrtc.nvrtcCompileProgram(
+                program, len(compiler_options), compiler_options
+            )
+            cuda_error_check(err)
             err, size = nvrtc.nvrtcGetPTXSize(program)
-            error_check(err)
-            buff = b' ' * size
+            cuda_error_check(err)
+            buff = b" " * size
             err = nvrtc.nvrtcGetPTX(program, buff)
-            error_check(err)
+            cuda_error_check(err)
             err, self.current_module = cuda.cuModuleLoadData(np.char.array(buff))
             if err == cuda.CUresult.CUDA_ERROR_INVALID_PTX:
                 raise SkippableFailure("uses too much shared data")
             else:
-                error_check(err)
-            err, self.func = cuda.cuModuleGetFunction(self.current_module, str.encode(kernel_name))
-            error_check(err)
+                cuda_error_check(err)
+            err, self.func = cuda.cuModuleGetFunction(
+                self.current_module, str.encode(kernel_name)
+            )
+            cuda_error_check(err)
 
         except RuntimeError as re:
             _, n = nvrtc.nvrtcGetProgramLogSize(program)
-            log = b' ' * n
+            log = b" " * n
             nvrtc.nvrtcGetProgramLog(program, log)
-            print(log.decode('utf-8'))
+            print(log.decode("utf-8"))
             raise re
 
         return self.func
 
     def start_event(self):
-        """ Records the event that marks the start of a measurement """
+        """Records the event that marks the start of a measurement"""
         err = cudart.cudaEventRecord(self.start, self.stream)
-        error_check(err)
+        cuda_error_check(err)
 
     def stop_event(self):
-        """ Records the event that marks the end of a measurement """
+        """Records the event that marks the end of a measurement"""
         err = cudart.cudaEventRecord(self.end, self.stream)
-        error_check(err)
+        cuda_error_check(err)
 
     def kernel_finished(self):
-        """ Returns True if the kernel has finished, False otherwise """
+        """Returns True if the kernel has finished, False otherwise"""
         err = cudart.cudaEventQuery(self.end)
         if err[0] == cudart.cudaError_t.cudaSuccess:
             return True
         else:
             return False
 
     @staticmethod
     def synchronize():
-        """ Halts execution until device has finished its tasks """
+        """Halts execution until device has finished its tasks"""
         err = cudart.cudaDeviceSynchronize()
-        error_check(err)
-
+        cuda_error_check(err)
 
     def copy_constant_memory_args(self, cmem_args):
         """adds constant memory arguments to the most recently compiled module
 
         :param cmem_args: A dictionary containing the data to be passed to the
             device constant memory. The format to be used is as follows: A
             string key is used to name the constant memory symbol to which the
             value needs to be copied. Similar to regular arguments, these need
             to be numpy objects, such as numpy.ndarray or numpy.int32, and so on.
         :type cmem_args: dict( string: numpy.ndarray, ... )
         """
         for k, v in cmem_args.items():
             err, symbol, _ = cuda.cuModuleGetGlobal(self.current_module, str.encode(k))
-            error_check(err)
+            cuda_error_check(err)
             err = cuda.cuMemcpyHtoD(symbol, v, v.nbytes)
-            error_check(err)
+            cuda_error_check(err)
 
     def copy_shared_memory_args(self, smem_args):
         """add shared memory arguments to the kernel"""
         self.smem_size = smem_args["size"]
 
     def copy_texture_memory_args(self, texmem_args):
         """adds texture memory arguments to the most recently compiled module
 
         :param texmem_args: A dictionary containing the data to be passed to the
             device texture memory. See tune_kernel().
         :type texmem_args: dict
         """
-        raise NotImplementedError('NVIDIA CUDA backend does not yet support texture memory')
+        raise NotImplementedError("NVIDIA CUDA backend does not support texture memory")
 
     def run_kernel(self, func, gpu_args, threads, grid, stream=None):
         """runs the CUDA kernel passed as 'func'
 
         :param func: A CUDA kernel compiled for this specific kernel configuration
         :type func: cuda.CUfunction
 
@@ -300,17 +276,29 @@
         """
         arg_types = list()
         for arg in gpu_args:
             if isinstance(arg, cuda.CUdeviceptr):
                 arg_types.append(None)
             else:
                 arg_types.append(np.ctypeslib.as_ctypes_type(arg.dtype))
-        kernel_args  = (tuple(gpu_args), tuple(arg_types))
-        err = cuda.cuLaunchKernel(func, grid[0], grid[1], grid[2], threads[0], threads[1], threads[2], self.smem_size, stream, kernel_args, 0)
-        error_check(err)
+        kernel_args = (tuple(gpu_args), tuple(arg_types))
+        err = cuda.cuLaunchKernel(
+            func,
+            grid[0],
+            grid[1],
+            grid[2],
+            threads[0],
+            threads[1],
+            threads[2],
+            self.smem_size,
+            stream,
+            kernel_args,
+            0,
+        )
+        cuda_error_check(err)
 
     @staticmethod
     def memset(allocation, value, size):
         """set the memory in allocation to the value in value
 
         :param allocation: A GPU memory allocation unit
         :type allocation: cupy.ndarray
@@ -319,38 +307,38 @@
         :type value: a single 8-bit unsigned int
 
         :param size: The size of to the allocation unit in bytes
         :type size: int
 
         """
         err = cudart.cudaMemset(allocation, value, size)
-        error_check(err)
+        cuda_error_check(err)
 
     @staticmethod
     def memcpy_dtoh(dest, src):
         """perform a device to host memory copy
 
         :param dest: A numpy array in host memory to store the data
         :type dest: numpy.ndarray
 
         :param src: A GPU memory allocation unit
         :type src: cuda.CUdeviceptr
         """
         err = cuda.cuMemcpyDtoH(dest, src, dest.nbytes)
-        error_check(err)
+        cuda_error_check(err)
 
     @staticmethod
     def memcpy_htod(dest, src):
         """perform a host to device memory copy
 
         :param dest: A GPU memory allocation unit
         :type dest: cuda.CUdeviceptr
 
         :param src: A numpy array in host memory to store the data
         :type src: numpy.ndarray
         """
         err = cuda.cuMemcpyHtoD(dest, src, src.nbytes)
-        error_check(err)
+        cuda_error_check(err)
 
-    units = {'time': 'ms'}
+    units = {"time": "ms"}
 
     last_selected_device = None
```

## Comparing `kernel_tuner/nvml.py` & `kernel_tuner/observers/nvml.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,84 +1,96 @@
 import subprocess
 import time
 import re
 import numpy as np
 from collections import OrderedDict
 
-from kernel_tuner.observers import BenchmarkObserver, ContinuousObserver
+from kernel_tuner.observers.observer import BenchmarkObserver, ContinuousObserver
 
 try:
     import pynvml
 except ImportError:
     pynvml = None
 
-class nvml():
+
+class nvml:
     """Class that gathers the NVML functionality for one device"""
 
-    def __init__(self, device_id=0, nvidia_smi_fallback='nvidia-smi', use_locked_clocks=False):
+    def __init__(
+        self, device_id=0, nvidia_smi_fallback="nvidia-smi", use_locked_clocks=False
+    ):
         """Create object to control device using NVML"""
 
         pynvml.nvmlInit()
         self.dev = pynvml.nvmlDeviceGetHandleByIndex(device_id)
         self.id = device_id
         self.nvidia_smi = nvidia_smi_fallback
 
         try:
             self.pwr_limit_default = pynvml.nvmlDeviceGetPowerManagementLimit(self.dev)
-            self.pwr_constraints = pynvml.nvmlDeviceGetPowerManagementLimitConstraints(self.dev)
+            self.pwr_constraints = pynvml.nvmlDeviceGetPowerManagementLimitConstraints(
+                self.dev
+            )
         except pynvml.NVMLError_NotSupported:
             self.pwr_limit_default = None
-            self.pwr_constraints = [1, 0] # inverted range to make all range checks fail
+            # inverted range to make all range checks fail
+            self.pwr_constraints = [1, 0]
 
         try:
             self._persistence_mode = pynvml.nvmlDeviceGetPersistenceMode(self.dev)
         except pynvml.NVMLError_NotSupported:
             self._persistence_mode = None
 
         try:
-            self._auto_boost = pynvml.nvmlDeviceGetAutoBoostedClocksEnabled(self.dev)[0]  # returns [isEnabled, isDefaultEnabled]
+            # returns [isEnabled, isDefaultEnabled]
+            self._auto_boost = pynvml.nvmlDeviceGetAutoBoostedClocksEnabled(self.dev)[0]
         except pynvml.NVMLError_NotSupported:
             self._auto_boost = None
 
-        #try to initialize application clocks
+        # try to initialize application clocks
         self.modified_clocks = False
         try:
             if not use_locked_clocks:
-                self.gr_clock_default = pynvml.nvmlDeviceGetDefaultApplicationsClock(self.dev, pynvml.NVML_CLOCK_GRAPHICS)
-                self.mem_clock_default = pynvml.nvmlDeviceGetDefaultApplicationsClock(self.dev, pynvml.NVML_CLOCK_MEM)
+                self.gr_clock_default = pynvml.nvmlDeviceGetDefaultApplicationsClock(
+                    self.dev, pynvml.NVML_CLOCK_GRAPHICS
+                )
+                self.mem_clock_default = pynvml.nvmlDeviceGetDefaultApplicationsClock(
+                    self.dev, pynvml.NVML_CLOCK_MEM
+                )
         except pynvml.NVMLError_NotSupported:
             self.gr_clock_default = None
             self.sm_clock_default = None
             self.mem_clock_default = None
             self.supported_mem_clocks = []
             self.supported_gr_clocks = {}
 
         self.supported_mem_clocks = pynvml.nvmlDeviceGetSupportedMemoryClocks(self.dev)
 
-        #gather the supported gr clocks for each supported mem clock into a dict
+        # gather the supported gr clocks for each supported mem clock into a dict
         self.supported_gr_clocks = {}
         for mem_clock in self.supported_mem_clocks:
-            supported_gr_clocks = pynvml.nvmlDeviceGetSupportedGraphicsClocks(self.dev, mem_clock)
+            supported_gr_clocks = pynvml.nvmlDeviceGetSupportedGraphicsClocks(
+                self.dev, mem_clock
+            )
             self.supported_gr_clocks[mem_clock] = supported_gr_clocks
 
-        #test whether locked gr clocks and mem clocks are supported
+        # test whether locked gr clocks and mem clocks are supported
         self.use_locked_clocks = use_locked_clocks
         if use_locked_clocks:
             try:
-                #try to set highest supported clocks
+                # try to set highest supported clocks
                 mem_clock = self.supported_mem_clocks[0]
                 gr_clock = self.supported_gr_clocks[mem_clock][0]
                 self.set_clocks(mem_clock, gr_clock)
             except pynvml.NVMLError_NotSupported:
-                #switch to using application clocks
+                # switch to using application clocks
                 self.use_locked_clocks = False
 
-
     def __del__(self):
-        #try to restore to defaults
+        # try to restore to defaults
         if self.pwr_limit_default is not None:
             self.pwr_limit = self.pwr_limit_default
         if self.modified_clocks:
             self.reset_clocks()
 
     @property
     def pwr_state(self):
@@ -89,34 +101,45 @@
     def pwr_limit(self):
         """Control the power limit (may require permission), check pwr_constraints for the allowed range"""
         return pynvml.nvmlDeviceGetPowerManagementLimit(self.dev)
 
     @pwr_limit.setter
     def pwr_limit(self, new_limit):
         if not self.pwr_constraints[0] <= new_limit <= self.pwr_constraints[1]:
-            raise ValueError(f"Power limit {new_limit} out of range [{self.pwr_constraints[0]}, {self.pwr_constraints[1]}]")
+            raise ValueError(
+                f"Power limit {new_limit} out of range [{self.pwr_constraints[0]}, {self.pwr_constraints[1]}]"
+            )
         if new_limit == self.pwr_limit:
             return
         try:
             pynvml.nvmlDeviceSetPowerManagementLimit(self.dev, new_limit)
         except pynvml.NVMLError_NoPermission:
             if self.nvidia_smi:
-                new_limit_watt = int(new_limit / 1000.0) # nvidia-smi expects Watts rather than milliwatts
-                args = ["sudo", self.nvidia_smi, "-i", str(self.id), "--power-limit="+str(new_limit_watt)]
+                # nvidia-smi expects Watts rather than milliwatts
+                new_limit_watt = int(new_limit / 1000.0)
+                args = [
+                    "sudo",
+                    self.nvidia_smi,
+                    "-i",
+                    str(self.id),
+                    "--power-limit=" + str(new_limit_watt),
+                ]
                 subprocess.run(args, check=True)
 
     @property
     def persistence_mode(self):
         """Control persistence mode (may require permission), 0 for disabled, 1 for enabled"""
         return self._persistence_mode
 
     @persistence_mode.setter
     def persistence_mode(self, new_mode):
         if not new_mode in [0, 1]:
-            raise ValueError("Illegal value for persistence mode, should be either 0 or 1")
+            raise ValueError(
+                "Illegal value for persistence mode, should be either 0 or 1"
+            )
         pynvml.nvmlDeviceSetPersistenceMode(self.dev, new_mode)
         self._persistence_mode = pynvml.nvmlDeviceGetPersistenceMode(self.dev)
 
     def set_clocks(self, mem_clock, gr_clock):
         """Set the memory and graphics clock for this device (may require permission)"""
         self.modified_clocks = True
         if not mem_clock in self.supported_mem_clocks:
@@ -125,72 +148,119 @@
             raise ValueError("Graphics clock incompatible with memory clock")
         if self.use_locked_clocks:
             try:
                 pynvml.nvmlDeviceSetGpuLockedClocks(self.dev, gr_clock, gr_clock)
                 pynvml.nvmlDeviceSetMemoryLockedClocks(self.dev, mem_clock, mem_clock)
             except pynvml.NVMLError_NoPermission:
                 if self.nvidia_smi:
-                    args = ["sudo", self.nvidia_smi, "-i", str(self.id), "--lock-gpu-clocks="+str(gr_clock)+","+str(gr_clock)]
+                    args = [
+                        "sudo",
+                        self.nvidia_smi,
+                        "-i",
+                        str(self.id),
+                        "--lock-gpu-clocks=" + str(gr_clock) + "," + str(gr_clock),
+                    ]
                     subprocess.run(args, check=True)
-                    args = ["sudo", self.nvidia_smi, "-i", str(self.id), "--lock-memory-clocks="+str(mem_clock)+","+str(mem_clock)]
+                    args = [
+                        "sudo",
+                        self.nvidia_smi,
+                        "-i",
+                        str(self.id),
+                        "--lock-memory-clocks=" + str(mem_clock) + "," + str(mem_clock),
+                    ]
                     subprocess.run(args, check=True)
         else:
             try:
                 pynvml.nvmlDeviceSetApplicationsClocks(self.dev, mem_clock, gr_clock)
             except pynvml.NVMLError_NoPermission:
                 if self.nvidia_smi:
-                    args = ["sudo", self.nvidia_smi, "-i", str(self.id), "--applications-clocks="+str(mem_clock)+","+str(gr_clock)]
+                    args = [
+                        "sudo",
+                        self.nvidia_smi,
+                        "-i",
+                        str(self.id),
+                        "--applications-clocks=" + str(mem_clock) + "," + str(gr_clock),
+                    ]
                     subprocess.run(args, check=True)
 
     def reset_clocks(self):
-        """ Reset the clocks to the default clock if the device uses a non default clock """
+        """Reset the clocks to the default clock if the device uses a non default clock"""
         if self.use_locked_clocks:
             try:
                 pynvml.nvmlDeviceResetGpuLockedClocks(self.dev)
                 pynvml.nvmlDeviceResetMemoryLockedClocks(self.dev)
             except pynvml.NVMLError_NoPermission:
                 if self.nvidia_smi:
-                    args = ["sudo", self.nvidia_smi, "-i", str(self.id), "--reset-gpu-clocks"]
+                    args = [
+                        "sudo",
+                        self.nvidia_smi,
+                        "-i",
+                        str(self.id),
+                        "--reset-gpu-clocks",
+                    ]
                     subprocess.run(args, check=True)
-                    args = ["sudo", self.nvidia_smi, "-i", str(self.id), "--reset-memory-clocks"]
+                    args = [
+                        "sudo",
+                        self.nvidia_smi,
+                        "-i",
+                        str(self.id),
+                        "--reset-memory-clocks",
+                    ]
                     subprocess.run(args, check=True)
 
         elif self.gr_clock_default is not None:
-            gr_app_clock = pynvml.nvmlDeviceGetApplicationsClock(self.dev, pynvml.NVML_CLOCK_GRAPHICS)
-            mem_app_clock = pynvml.nvmlDeviceGetApplicationsClock(self.dev, pynvml.NVML_CLOCK_MEM)
-            if gr_app_clock != self.gr_clock_default or mem_app_clock != self.mem_clock_default:
+            gr_app_clock = pynvml.nvmlDeviceGetApplicationsClock(
+                self.dev, pynvml.NVML_CLOCK_GRAPHICS
+            )
+            mem_app_clock = pynvml.nvmlDeviceGetApplicationsClock(
+                self.dev, pynvml.NVML_CLOCK_MEM
+            )
+            if (
+                gr_app_clock != self.gr_clock_default
+                or mem_app_clock != self.mem_clock_default
+            ):
                 self.set_clocks(self.mem_clock_default, self.gr_clock_default)
 
     @property
     def gr_clock(self):
         """Control the graphics clock (may require permission), only values compatible with the memory clock can be set directly"""
         return pynvml.nvmlDeviceGetClockInfo(self.dev, pynvml.NVML_CLOCK_GRAPHICS)
 
     @gr_clock.setter
     def gr_clock(self, new_clock):
-        cur_clock = pynvml.nvmlDeviceGetClockInfo(self.dev, pynvml.NVML_CLOCK_GRAPHICS) if self.use_locked_clocks else \
-                    pynvml.nvmlDeviceGetApplicationsClock(self.dev, pynvml.NVML_CLOCK_GRAPHICS)
+        cur_clock = (
+            pynvml.nvmlDeviceGetClockInfo(self.dev, pynvml.NVML_CLOCK_GRAPHICS)
+            if self.use_locked_clocks
+            else pynvml.nvmlDeviceGetApplicationsClock(
+                self.dev, pynvml.NVML_CLOCK_GRAPHICS
+            )
+        )
         if new_clock != cur_clock:
             self.set_clocks(self.mem_clock, new_clock)
 
     @property
     def mem_clock(self):
         """Control the memory clock (may require permission), only values compatible with the graphics clock can be set directly"""
         if self.use_locked_clocks:
-            #nvmlDeviceGetClock returns slightly different values than nvmlDeviceGetSupportedMemoryClocks,
-            #therefore set mem_clock to the closest supported value
+            # nvmlDeviceGetClock returns slightly different values than nvmlDeviceGetSupportedMemoryClocks,
+            # therefore set mem_clock to the closest supported value
             mem_clock = pynvml.nvmlDeviceGetClockInfo(self.dev, pynvml.NVML_CLOCK_MEM)
-            return min(self.supported_mem_clocks, key=lambda x:abs(x-mem_clock))
+            return min(self.supported_mem_clocks, key=lambda x: abs(x - mem_clock))
         else:
-            return pynvml.nvmlDeviceGetApplicationsClock(self.dev, pynvml.NVML_CLOCK_MEM)
+            return pynvml.nvmlDeviceGetApplicationsClock(
+                self.dev, pynvml.NVML_CLOCK_MEM
+            )
 
     @mem_clock.setter
     def mem_clock(self, new_clock):
-        cur_clock = pynvml.nvmlDeviceGetClockInfo(self.dev, pynvml.NVML_CLOCK_MEM) if self.use_locked_clocks else \
-                    pynvml.nvmlDeviceGetApplicationsClock(self.dev, pynvml.NVML_CLOCK_MEM)
+        cur_clock = (
+            pynvml.nvmlDeviceGetClockInfo(self.dev, pynvml.NVML_CLOCK_MEM)
+            if self.use_locked_clocks
+            else pynvml.nvmlDeviceGetApplicationsClock(self.dev, pynvml.NVML_CLOCK_MEM)
+        )
         if new_clock != cur_clock:
             self.set_clocks(new_clock, self.gr_clock)
 
     @property
     def temperature(self):
         """Get the GPU temperature"""
         return pynvml.nvmlDeviceGetTemperature(self.dev, pynvml.NVML_TEMPERATURE_GPU)
@@ -198,37 +268,39 @@
     @property
     def auto_boost(self):
         """Control the auto boost setting (may require permission), 0 for disable, 1 for enabled"""
         return self._auto_boost
 
     @auto_boost.setter
     def auto_boost(self, setting):
-        #might need to use pynvml.NVML_FEATURE_DISABLED or pynvml.NVML_FEATURE_ENABLED instead of 0 or 1
+        # might need to use pynvml.NVML_FEATURE_DISABLED or pynvml.NVML_FEATURE_ENABLED instead of 0 or 1
         if not setting in [0, 1]:
-            raise ValueError("Illegal value for auto boost enabled, should be either 0 or 1")
+            raise ValueError(
+                "Illegal value for auto boost enabled, should be either 0 or 1"
+            )
         pynvml.nvmlDeviceSetAutoBoostedClocksEnabled(self.dev, setting)
         self._auto_boost = pynvml.nvmlDeviceGetAutoBoostedClocksEnabled(self.dev)[0]
 
     def pwr_usage(self):
         """Return current power usage in milliwatts"""
         return pynvml.nvmlDeviceGetPowerUsage(self.dev)
 
     def gr_voltage(self):
         """Return current graphics voltage in millivolts"""
-        args = ["nvidia-smi", "-i", str(self.id), "-q",  "-d", "VOLTAGE"]
+        args = ["nvidia-smi", "-i", str(self.id), "-q", "-d", "VOLTAGE"]
         try:
             result = subprocess.run(args, check=True, capture_output=True)
             m = re.search(r"(\d+\.\d+) mV", result.stdout.decode())
             return float(m.group(1))
         except:
             return np.nan
 
 
 class NVMLObserver(BenchmarkObserver):
-    """ Observer that uses NVML to monitor power, energy, clock frequencies, voltages and temperature
+    """Observer that uses NVML to monitor power, energy, clock frequencies, voltages and temperature
 
     The NVMLObserver can also be used to tune application-specific clock frequencies or power limits
     in combination with other parameters.
 
     :param observables: List of quantities that should be observed during tuning, supported are: "power_readings",
         "nvml_power", "nvml_energy", "core_freq", "mem_freq", "temperature", "gr_voltage". If you want to measure the average power
         consumption of a GPU kernel executing on the GPU use "nvml_power". The "power_readings" are the individual power readings
@@ -253,130 +325,168 @@
     :type use_locked_clocks: boolean
 
     :param continuous_duration: Duration to use for energy/power measurements in seconds, default 1 second.
     :type continuous_duration: float
 
     """
 
-    def __init__(self, observables, device=0, save_all=False, nvidia_smi_fallback=None, use_locked_clocks=False, continous_duration=1):
+    def __init__(
+        self,
+        observables,
+        device=0,
+        save_all=False,
+        nvidia_smi_fallback=None,
+        use_locked_clocks=False,
+        continous_duration=1,
+    ):
         """
 
         Create an NVMLObserver.
 
 
         """
         if nvidia_smi_fallback:
-            self.nvml = nvml(device, nvidia_smi_fallback=nvidia_smi_fallback, use_locked_clocks=use_locked_clocks)
+            self.nvml = nvml(
+                device,
+                nvidia_smi_fallback=nvidia_smi_fallback,
+                use_locked_clocks=use_locked_clocks,
+            )
         else:
             self.nvml = nvml(device, use_locked_clocks=use_locked_clocks)
         self.save_all = save_all
 
-        supported = ["power_readings", "nvml_power", "nvml_energy", "core_freq", "mem_freq", "temperature", "gr_voltage"]
+        supported = [
+            "power_readings",
+            "nvml_power",
+            "nvml_energy",
+            "core_freq",
+            "mem_freq",
+            "temperature",
+            "gr_voltage",
+        ]
         for obs in observables:
             if not obs in supported:
                 raise ValueError(f"Observable {obs} not in supported: {supported}")
         self.observables = observables
 
         self.measure_power = False
         self.needs_power = ["power_readings", "nvml_power", "nvml_energy"]
         if any([obs in self.needs_power for obs in observables]):
             self.measure_power = True
             power_observables = [obs for obs in observables if obs in self.needs_power]
-            self.continuous_observer = NVMLPowerObserver(power_observables, self, self.nvml, continous_duration)
+            self.continuous_observer = NVMLPowerObserver(
+                power_observables, self, self.nvml, continous_duration
+            )
 
-        #remove power observables
+        # remove power observables
         self.observables = [obs for obs in observables if obs not in self.needs_power]
 
         self.record_gr_voltage = False
         self.t0 = 0
         if "gr_voltage" in observables:
             self.record_gr_voltage = True
             self.gr_voltage_readings = []
 
         self.results = {}
         for obs in self.observables:
             self.results[obs + "s"] = []
 
-        self.during_obs = [obs for obs in observables if obs in ["core_freq", "mem_freq", "temperature"]]
-        self.iteration = {obs:[] for obs in self.during_obs}
+        self.during_obs = [
+            obs
+            for obs in observables
+            if obs in ["core_freq", "mem_freq", "temperature"]
+        ]
+        self.iteration = {obs: [] for obs in self.during_obs}
 
     def before_start(self):
-        #clear results of the observables for next measurement
-        self.iteration = {obs:[] for obs in self.during_obs}
+        # clear results of the observables for next measurement
+        self.iteration = {obs: [] for obs in self.during_obs}
         if self.record_gr_voltage:
             self.gr_voltage_readings = []
 
     def after_start(self):
         self.t0 = time.perf_counter()
-        self.during() # ensure during is called at least once
+        # ensure during is called at least once
+        self.during()
 
     def during(self):
         if "temperature" in self.observables:
             self.iteration["temperature"].append(self.nvml.temperature)
         if "core_freq" in self.observables:
             self.iteration["core_freq"].append(self.nvml.gr_clock)
         if "mem_freq" in self.observables:
             self.iteration["mem_freq"].append(self.nvml.mem_clock)
         if self.record_gr_voltage:
-            self.gr_voltage_readings.append([time.perf_counter()-self.t0, self.nvml.gr_voltage()])
+            self.gr_voltage_readings.append(
+                [time.perf_counter() - self.t0, self.nvml.gr_voltage()]
+            )
 
     def after_finish(self):
         if "temperature" in self.observables:
-            self.results["temperatures"].append(np.average(self.iteration["temperature"]))
+            self.results["temperatures"].append(
+                np.average(self.iteration["temperature"])
+            )
         if "core_freq" in self.observables:
             self.results["core_freqs"].append(np.average(self.iteration["core_freq"]))
         if "mem_freq" in self.observables:
             self.results["mem_freqs"].append(np.average(self.iteration["mem_freq"]))
 
         if "gr_voltage" in self.observables:
             execution_time = time.time() - self.t0
             gr_voltage_readings = self.gr_voltage_readings
-            gr_voltage_readings = [[0.0, gr_voltage_readings[0][1]]] + gr_voltage_readings
-            gr_voltage_readings = gr_voltage_readings + [[execution_time, gr_voltage_readings[-1][1]]]
-            self.results["gr_voltages"].append(np.average(gr_voltage_readings[:][:][1])) #time in s, graphics voltage in millivolts
+            gr_voltage_readings = [
+                [0.0, gr_voltage_readings[0][1]]
+            ] + gr_voltage_readings
+            gr_voltage_readings = gr_voltage_readings + [
+                [execution_time, gr_voltage_readings[-1][1]]
+            ]
+            # time in s, graphics voltage in millivolts
+            self.results["gr_voltages"].append(np.average(gr_voltage_readings[:][:][1]))
 
     def get_results(self):
         averaged_results = {}
 
-        #return averaged results, except when save_all is True
+        # return averaged results, except when save_all is True
         for obs in self.observables:
-            #save all information, if the user requested
+            # save all information, if the user requested
             if self.save_all:
                 averaged_results[obs + "s"] = self.results[obs + "s"]
-            #save averaged results, default
+            # save averaged results, default
             averaged_results[obs] = np.average(self.results[obs + "s"])
 
-        #clear results for next round
+        # clear results for next round
         for obs in self.observables:
             self.results[obs + "s"] = []
 
         return averaged_results
 
 
-
 class NVMLPowerObserver(ContinuousObserver):
-    """ Observer that measures power using NVML and continuous benchmarking """
+    """Observer that measures power using NVML and continuous benchmarking"""
+
     def __init__(self, observables, parent, nvml_instance, continous_duration=1):
         self.parent = parent
         self.nvml = nvml_instance
 
         supported = ["power_readings", "nvml_power", "nvml_energy"]
         for obs in observables:
             if obs not in supported:
                 raise ValueError(f"Observable {obs} not in supported: {supported}")
         self.observables = observables
 
-        self.continuous_duration = continous_duration # seconds
+        # duration in seconds
+        self.continuous_duration = continous_duration
 
         self.power = 0
         self.energy = 0
         self.power_readings = []
         self.t0 = 0
 
-        self.results = None # results from the last iteration-based benchmark
+        # results from the last iteration-based benchmark
+        self.results = None
 
     def before_start(self):
         self.parent.before_start()
         self.power = 0
         self.energy = 0
         self.power_readings = []
 
@@ -385,109 +495,112 @@
         self.t0 = time.perf_counter()
 
     def during(self):
         self.parent.during()
         power_usage = self.nvml.pwr_usage()
         timestamp = time.perf_counter() - self.t0
         # only store the result if we get a new measurement from NVML
-        if len(self.power_readings) == 0 or (self.power_readings[-1][1] != power_usage or timestamp-self.power_readings[-1][0] > 0.01):
+        if len(self.power_readings) == 0 or (
+            self.power_readings[-1][1] != power_usage
+            or timestamp - self.power_readings[-1][0] > 0.01
+        ):
             self.power_readings.append([timestamp, power_usage])
 
     def after_finish(self):
         self.parent.after_finish()
         # safeguard in case we have no measurements, perhaps the kernel was too short to measure anything
         if not self.power_readings:
             return
 
-        execution_time = (self.results["time"]/1e3) # converted to seconds from milliseconds
+        # convert to seconds from milliseconds
+        execution_time = self.results["time"] / 1e3
         self.power = np.median([d[1] / 1e3 for d in self.power_readings])
         self.energy = self.power * execution_time
 
-
     def get_results(self):
         results = self.parent.get_results()
         keys = list(results.keys())
         for key in keys:
             results["pwr_" + key] = results.pop(key)
         if "nvml_energy" in self.observables:
             results["nvml_energy"] = self.energy
         if "nvml_power" in self.observables:
             results["nvml_power"] = self.power
         if "power_readings" in self.observables:
             results["power_readings"] = self.power_readings
         return results
 
 
-
 # High-level Helper functions
 
+
 def get_nvml_pwr_limits(device, n=None, quiet=False):
-    """ Get tunable parameter for NVML power limits, n is desired number of values """
+    """Get tunable parameter for NVML power limits, n is desired number of values"""
 
     d = nvml(device)
     power_limits = d.pwr_constraints
     power_limit_min = power_limits[0]
     power_limit_max = power_limits[-1]
-    power_limit_min *= 1e-3  # Convert to Watt
-    power_limit_max *= 1e-3  # Convert to Watt
+    # Min and Max converted to Watt
+    power_limit_min *= 1e-3
+    power_limit_max *= 1e-3
     power_limit_round = 5
     tune_params = OrderedDict()
     if n == None:
-        n = int((power_limit_max - power_limit_min) / power_limit_round)+1
+        n = int((power_limit_max - power_limit_min) / power_limit_round) + 1
 
     # Rounded power limit values
     power_limits = power_limit_round * np.round(
-        (np.linspace(power_limit_min, power_limit_max, n) / power_limit_round))
-    power_limits = sorted(
-        list(set([int(power_limit) for power_limit in power_limits])))
+        (np.linspace(power_limit_min, power_limit_max, n) / power_limit_round)
+    )
+    power_limits = sorted(list(set([int(power_limit) for power_limit in power_limits])))
     tune_params["nvml_pwr_limit"] = power_limits
 
     if not quiet:
         print("Using power limits:", tune_params["nvml_pwr_limit"])
     return tune_params
 
 
 def get_nvml_gr_clocks(device, n=None, quiet=False):
-    """ Get tunable parameter for NVML graphics clock, n is desired number of values """
+    """Get tunable parameter for NVML graphics clock, n is desired number of values"""
 
     d = nvml(device)
     mem_clock = max(d.supported_mem_clocks)
     gr_clocks = d.supported_gr_clocks[mem_clock]
 
     if n and (len(gr_clocks) > n):
-        indices = np.array(
-            np.ceil(np.linspace(0, len(gr_clocks)-1, n)), dtype=int)
+        indices = np.array(np.ceil(np.linspace(0, len(gr_clocks) - 1, n)), dtype=int)
         gr_clocks = np.array(gr_clocks)[indices]
 
     tune_params = OrderedDict()
     tune_params["nvml_gr_clock"] = list(gr_clocks)
 
     if not quiet:
         print("Using gr frequencies:", tune_params["nvml_gr_clock"])
     return tune_params
 
 
 def get_nvml_mem_clocks(device, n=None, quiet=False):
-    """ Get tunable parameter for NVML memory clock, n is desired number of values """
+    """Get tunable parameter for NVML memory clock, n is desired number of values"""
 
     d = nvml(device)
     mem_clocks = d.supported_mem_clocks
 
     if n and len(mem_clocks) > n:
-        mem_clocks = mem_clocks[::int(len(mem_clocks)/n)]
+        mem_clocks = mem_clocks[:: int(len(mem_clocks) / n)]
 
     tune_params = OrderedDict()
     tune_params["nvml_mem_clock"] = mem_clocks
 
     if not quiet:
         print("Using mem frequencies:", tune_params["nvml_mem_clock"])
     return tune_params
 
 
 def get_idle_power(device, n=5, sleep_s=0.1):
-    """ Use NVML to measure device idle power consumption """
+    """Use NVML to measure device idle power consumption"""
     d = nvml(device)
     readings = []
     for _ in range(n):
         time.sleep(sleep_s)
         readings.append(d.pwr_usage())
     return np.mean(readings) * 1e-3  # Watt
```

## Comparing `kernel_tuner/opencl.py` & `kernel_tuner/backends/opencl.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,71 +1,64 @@
 """This module contains all OpenCL specific kernel_tuner functions"""
 from __future__ import print_function
 import time
 import numpy as np
 
-from kernel_tuner.observers import BenchmarkObserver
+from kernel_tuner.backends.backend import GPUBackend
+from kernel_tuner.observers.opencl import OpenCLObserver
 
-#embedded in try block to be able to generate documentation
+# embedded in try block to be able to generate documentation
 try:
     import pyopencl as cl
 except ImportError:
     cl = None
 
 
-class OpenCLObserver(BenchmarkObserver):
-    """ Observer that measures time using CUDA events during benchmarking """
-    def __init__(self, dev):
-        self.dev = dev
-        self.times = []
-
-    def after_finish(self):
-        event = self.dev.event
-        self.times.append((event.profile.end - event.profile.start)*1e-6) #ms
-
-    def get_results(self):
-        results = {"time": np.average(self.times), "times": self.times.copy()}
-        self.times = []
-        return results
-
-
-class OpenCLFunctions():
+class OpenCLFunctions(GPUBackend):
     """Class that groups the OpenCL functions on maintains some state about the device"""
 
-    def __init__(self, device=0, platform=0, iterations=7, compiler_options=None, observers=None):
+    def __init__(
+        self, device=0, platform=0, iterations=7, compiler_options=None, observers=None
+    ):
         """Creates OpenCL device context and reads device properties
 
         :param device: The ID of the OpenCL device to use for benchmarking
         :type device: int
 
         :param iterations: The number of iterations to run the kernel during benchmarking, 7 by default.
         :type iterations: int
         """
         if not cl:
-            raise ImportError("Error: pyopencl not installed, please install e.g. using 'pip install pyopencl'.")
+            raise ImportError(
+                "Error: pyopencl not installed, please install e.g. using 'pip install pyopencl'."
+            )
 
         self.iterations = iterations
-        #setup context and queue
+        # setup context and queue
         platforms = cl.get_platforms()
         self.ctx = cl.Context(devices=[platforms[platform].get_devices()[device]])
 
-        self.queue = cl.CommandQueue(self.ctx, properties=cl.command_queue_properties.PROFILING_ENABLE)
+        self.queue = cl.CommandQueue(
+            self.ctx, properties=cl.command_queue_properties.PROFILING_ENABLE
+        )
         self.mf = cl.mem_flags
-        #inspect device properties
-        self.max_threads = self.ctx.devices[0].get_info(cl.device_info.MAX_WORK_GROUP_SIZE)
+        # inspect device properties
+        self.max_threads = self.ctx.devices[0].get_info(
+            cl.device_info.MAX_WORK_GROUP_SIZE
+        )
         self.compiler_options = compiler_options or []
 
-        #observer stuff
+        # observer stuff
         self.observers = observers or []
         self.observers.append(OpenCLObserver(self))
         self.event = None
         for obs in self.observers:
             obs.register_device(self)
 
-        #collect environment information
+        # collect environment information
         dev = self.ctx.devices[0]
         env = dict()
         env["platform_name"] = dev.platform.name
         env["platform_version"] = dev.platform.version
         env["device_name"] = dev.name
         env["device_version"] = dev.version
         env["opencl_c_version"] = dev.opencl_c_version
@@ -86,16 +79,23 @@
         :returns: A list of arguments that can be passed to an OpenCL kernel.
         :rtype: list( pyopencl.Buffer, numpy.int32, ... )
         """
         gpu_args = []
         for arg in arguments:
             # if arg i is a numpy array copy to device
             if isinstance(arg, np.ndarray):
-                gpu_args.append(cl.Buffer(self.ctx, self.mf.READ_WRITE | self.mf.COPY_HOST_PTR, hostbuf=arg))
-            else: # if not an array, just pass argument along
+                gpu_args.append(
+                    cl.Buffer(
+                        self.ctx,
+                        self.mf.READ_WRITE | self.mf.COPY_HOST_PTR,
+                        hostbuf=arg,
+                    )
+                )
+            # if not an array, just pass argument along
+            else:
                 gpu_args.append(arg)
         return gpu_args
 
     def compile(self, kernel_instance):
         """call the OpenCL compiler to compile the kernel, return the device function
 
         :param kernel_name: The name of the kernel to be compiled, used to lookup the
@@ -104,40 +104,40 @@
 
         :param kernel_string: The OpenCL kernel code that contains the function `kernel_name`
         :type kernel_string: string
 
         :returns: An OpenCL kernel that can be called directly.
         :rtype: pyopencl.Kernel
         """
-        prg = cl.Program(self.ctx, kernel_instance.kernel_string).build(options=self.compiler_options)
+        prg = cl.Program(self.ctx, kernel_instance.kernel_string).build(
+            options=self.compiler_options
+        )
         func = getattr(prg, kernel_instance.name)
         return func
 
-
     def start_event(self):
-        """ Records the event that marks the start of a measurement
+        """Records the event that marks the start of a measurement
 
-        In OpenCL the event is created when the kernel is launched """
+        In OpenCL the event is created when the kernel is launched"""
         pass
 
     def stop_event(self):
-        """ Records the event that marks the end of a measurement
+        """Records the event that marks the end of a measurement
 
-        In OpenCL the event is created when the kernel is launched """
+        In OpenCL the event is created when the kernel is launched"""
         pass
 
     def kernel_finished(self):
-        """ Returns True if the kernel has finished, False otherwise """
+        """Returns True if the kernel has finished, False otherwise"""
         return self.event.get_info(cl.event_info.COMMAND_EXECUTION_STATUS) == 0
 
     def synchronize(self):
-        """ Halts execution until device has finished its tasks """
+        """Halts execution until device has finished its tasks"""
         self.queue.finish()
 
-
     def run_kernel(self, func, gpu_args, threads, grid):
         """runs the OpenCL kernel passed as 'func'
 
         :param func: An OpenCL Kernel
         :type func: pyopencl.Kernel
 
         :param gpu_args: A list of arguments to the kernel, order should match the
@@ -149,15 +149,15 @@
             the work group.
         :type threads: tuple(int, int, int)
 
         :param grid: A tuple listing the number of work groups in each dimension
             of the NDRange.
         :type grid: tuple(int, int)
         """
-        global_size = (grid[0]*threads[0], grid[1]*threads[1], grid[2]*threads[2])
+        global_size = (grid[0] * threads[0], grid[1] * threads[1], grid[2] * threads[2])
         local_size = threads
         self.event = func(self.queue, global_size, local_size, *gpu_args)
 
     def memset(self, buffer, value, size):
         """set the memory in allocation to the value in value
 
         :param allocation: An OpenCL Buffer to fill
@@ -170,15 +170,15 @@
         :type size: int
 
         """
         if isinstance(buffer, cl.Buffer):
             try:
                 cl.enqueue_fill_buffer(self.queue, buffer, np.uint32(value), 0, size)
             except AttributeError:
-                src=np.zeros(size, dtype='uint8')+np.uint8(value)
+                src = np.zeros(size, dtype="uint8") + np.uint8(value)
                 cl.enqueue_copy(self.queue, buffer, src)
 
     def memcpy_dtoh(self, dest, src):
         """perform a device to host memory copy
 
         :param dest: A numpy array in host memory to store the data
         :type dest: numpy.ndarray
@@ -197,8 +197,17 @@
 
         :param src: A numpy array in host memory to store the data
         :type src: numpy.ndarray
         """
         if isinstance(dest, cl.Buffer):
             cl.enqueue_copy(self.queue, dest, src)
 
-    units = {'time': 'ms'}
+    def copy_constant_memory_args(self, cmem_args):
+        raise NotImplementedError("PyOpenCL backend does not support constant memory")
+
+    def copy_shared_memory_args(self, smem_args):
+        raise NotImplementedError("PyOpenCL backend does not support shared memory")
+
+    def copy_texture_memory_args(self, texmem_args):
+        raise NotImplementedError("PyOpenCL backend does not support texture memory")
+
+    units = {"time": "ms"}
```

## Comparing `kernel_tuner/pycuda.py` & `kernel_tuner/backends/pycuda.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,27 +1,28 @@
 """This module contains all CUDA specific kernel_tuner functions"""
 from __future__ import print_function
 
 import logging
 import time
 import numpy as np
 
-from kernel_tuner.observers import BenchmarkObserver
-from kernel_tuner.nvml import nvml
+from kernel_tuner.backends.backend import GPUBackend
+from kernel_tuner.observers.pycuda import PyCudaRuntimeObserver
+from kernel_tuner.observers.nvml import nvml
 from kernel_tuner.util import TorchPlaceHolder, SkippableFailure
 
-#embedded in try block to be able to generate documentation
-#and run tests without pycuda installed
+# embedded in try block to be able to generate documentation
+# and run tests without pycuda installed
 try:
     import pycuda.driver as drv
+
     pycuda_available = True
 except ImportError:
 
-    class PyCudaPlaceHolder():
-
+    class PyCudaPlaceHolder:
         def __init__(self):
             self.PointerHolderBase = object
 
     drv = PyCudaPlaceHolder()
     pycuda_available = False
 
 try:
@@ -36,48 +37,26 @@
 try:
     import torch
 except ImportError:
     torch = TorchPlaceHolder()
 
 
 class Holder(drv.PointerHolderBase):
-    """ class to interoperate torch device memory allocations with PyCUDA """
+    """class to interoperate torch device memory allocations with PyCUDA"""
 
     def __init__(self, tensor):
         super(Holder, self).__init__()
         self.tensor = tensor
         self.gpudata = tensor.data_ptr()
 
     def get_pointer(self):
         return self.t.data_ptr()
 
 
-class PyCudaRuntimeObserver(BenchmarkObserver):
-    """ Observer that measures time using CUDA events during benchmarking """
-
-    def __init__(self, dev):
-        self.dev = dev
-        self.stream = dev.stream
-        self.start = dev.start
-        self.end = dev.end
-        self.times = []
-
-    def after_finish(self):
-        self.times.append(self.end.time_since(self.start))    #ms
-
-    def get_results(self):
-        results = {
-            "time": np.average(self.times),
-            "times": self.times.copy()
-        }
-        self.times = []
-        return results
-
-
-class PyCudaFunctions(object):
+class PyCudaFunctions(GPUBackend):
     """Class that groups the CUDA functions on maintains state about the device"""
 
     def __init__(self, device=0, iterations=7, compiler_options=None, observers=None):
         """instantiate PyCudaFunctions object used for interacting with the CUDA device
 
         Instantiating this object will inspect and store certain device properties at
         runtime, which are used during compilation and/or execution of kernels by the
@@ -88,84 +67,94 @@
         :type device: int
 
         :param iterations: Number of iterations used while benchmarking a kernel, 7 by default.
         :type iterations: int
         """
         self.allocations = []
         self.texrefs = []
-        if not pycuda_available and isinstance(drv, PyCudaPlaceHolder):    #and part to allow mocking
-            raise ImportError("Error: pycuda not installed, please install e.g. using 'pip install pycuda'.")
+        # if not PyCuda available, check if mocking before raising exception
+        if not pycuda_available and isinstance(drv, PyCudaPlaceHolder):
+            raise ImportError(
+                "Error: pycuda not installed, please install e.g. using 'pip install pycuda'."
+            )
 
         drv.init()
         self.context = drv.Device(device).retain_primary_context()
         if PyCudaFunctions.last_selected_device != device:
             # pycuda does not wrap cuCtxSetCurrent.
             # As an approximation we push the new device's primary context
             # when switching to a different device.
             if PyCudaFunctions.last_selected_context is not None:
                 PyCudaFunctions.last_selected_context.pop()
             else:
                 import atexit
+
                 def _finish_up():
                     PyCudaFunctions.last_selected_context.pop()
+
                 atexit.register(_finish_up)
             self.context.push()
             PyCudaFunctions.last_selected_device = device
             PyCudaFunctions.last_selected_context = self.context
 
-        #inspect device properties
-        devprops = {str(k): v
-                    for (k, v) in self.context.get_device().get_attributes().items()}
-        self.max_threads = devprops['MAX_THREADS_PER_BLOCK']
-        cc = str(devprops.get('COMPUTE_CAPABILITY_MAJOR', '0')) + str(devprops.get('COMPUTE_CAPABILITY_MINOR', '0'))
+        # inspect device properties
+        devprops = {
+            str(k): v for (k, v) in self.context.get_device().get_attributes().items()
+        }
+        self.max_threads = devprops["MAX_THREADS_PER_BLOCK"]
+        cc = str(devprops.get("COMPUTE_CAPABILITY_MAJOR", "0")) + str(
+            devprops.get("COMPUTE_CAPABILITY_MINOR", "0")
+        )
         if cc == "00":
             cc = self.context.get_device().compute_capability()
         self.cc = str(cc[0]) + str(cc[1])
         self.iterations = iterations
         self.current_module = None
         self.func = None
         self.compiler_options = compiler_options or []
 
-        #select PyCUDA source module
+        # select PyCUDA source module
         if int(self.cc) >= 35:
             self.source_mod = DynamicSourceModule
         else:
             self.source_mod = SourceModule
         if not self.source_mod:
             raise ImportError(
-                "Error: pycuda not correctly installed, please ensure pycuda is installed on the same CUDA installation as you're using right now")
+                "Error: pycuda not correctly installed, please ensure pycuda is installed on the same CUDA installation as you're using right now"
+            )
 
-        #create a stream and events
+        # create a stream and events
         self.stream = drv.Stream()
         self.start = drv.Event()
         self.end = drv.Event()
 
-        #default dynamically allocated shared memory size, can be overwritten using smem_args
+        # default dynamically allocated shared memory size, can be overwritten using smem_args
         self.smem_size = 0
 
-        #setup observers
+        # setup observers
         self.observers = observers or []
         self.observers.append(PyCudaRuntimeObserver(self))
         for obs in self.observers:
             obs.register_device(self)
 
-        #collect environment information
+        # collect environment information
         env = dict()
         env["device_name"] = self.context.get_device().name()
         env["cuda_version"] = ".".join([str(i) for i in drv.get_version()])
         env["compute_capability"] = self.cc
         env["iterations"] = self.iterations
         env["compiler_options"] = compiler_options
         env["device_properties"] = devprops
         self.env = env
         self.name = env["device_name"]
 
     def __del__(self):
         for gpu_mem in self.allocations:
-            if hasattr(gpu_mem, 'free'):    #if needed for when using mocks during testing
+            # if needed for when using mocks during testing
+            if hasattr(gpu_mem, "free"):
                 gpu_mem.free()
 
     def ready_argument_list(self, arguments):
         """ready argument list to be passed to the kernel, allocates gpu mem
 
         :param arguments: List of arguments to be passed to the kernel.
             The order should match the argument list on the CUDA kernel.
@@ -187,15 +176,16 @@
                 if arg.is_cuda:
                     gpu_args.append(Holder(arg))
                 else:
                     gpu_args.append(Holder(arg.cuda()))
             # pycuda does not support bool, convert to uint8 instead
             elif isinstance(arg, np.bool_):
                 gpu_args.append(arg.astype(np.uint8))
-            else:    # if not an array, just pass argument along
+            # if not an array, just pass argument along
+            else:
                 gpu_args.append(arg)
         return gpu_args
 
     def compile(self, kernel_instance):
         """call the CUDA compiler to compile the kernel, return the device function
 
         :param kernel_name: The name of the kernel to be compiled, used to lookup the
@@ -210,64 +200,67 @@
         """
         kernel_string = kernel_instance.kernel_string
         kernel_name = kernel_instance.name
 
         try:
             no_extern_c = 'extern "C"' in kernel_string
 
-            compiler_options = ['-Xcompiler=-Wall']
+            compiler_options = ["-Xcompiler=-Wall"]
             if self.compiler_options:
                 compiler_options += self.compiler_options
 
-            self.current_module = self.source_mod(kernel_string, options=compiler_options + ["-e", kernel_name],
-                                                  arch=('compute_' + self.cc) if self.cc != "00" else None, code=('sm_' + self.cc) if self.cc != "00" else None,
-                                                  cache_dir=False, no_extern_c=no_extern_c)
+            self.current_module = self.source_mod(
+                kernel_string,
+                options=compiler_options + ["-e", kernel_name],
+                arch=("compute_" + self.cc) if self.cc != "00" else None,
+                code=("sm_" + self.cc) if self.cc != "00" else None,
+                cache_dir=False,
+                no_extern_c=no_extern_c,
+            )
 
             self.func = self.current_module.get_function(kernel_name)
             return self.func
         except drv.CompileError as e:
             if "uses too much shared data" in e.stderr:
                 raise SkippableFailure("uses too much shared data")
             else:
                 raise e
 
-
     def start_event(self):
-        """ Records the event that marks the start of a measurement """
+        """Records the event that marks the start of a measurement"""
         self.start.record(stream=self.stream)
 
     def stop_event(self):
-        """ Records the event that marks the end of a measurement """
+        """Records the event that marks the end of a measurement"""
         self.end.record(stream=self.stream)
 
     def kernel_finished(self):
-        """ Returns True if the kernel has finished, False otherwise """
+        """Returns True if the kernel has finished, False otherwise"""
         return self.end.query()
 
     def synchronize(self):
-        """ Halts execution until device has finished its tasks """
+        """Halts execution until device has finished its tasks"""
         self.context.synchronize()
 
-
     def copy_constant_memory_args(self, cmem_args):
         """adds constant memory arguments to the most recently compiled module
 
         :param cmem_args: A dictionary containing the data to be passed to the
             device constant memory. The format to be used is as follows: A
             string key is used to name the constant memory symbol to which the
             value needs to be copied. Similar to regular arguments, these need
             to be numpy objects, such as numpy.ndarray or numpy.int32, and so on.
         :type cmem_args: dict( string: numpy.ndarray, ... )
         """
-        logging.debug('copy_constant_memory_args called')
-        logging.debug('current module: ' + str(self.current_module))
+        logging.debug("copy_constant_memory_args called")
+        logging.debug("current module: " + str(self.current_module))
         for k, v in cmem_args.items():
             symbol = self.current_module.get_global(k)[0]
-            logging.debug('copying to symbol: ' + str(symbol))
-            logging.debug('array to be copied: ')
+            logging.debug("copying to symbol: " + str(symbol))
+            logging.debug("array to be copied: ")
             logging.debug(v.nbytes)
             logging.debug(v.dtype)
             logging.debug(v.flags)
             drv.memcpy_htod(symbol, v)
 
     def copy_shared_memory_args(self, smem_args):
         """add shared memory arguments to the kernel"""
@@ -278,62 +271,62 @@
 
         :param texmem_args: A dictionary containing the data to be passed to the
             device texture memory. See tune_kernel().
         :type texmem_args: dict
         """
 
         filter_mode_map = {
-            'point': drv.filter_mode.POINT,
-            'linear': drv.filter_mode.LINEAR
+            "point": drv.filter_mode.POINT,
+            "linear": drv.filter_mode.LINEAR,
         }
         address_mode_map = {
-            'border': drv.address_mode.BORDER,
-            'clamp': drv.address_mode.CLAMP,
-            'mirror': drv.address_mode.MIRROR,
-            'wrap': drv.address_mode.WRAP
+            "border": drv.address_mode.BORDER,
+            "clamp": drv.address_mode.CLAMP,
+            "mirror": drv.address_mode.MIRROR,
+            "wrap": drv.address_mode.WRAP,
         }
 
-        logging.debug('copy_texture_memory_args called')
-        logging.debug('current module: ' + str(self.current_module))
+        logging.debug("copy_texture_memory_args called")
+        logging.debug("current module: " + str(self.current_module))
         self.texrefs = []
         for k, v in texmem_args.items():
             tex = self.current_module.get_texref(k)
             self.texrefs.append(tex)
 
-            logging.debug('copying to texture: ' + str(k))
+            logging.debug("copying to texture: " + str(k))
             if not isinstance(v, dict):
                 data = v
             else:
-                data = v['array']
-            logging.debug('texture to be copied: ')
+                data = v["array"]
+            logging.debug("texture to be copied: ")
             logging.debug(data.nbytes)
             logging.debug(data.dtype)
             logging.debug(data.flags)
 
             drv.matrix_to_texref(data, tex, order="C")
 
             if isinstance(v, dict):
-                if 'address_mode' in v and v['address_mode'] is not None:
+                if "address_mode" in v and v["address_mode"] is not None:
                     # address_mode is set per axis
-                    amode = v['address_mode']
+                    amode = v["address_mode"]
                     if not isinstance(amode, list):
                         amode = [amode] * data.ndim
                     for i, m in enumerate(amode):
                         try:
                             if m is not None:
                                 tex.set_address_mode(i, address_mode_map[m])
                         except KeyError:
-                            raise ValueError('Unknown address mode: ' + m)
-                if 'filter_mode' in v and v['filter_mode'] is not None:
-                    fmode = v['filter_mode']
+                            raise ValueError("Unknown address mode: " + m)
+                if "filter_mode" in v and v["filter_mode"] is not None:
+                    fmode = v["filter_mode"]
                     try:
                         tex.set_filter_mode(filter_mode_map[fmode])
                     except KeyError:
-                        raise ValueError('Unknown filter mode: ' + fmode)
-                if 'normalized_coordinates' in v and v['normalized_coordinates']:
+                        raise ValueError("Unknown filter mode: " + fmode)
+                if "normalized_coordinates" in v and v["normalized_coordinates"]:
                     tex.set_flags(tex.get_flags() | drv.TRSF_NORMALIZED_COORDINATES)
 
     def run_kernel(self, func, gpu_args, threads, grid, stream=None):
         """runs the CUDA kernel passed as 'func'
 
         :param func: A PyCuda kernel compiled for this specific kernel configuration
         :type func: pycuda.driver.Function
@@ -349,15 +342,22 @@
 
         :param grid: A tuple listing the number of thread blocks in each dimension
             of the grid
         :type grid: tuple(int, int)
         """
         if stream is None:
             stream = self.stream
-        func(*gpu_args, block=threads, grid=grid, stream=stream, shared=self.smem_size, texrefs=self.texrefs)
+        func(
+            *gpu_args,
+            block=threads,
+            grid=grid,
+            stream=stream,
+            shared=self.smem_size,
+            texrefs=self.texrefs
+        )
 
     def memset(self, allocation, value, size):
         """set the memory in allocation to the value in value
 
         :param allocation: A GPU memory allocation unit
         :type allocation: pycuda.driver.DeviceAllocation
 
@@ -392,15 +392,11 @@
 
         :param src: A numpy array in host memory to store the data
         :type src: numpy.ndarray
         """
         if isinstance(dest, drv.DeviceAllocation):
             drv.memcpy_htod(dest, src)
 
-    units = {
-        'time': 'ms',
-        'power': 's,mW',
-        'energy': 'J'
-    }
+    units = {"time": "ms", "power": "s,mW", "energy": "J"}
 
     last_selected_device = None
     last_selected_context = None
```

## Comparing `kernel_tuner-0.4.4.dist-info/LICENSE` & `kernel_tuner-0.4.5.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `kernel_tuner-0.4.4.dist-info/METADATA` & `kernel_tuner-0.4.5.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: kernel-tuner
-Version: 0.4.4
+Version: 0.4.5
 Summary: An easy to use CUDA/OpenCL kernel tuner in Python
 Home-page: https://KernelTuner.github.io/kernel_tuner/
 Author: Ben van Werkhoven
 Author-email: b.vanwerkhoven@esciencecenter.nl
 License: Apache 2.0
 Project-URL: Documentation, https://KernelTuner.github.io/kernel_tuner/
 Project-URL: Source, https://github.com/KernelTuner/kernel_tuner
```

