# Comparing `tmp/pytorch_clip_interrogator-2023.2.20.1-py3-none-any.whl.zip` & `tmp/pytorch_clip_interrogator-2023.5.30.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,19 +1,19 @@
-Zip file size: 789124 bytes, number of entries: 17
--rw-rw-r--  2.0 unx      604 b- defN 23-Feb-19 17:24 pytorch_clip_interrogator/__init__.py
--rw-rw-r--  2.0 unx     3249 b- defN 23-Feb-20 14:59 pytorch_clip_interrogator/blip.py
--rw-rw-r--  2.0 unx     4237 b- defN 23-Feb-20 11:34 pytorch_clip_interrogator/clip_interrogator.py
--rw-rw-r--  2.0 unx     2251 b- defN 23-Feb-19 17:24 pytorch_clip_interrogator/prompt_engineer.py
--rw-rw-r--  2.0 unx     2909 b- defN 23-Feb-19 17:24 pytorch_clip_interrogator/utils.py
--rw-rw-r--  2.0 unx     3430 b- defN 23-Feb-19 20:48 pytorch_clip_interrogator/vocab.py
--rw-rw-r--  2.0 unx    82438 b- defN 23-Feb-19 17:24 pytorch_clip_interrogator/data/artists.txt
--rw-rw-r--  2.0 unx  1804274 b- defN 23-Feb-19 17:24 pytorch_clip_interrogator/data/flavors.txt
--rw-rw-r--  2.0 unx     1606 b- defN 23-Feb-19 17:24 pytorch_clip_interrogator/data/mediums.txt
--rw-rw-r--  2.0 unx     2683 b- defN 23-Feb-19 17:24 pytorch_clip_interrogator/data/movements.txt
--rw-rw-r--  2.0 unx      168 b- defN 23-Feb-19 17:24 pytorch_clip_interrogator/data/sites.txt
--rw-rw-r--  2.0 unx     1304 b- defN 23-Feb-20 15:00 pytorch_clip_interrogator-2023.2.20.1.dist-info/LICENSE
--rw-rw-r--  2.0 unx     1952 b- defN 23-Feb-20 15:00 pytorch_clip_interrogator-2023.2.20.1.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Feb-20 15:00 pytorch_clip_interrogator-2023.2.20.1.dist-info/WHEEL
--rw-rw-r--  2.0 unx       26 b- defN 23-Feb-20 15:00 pytorch_clip_interrogator-2023.2.20.1.dist-info/top_level.txt
--rw-rw-r--  2.0 unx        1 b- defN 23-Feb-19 16:18 pytorch_clip_interrogator-2023.2.20.1.dist-info/zip-safe
-?rw-rw-r--  2.0 unx     1683 b- defN 23-Feb-20 15:00 pytorch_clip_interrogator-2023.2.20.1.dist-info/RECORD
-17 files, 1912907 bytes uncompressed, 786264 bytes compressed:  58.9%
+Zip file size: 790087 bytes, number of entries: 17
+-rw-rw-r--  2.0 unx      604 b- defN 23-May-23 10:51 pytorch_clip_interrogator/__init__.py
+-rw-rw-r--  2.0 unx     5157 b- defN 23-May-31 13:30 pytorch_clip_interrogator/blip.py
+-rw-rw-r--  2.0 unx     6039 b- defN 23-May-31 13:20 pytorch_clip_interrogator/clip_interrogator.py
+-rw-rw-r--  2.0 unx     3611 b- defN 23-May-31 13:30 pytorch_clip_interrogator/prompt_engineer.py
+-rw-rw-r--  2.0 unx     2909 b- defN 23-May-31 12:52 pytorch_clip_interrogator/utils.py
+-rw-rw-r--  2.0 unx     3948 b- defN 23-May-31 12:51 pytorch_clip_interrogator/vocab.py
+-rw-rw-r--  2.0 unx    82438 b- defN 23-May-23 10:51 pytorch_clip_interrogator/data/artists.txt
+-rw-rw-r--  2.0 unx  1804274 b- defN 23-May-23 10:51 pytorch_clip_interrogator/data/flavors.txt
+-rw-rw-r--  2.0 unx     1606 b- defN 23-May-23 10:51 pytorch_clip_interrogator/data/mediums.txt
+-rw-rw-r--  2.0 unx     2683 b- defN 23-May-23 10:51 pytorch_clip_interrogator/data/movements.txt
+-rw-rw-r--  2.0 unx      168 b- defN 23-May-23 10:51 pytorch_clip_interrogator/data/sites.txt
+-rw-rw-r--  2.0 unx     1304 b- defN 23-Jun-01 08:25 pytorch_clip_interrogator-2023.5.30.0.dist-info/LICENSE
+-rw-rw-r--  2.0 unx     1952 b- defN 23-Jun-01 08:25 pytorch_clip_interrogator-2023.5.30.0.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Jun-01 08:25 pytorch_clip_interrogator-2023.5.30.0.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       26 b- defN 23-Jun-01 08:25 pytorch_clip_interrogator-2023.5.30.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx        1 b- defN 23-Jun-01 08:25 pytorch_clip_interrogator-2023.5.30.0.dist-info/zip-safe
+?rw-rw-r--  2.0 unx     1683 b- defN 23-Jun-01 08:25 pytorch_clip_interrogator-2023.5.30.0.dist-info/RECORD
+17 files, 1918495 bytes uncompressed, 787227 bytes compressed:  59.0%
```

## zipnote {}

```diff
@@ -27,26 +27,26 @@
 
 Filename: pytorch_clip_interrogator/data/movements.txt
 Comment: 
 
 Filename: pytorch_clip_interrogator/data/sites.txt
 Comment: 
 
-Filename: pytorch_clip_interrogator-2023.2.20.1.dist-info/LICENSE
+Filename: pytorch_clip_interrogator-2023.5.30.0.dist-info/LICENSE
 Comment: 
 
-Filename: pytorch_clip_interrogator-2023.2.20.1.dist-info/METADATA
+Filename: pytorch_clip_interrogator-2023.5.30.0.dist-info/METADATA
 Comment: 
 
-Filename: pytorch_clip_interrogator-2023.2.20.1.dist-info/WHEEL
+Filename: pytorch_clip_interrogator-2023.5.30.0.dist-info/WHEEL
 Comment: 
 
-Filename: pytorch_clip_interrogator-2023.2.20.1.dist-info/top_level.txt
+Filename: pytorch_clip_interrogator-2023.5.30.0.dist-info/top_level.txt
 Comment: 
 
-Filename: pytorch_clip_interrogator-2023.2.20.1.dist-info/zip-safe
+Filename: pytorch_clip_interrogator-2023.5.30.0.dist-info/zip-safe
 Comment: 
 
-Filename: pytorch_clip_interrogator-2023.2.20.1.dist-info/RECORD
+Filename: pytorch_clip_interrogator-2023.5.30.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## pytorch_clip_interrogator/blip.py

```diff
@@ -6,54 +6,51 @@
 http://www.apache.org/licenses/LICENSE-2.0
 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
 """
+import os
 from typing import Union, List, Optional, Dict
 import torch
 # BLIP Model
 from transformers import BlipProcessor, BlipForConditionalGeneration
 try:
     from transformers import Blip2Processor, Blip2ForConditionalGeneration
 except:
     print("BLIP2 model is not available. \nPlease, try to install latest version of transformers library.")
 # utils
 import PIL
 from PIL import Image
+import json
 
 
 class BLIP:
     """ BLIP Image-To-Text model
 
     Args:
         blip_model (str): BLIP model name.
         device (str): target device.
         torch_dtype (torch.dtype): target type.
     """
     def __init__(
             self,
-            blip_model: str = "Salesforce/blip-image-captioning-large",
+            blip,
+            blip_processor,
             device: str = "cpu",
             torch_dtype: torch.dtype = torch.float32
     ):
         super().__init__()
         # params
         self.device = device
         self.torch_dtype = torch_dtype
-        # BLIP model
-        print(f"load BLIP model: {blip_model}...")
-        if "blip2" in blip_model:
-            self.blip_processor = Blip2Processor.from_pretrained(blip_model)
-            self.blip = Blip2ForConditionalGeneration.from_pretrained(blip_model, torch_dtype=torch_dtype).to(device)
-        else:
-            self.blip_processor = BlipProcessor.from_pretrained(blip_model)
-            self.blip = BlipForConditionalGeneration.from_pretrained(blip_model, torch_dtype=torch_dtype).to(device)
-        self.blip.eval()
+        # model
+        self.blip = blip
+        self.blip_processor = blip_processor
 
     @torch.inference_mode()
     def __call__(
             self,
             image: Union[PIL.Image.Image, List[PIL.Image.Image]],
     ) -> str:
         """ Generate caption using BLIP model.
@@ -81,7 +78,65 @@
         for image in images:
             _pixel_values = self.blip_processor(image, return_tensors="pt")["pixel_values"]
             if pixel_values is not None:
                 pixel_values = torch.cat([pixel_values, _pixel_values], dim=0)
             else:
                 pixel_values = _pixel_values
         return pixel_values.to(device=self.device, dtype=self.torch_dtype)
+
+    @classmethod
+    def load_model(
+            cls,
+            blip_model: str = "Salesforce/blip-image-captioning-large",
+            device: str = "cpu",
+            torch_dtype: torch.dtype = torch.float32
+    ):
+        """ Load pretrained BLIP model.
+
+        Args:
+            blip_model (str): BLIP model name.
+            device (str): target device.
+            torch_dtype (torch.dtype): target type.
+        """
+        print(f"load BLIP model: {blip_model}...")
+        if "blip2" in blip_model:
+            blip = Blip2ForConditionalGeneration.from_pretrained(blip_model, torch_dtype=torch_dtype).to(device)
+            blip_processor = Blip2Processor.from_pretrained(blip_model)
+        else:
+            blip = BlipForConditionalGeneration.from_pretrained(blip_model, torch_dtype=torch_dtype).to(device)
+            blip_processor = BlipProcessor.from_pretrained(blip_model)
+        blip.eval()
+        return cls(blip, blip_processor, device, torch_dtype)
+
+    def save_pretrained(self, path: str) -> None:
+        """ Save pretrained interrogator to disk.
+
+        Args:
+            path (str): path.
+        """
+        if not os.path.exists(path):
+            os.makedirs(path)
+        self.blip.save_pretrained(os.path.join(path, "blip"))
+        self.blip_processor.save_pretrained(os.path.join(path, "blip_processor"))
+
+    @classmethod
+    def from_pretrained(
+            cls,
+            path: str,
+            torch_dtype: torch.dtype = torch.float32,
+            device: str = "cpu"
+    ):
+        """ Load pretrained interrogator from disk.
+
+        Args:
+            path (str): path.
+        """
+        with open(os.path.join(path, "blip", "config.json")) as f:
+            cfg = json.load(f)
+            if "blip2" in cfg["architectures"][0].lower():
+                blip = Blip2ForConditionalGeneration.from_pretrained(os.path.join(path, "blip"), torch_dtype=torch_dtype).to(device)
+                blip_processor = Blip2Processor.from_pretrained(os.path.join(path, "blip_processor"))
+            else:
+                blip = BlipForConditionalGeneration.from_pretrained(os.path.join(path, "blip"), torch_dtype=torch_dtype).to(device)
+                blip_processor = BlipProcessor.from_pretrained(os.path.join(path, "blip_processor"))
+            blip.eval()
+        return cls(blip, blip_processor, device, torch_dtype)
```

## pytorch_clip_interrogator/clip_interrogator.py

```diff
@@ -21,38 +21,62 @@
 # utils
 import numpy as np
 import PIL
 from addict import Dict as addict
 from .utils import *
 
 
+def preprocess_vocabulary(
+        clip: CLIPModel,
+        clip_processor: CLIPProcessor,
+        batch_size: int = 64,
+        device: str = "cpu"
+) -> addict:
+    """ Build prompt vocabulary.
+
+        Args:
+            batch_size (int): batch size.
+        Returns:
+            addict vocabulary.
+        """
+    vocab = {}
+    for name in ["artists", "flavors", "mediums", "movements", "sites"]:
+        vocab[name] = Vocab.from_corpus(
+            os.path.join(res_path("data"), f"{name}.txt"),
+            clip,
+            clip_processor,
+            batch_size,
+            device
+        )
+    return addict(vocab)
+
+
 class CLIPInterrogator:
     """ CLIP Interrogator
 
     Args:
         clip_model (str): CLIP model name.
         device (str): target device.
         torch_dtype (torch.dtype): target type.
     """
     def __init__(
             self,
-            clip_model: str = "openai/clip-vit-base-patch32",
+            clip: CLIPModel,
+            clip_processor: CLIPProcessor,
+            vocab: Vocab,
             device: str = "cpu",
             torch_dtype: torch.dtype = torch.float32
     ):
         # params
         self.device = device
         self.torch_dtype = torch_dtype
-        # CLIP model
-        print(f"load CLIP model: {clip_model}...")
-        self.clip_processor = CLIPProcessor.from_pretrained(clip_model)
-        self.clip = CLIPModel.from_pretrained(clip_model, torch_dtype=torch_dtype).to(device)
-        self.clip.eval()
-        # initialize corpus
-        self.vocab = self._build_vocabulary()
+        # models
+        self.clip = clip
+        self.clip_processor = clip_processor
+        self.vocab = vocab
 
     def __call__(
             self,
             images: Union[PIL.Image.Image, List[PIL.Image.Image]],
             caption: Union[str, List[str]],
             max_flavors: int = 3
     ) -> List[str]:
@@ -80,33 +104,14 @@
                 prompt = f"{caption[i]}, {artist[i]}, {movement[i]}, {', '.join(flaves[i])}"
             else:
                 prompt = f"{caption[i]}, {medium[i]} {artist[i]}, {movement[i]}, {', '.join(flaves[i])}"
             output.append(prompt)
 
         return output
 
-    def _build_vocabulary(self, batch_size: int = 64) -> addict:
-        """ Build prompt vocabulary.
-
-        Args:
-            batch_size (int): batch size.
-        Returns:
-            addict vocabulary.
-        """
-        vocab = {}
-        for name in ["artists", "flavors", "mediums", "movements", "sites"]:
-            vocab[name] = Vocab.from_corpus(
-                os.path.join(res_path("data"), f"{name}.txt"),
-                self.clip,
-                self.clip_processor,
-                batch_size,
-                self.device
-            )
-        return addict(vocab)
-
     @torch.inference_mode()
     def _image_to_features(
             self,
             images: Union[PIL.Image.Image, List[PIL.Image.Image]]
     ) -> np.ndarray:
         """ Image to CLIP features.
 
@@ -117,7 +122,62 @@
         """
         if not isinstance(images, list):
             images = [images]
         inputs = self.clip_processor(images=images, return_tensors="pt")
         image_features = self.clip.get_image_features(**to_device(inputs, self.device, dtype=self.torch_dtype))
         image_features = F.normalize(image_features, p=2, dim=1).float()
         return image_features.cpu().numpy()
+
+    def save_pretrained(self, path: str) -> None:
+        """ Save pretrained interrogator to disk.
+
+        Args:
+            path (str): path.
+        """
+        if not os.path.exists(path):
+            os.makedirs(path)
+        for key, val in self.vocab.items():
+            val.save_pretrained(os.path.join(path, key))
+        self.clip_processor.save_pretrained(os.path.join(path, "clip_processor"))
+        self.clip.save_pretrained(os.path.join(path, "clip"))
+
+    @classmethod
+    def from_pretrained(
+            cls,
+            path: str,
+            torch_dtype: torch.dtype = torch.float32,
+            device: str = "cpu"
+    ):
+        """ Load pretrained interrogator from disk.
+
+        Args:
+            path (str): path.
+        """
+        vocab = {}
+        for name in ["artists", "flavors", "mediums", "movements", "sites"]:
+            vocab[name] = Vocab.from_pretrained(
+                os.path.join(path, name)
+            )
+        vocab = addict(vocab)
+        clip_processor = CLIPProcessor.from_pretrained(os.path.join(path, "clip_processor"))
+        clip = CLIPModel.from_pretrained(os.path.join(path, "clip"), torch_dtype=torch_dtype).to(device)
+        clip.eval()
+        return cls(clip, clip_processor, vocab, device, torch_dtype)
+
+    @classmethod
+    def load_model(
+            cls,
+            clip_model: str = "openai/clip-vit-base-patch32",
+            device: str = "cpu",
+            torch_dtype: torch.dtype = torch.float32,
+            batch_size: int = 64
+    ):
+        """ Load pretrained interrogator from disk.
+
+        Args:
+            path (str): path.
+        """
+        clip = CLIPModel.from_pretrained(clip_model, torch_dtype=torch_dtype).to(device)
+        clip.eval()
+        clip_processor = CLIPProcessor.from_pretrained(clip_model)
+        vocab = preprocess_vocabulary(clip, clip_processor, batch_size, device)
+        return cls(clip, clip_processor, vocab, device, torch_dtype)
```

## pytorch_clip_interrogator/prompt_engineer.py

```diff
@@ -6,14 +6,15 @@
 http://www.apache.org/licenses/LICENSE-2.0
 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
 """
+import os
 from typing import Union, List
 import torch
 import PIL
 from PIL import Image
 from .blip import BLIP
 from .clip_interrogator import CLIPInterrogator
 
@@ -23,31 +24,17 @@
 
     Args:
         blip_model (str): BLIP model name.
         clip_model (str): CLIP model name.
         device (str): target device.
         torch_dtype (torch.dtype): target type.
     """
-    def __init__(
-            self,
-            blip_model: str = "Salesforce/blip-image-captioning-large",
-            clip_model: str = "openai/clip-vit-base-patch32",
-            device: str = "cpu",
-            torch_dtype: torch.dtype = torch.float32
-    ):
-        self.blip = BLIP(
-            blip_model=blip_model,
-            device=device,
-            torch_dtype=torch_dtype
-        )
-        self.clip_interrogator = CLIPInterrogator(
-            clip_model=clip_model,
-            device=device,
-            torch_dtype=torch_dtype
-        )
+    def __init__(self, blip, clip_interrogator):
+        self.blip = blip
+        self.clip_interrogator = clip_interrogator
 
     def __call__(
             self,
             images: Union[PIL.Image.Image, List[PIL.Image.Image]],
             max_flavors: int = 3,
             interrogate: bool = True
     ):
@@ -64,7 +51,67 @@
         if interrogate:
             caption = self.clip_interrogator(
                 caption=caption,
                 images=images,
                 max_flavors=max_flavors
             )
         return caption
+
+    @classmethod
+    def load_model(
+            cls,
+            blip_model: str = "Salesforce/blip-image-captioning-large",
+            clip_model: str = "openai/clip-vit-base-patch32",
+            device: str = "cpu",
+            torch_dtype: torch.dtype = torch.float32
+    ):
+        """ Load pretrained prompt engineer.
+
+        Args:
+            path (str): path.
+        """
+        blip = BLIP.load_model(
+            blip_model=blip_model,
+            device=device,
+            torch_dtype=torch_dtype
+        )
+        clip_interrogator = CLIPInterrogator.load_model(
+            clip_model=clip_model,
+            device=device,
+            torch_dtype=torch_dtype
+        )
+        return cls(blip, clip_interrogator)
+
+    def save_pretrained(self, path: str) -> None:
+        """ Save pretrained prompt engineer to disk.
+
+        Args:
+            path (str): path.
+        """
+        if not os.path.exists(path):
+            os.makedirs(path)
+        self.blip.save_pretrained(os.path.join(path, "blip"))
+        self.clip_interrogator.save_pretrained(os.path.join(path, "clip_interrogator"))
+
+    @classmethod
+    def from_pretrained(
+            cls,
+            path: str,
+            torch_dtype: torch.dtype = torch.float32,
+            device: str = "cpu"
+    ):
+        """ Load pretrained prompt engineer from disk.
+
+        Args:
+            path (str): path.
+        """
+        blip = BLIP.from_pretrained(
+            os.path.join(path, "blip"),
+            torch_dtype=torch_dtype,
+            device=device
+        )
+        clip_interrogator = CLIPInterrogator.from_pretrained(
+            os.path.join(path, "clip_interrogator"),
+            torch_dtype=torch_dtype,
+            device=device
+        )
+        return cls(blip, clip_interrogator)
```

## pytorch_clip_interrogator/vocab.py

```diff
@@ -53,21 +53,35 @@
         """
         scores, ids = self.index.search(embedding, k)
         output = []
         for i in range(ids.shape[0]):
             output.append(itemgetter(*ids.tolist()[i])(self.dataset["text"]))
         return output
 
-    def save_pretrained(self, filename: str) -> None:
-        """ Save pretrained index to file.
+    def save_pretrained(self, path: str) -> None:
+        """ Save pretrained vocab to disk.
 
         Args:
-            filename (str): output filename.
+            path (str): path to store vocab.
         """
-        faiss.write_index(self.index, filename)
+        if not os.path.exists(path):
+            os.makedirs(path)
+        faiss.write_index(self.index, os.path.join(path, "vocab.index"))
+        self.dataset.to_csv(os.path.join(path, "vocab.dataset"))
+
+    @classmethod
+    def from_pretrained(cls, path: str):
+        """ Load pretrained vocab from disk.
+
+        Args:
+            path (str): vocab path.
+        """
+        index = faiss.read_index(os.path.join(path, "vocab.index"))
+        dataset = datasets.arrow_dataset.Dataset.from_csv(os.path.join(path, "vocab.dataset"))
+        return cls(dataset, index)
 
     @classmethod
     def from_corpus(
             cls,
             path: str,
             model: CLIPModel,
             processor: CLIPProcessor,
```

## Comparing `pytorch_clip_interrogator-2023.2.20.1.dist-info/LICENSE` & `pytorch_clip_interrogator-2023.5.30.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `pytorch_clip_interrogator-2023.2.20.1.dist-info/METADATA` & `pytorch_clip_interrogator-2023.5.30.0.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pytorch-clip-interrogator
-Version: 2023.2.20.1
+Version: 2023.5.30.0
 Summary: Prompt engineering tool using BLIP 1/2 + CLIP Interrogate approach.
 Home-page: UNKNOWN
 Author: Sergei Belousov aka BeS
 Author-email: sergei.o.belousov@gmail.com
 License: UNKNOWN
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
```

