# Comparing `tmp/mltable-1.3.0-py3-none-any.whl.zip` & `tmp/mltable-1.4.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,72 +1,72 @@
-Zip file size: 178694 bytes, number of entries: 70
--rw-rw-rw-  2.0 fat      937 b- defN 23-Apr-07 22:56 mltable/__init__.py
--rw-rw-rw-  2.0 fat     4193 b- defN 23-Apr-07 22:56 mltable/_utils.py
--rw-rw-rw-  2.0 fat     4404 b- defN 23-Apr-07 22:56 mltable/_validation_and_error_handler.py
--rw-rw-rw-  2.0 fat    90746 b- defN 23-Apr-07 22:56 mltable/mltable.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-Apr-07 22:57 mltable/_aml_utilities/__init__.py
--rw-rw-rw-  2.0 fat     7321 b- defN 23-Apr-07 22:57 mltable/_aml_utilities/_aml_rest_client_helper.py
--rw-rw-rw-  2.0 fat    23222 b- defN 23-Apr-07 22:57 mltable/_aml_utilities/_azureml_token_authentication.py
--rw-rw-rw-  2.0 fat      893 b- defN 23-Apr-07 22:59 mltable/_aml_utilities/_restclient/__init__.py
--rw-rw-rw-  2.0 fat     4218 b- defN 23-Apr-07 22:59 mltable/_aml_utilities/_restclient/_azure_machine_learning_workspaces.py
--rw-rw-rw-  2.0 fat     3538 b- defN 23-Apr-07 22:59 mltable/_aml_utilities/_restclient/_configuration.py
--rw-rw-rw-  2.0 fat     1561 b- defN 23-Apr-07 22:59 mltable/_aml_utilities/_restclient/_patch.py
--rw-rw-rw-  2.0 fat      495 b- defN 23-Apr-07 22:59 mltable/_aml_utilities/_restclient/_version.py
--rw-rw-rw-  2.0 fat      365 b- defN 23-Apr-07 22:59 mltable/_aml_utilities/_restclient/models.py
--rw-rw-rw-  2.0 fat      893 b- defN 23-Apr-07 23:00 mltable/_aml_utilities/_restclient/dataset/__init__.py
--rw-rw-rw-  2.0 fat     3954 b- defN 23-Apr-07 23:00 mltable/_aml_utilities/_restclient/dataset/_azure_machine_learning_workspaces.py
--rw-rw-rw-  2.0 fat     3209 b- defN 23-Apr-07 23:00 mltable/_aml_utilities/_restclient/dataset/_configuration.py
--rw-rw-rw-  2.0 fat     1561 b- defN 23-Apr-07 23:00 mltable/_aml_utilities/_restclient/dataset/_patch.py
--rw-rw-rw-  2.0 fat     1255 b- defN 23-Apr-07 23:00 mltable/_aml_utilities/_restclient/dataset/_vendor.py
--rw-rw-rw-  2.0 fat      495 b- defN 23-Apr-07 23:00 mltable/_aml_utilities/_restclient/dataset/_version.py
--rw-rw-rw-  2.0 fat    10737 b- defN 23-Apr-07 23:00 mltable/_aml_utilities/_restclient/dataset/models/__init__.py
--rw-rw-rw-  2.0 fat     4514 b- defN 23-Apr-07 23:00 mltable/_aml_utilities/_restclient/dataset/models/_azure_machine_learning_workspaces_enums.py
--rw-rw-rw-  2.0 fat   116583 b- defN 23-Apr-07 23:00 mltable/_aml_utilities/_restclient/dataset/models/_models.py
--rw-rw-rw-  2.0 fat   125881 b- defN 23-Apr-07 23:00 mltable/_aml_utilities/_restclient/dataset/models/_models_py3.py
--rw-rw-rw-  2.0 fat      585 b- defN 23-Apr-07 23:00 mltable/_aml_utilities/_restclient/dataset/operations/__init__.py
--rw-rw-rw-  2.0 fat    67091 b- defN 23-Apr-07 23:00 mltable/_aml_utilities/_restclient/dataset/operations/_data_version_operations.py
--rw-rw-rw-  2.0 fat      893 b- defN 23-Apr-07 23:00 mltable/_aml_utilities/_restclient/token/__init__.py
--rw-rw-rw-  2.0 fat     3697 b- defN 23-Apr-07 23:00 mltable/_aml_utilities/_restclient/token/_azure_machine_learning_workspaces.py
--rw-rw-rw-  2.0 fat     3068 b- defN 23-Apr-07 23:00 mltable/_aml_utilities/_restclient/token/_configuration.py
--rw-rw-rw-  2.0 fat     1559 b- defN 23-Apr-07 23:00 mltable/_aml_utilities/_restclient/token/_patch.py
--rw-rw-rw-  2.0 fat     1224 b- defN 23-Apr-07 23:00 mltable/_aml_utilities/_restclient/token/_vendor.py
--rw-rw-rw-  2.0 fat      495 b- defN 23-Apr-07 23:00 mltable/_aml_utilities/_restclient/token/_version.py
--rw-rw-rw-  2.0 fat      837 b- defN 23-Apr-07 23:00 mltable/_aml_utilities/_restclient/token/aio/__init__.py
--rw-rw-rw-  2.0 fat     3645 b- defN 23-Apr-07 23:00 mltable/_aml_utilities/_restclient/token/aio/_azure_machine_learning_workspaces.py
--rw-rw-rw-  2.0 fat     3021 b- defN 23-Apr-07 23:00 mltable/_aml_utilities/_restclient/token/aio/_configuration.py
--rw-rw-rw-  2.0 fat     1559 b- defN 23-Apr-07 23:00 mltable/_aml_utilities/_restclient/token/aio/_patch.py
--rw-rw-rw-  2.0 fat      563 b- defN 23-Apr-07 23:01 mltable/_aml_utilities/_restclient/token/aio/operations/__init__.py
--rw-rw-rw-  2.0 fat     7639 b- defN 23-Apr-07 23:01 mltable/_aml_utilities/_restclient/token/aio/operations/_runs_operations.py
--rw-rw-rw-  2.0 fat    10472 b- defN 23-Apr-07 23:00 mltable/_aml_utilities/_restclient/token/models/__init__.py
--rw-rw-rw-  2.0 fat     2244 b- defN 23-Apr-07 23:00 mltable/_aml_utilities/_restclient/token/models/_azure_machine_learning_workspaces_enums.py
--rw-rw-rw-  2.0 fat   166455 b- defN 23-Apr-07 23:00 mltable/_aml_utilities/_restclient/token/models/_models.py
--rw-rw-rw-  2.0 fat   179783 b- defN 23-Apr-07 23:00 mltable/_aml_utilities/_restclient/token/models/_models_py3.py
--rw-rw-rw-  2.0 fat      563 b- defN 23-Apr-07 23:00 mltable/_aml_utilities/_restclient/token/operations/__init__.py
--rw-rw-rw-  2.0 fat    10661 b- defN 23-Apr-07 23:00 mltable/_aml_utilities/_restclient/token/operations/_runs_operations.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-Apr-07 22:57 mltable/_common/__init__.py
--rw-rw-rw-  2.0 fat     2393 b- defN 23-Apr-07 22:57 mltable/_common/chained_identity.py
--rw-rw-rw-  2.0 fat      933 b- defN 23-Apr-07 22:57 mltable/_common/logged_lock.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-Apr-07 22:59 mltable/_common/async_utils/__init__.py
--rw-rw-rw-  2.0 fat     2827 b- defN 23-Apr-07 22:59 mltable/_common/async_utils/async_task.py
--rw-rw-rw-  2.0 fat     2329 b- defN 23-Apr-07 22:59 mltable/_common/async_utils/daemon.py
--rw-rw-rw-  2.0 fat     5911 b- defN 23-Apr-07 22:59 mltable/_common/async_utils/task_queue.py
--rw-rw-rw-  2.0 fat     1112 b- defN 23-Apr-07 22:59 mltable/_common/async_utils/worker_pool.py
--rw-rw-rw-  2.0 fat    31845 b- defN 23-Apr-07 22:57 mltable/schema/MLTable.json
--rw-rw-rw-  2.0 fat        0 b- defN 23-Apr-07 22:57 mltable/tests/__init__.py
--rw-rw-rw-  2.0 fat     2129 b- defN 23-Apr-07 22:57 mltable/tests/conftest.py
--rw-rw-rw-  2.0 fat     2120 b- defN 23-Apr-07 22:57 mltable/tests/helper_functions.py
--rw-rw-rw-  2.0 fat    11329 b- defN 23-Apr-07 22:57 mltable/tests/test_from_delta_lake.py
--rw-rw-rw-  2.0 fat     3968 b- defN 23-Apr-07 22:57 mltable/tests/test_from_json_lines.py
--rw-rw-rw-  2.0 fat    61415 b- defN 23-Apr-07 22:57 mltable/tests/test_mltable_authoring_apis.py
--rw-rw-rw-  2.0 fat    15184 b- defN 23-Apr-07 22:57 mltable/tests/test_mltable_inner_functions.py
--rw-rw-rw-  2.0 fat     5148 b- defN 23-Apr-07 22:57 mltable/tests/test_mltable_load.py
--rw-rw-rw-  2.0 fat     9662 b- defN 23-Apr-07 22:57 mltable/tests/test_mltable_load_to_pandas.py
--rw-rw-rw-  2.0 fat     6708 b- defN 23-Apr-07 22:57 mltable/tests/test_mltable_save.py
--rw-rw-rw-  2.0 fat     1477 b- defN 23-Apr-07 22:57 mltable/tests/test_partition_api.py
--rw-rw-rw-  2.0 fat     4438 b- defN 23-Apr-07 22:57 mltable/tests/test_schema.py
--rw-rw-rw-  2.0 fat     5294 b- defN 23-Apr-07 22:57 mltable/tests/test_validation_and_error_handler.py
--rw-rw-rw-  2.0 fat     1021 b- defN 23-Apr-07 23:07 mltable-1.3.0.dist-info/LICENSE.txt
--rw-rw-rw-  2.0 fat     3886 b- defN 23-Apr-07 23:07 mltable-1.3.0.dist-info/METADATA
--rw-rw-rw-  2.0 fat       97 b- defN 23-Apr-07 23:07 mltable-1.3.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        8 b- defN 23-Apr-07 23:07 mltable-1.3.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     7337 b- defN 23-Apr-07 23:07 mltable-1.3.0.dist-info/RECORD
-70 files, 1066149 bytes uncompressed, 166538 bytes compressed:  84.4%
+Zip file size: 179167 bytes, number of entries: 70
+-rw-rw-rw-  2.0 fat      937 b- defN 23-May-31 23:53 mltable/__init__.py
+-rw-rw-rw-  2.0 fat     4378 b- defN 23-May-31 23:53 mltable/_utils.py
+-rw-rw-rw-  2.0 fat     6793 b- defN 23-May-31 23:53 mltable/_validation_and_error_handler.py
+-rw-rw-rw-  2.0 fat    91002 b- defN 23-May-31 23:53 mltable/mltable.py
+-rw-rw-rw-  2.0 fat      183 b- defN 23-May-31 23:54 mltable/_aml_utilities/__init__.py
+-rw-rw-rw-  2.0 fat     7521 b- defN 23-May-31 23:54 mltable/_aml_utilities/_aml_rest_client_helper.py
+-rw-rw-rw-  2.0 fat    23222 b- defN 23-May-31 23:54 mltable/_aml_utilities/_azureml_token_authentication.py
+-rw-rw-rw-  2.0 fat      893 b- defN 23-May-31 23:56 mltable/_aml_utilities/_restclient/__init__.py
+-rw-rw-rw-  2.0 fat     4218 b- defN 23-May-31 23:56 mltable/_aml_utilities/_restclient/_azure_machine_learning_workspaces.py
+-rw-rw-rw-  2.0 fat     3538 b- defN 23-May-31 23:56 mltable/_aml_utilities/_restclient/_configuration.py
+-rw-rw-rw-  2.0 fat     1561 b- defN 23-May-31 23:56 mltable/_aml_utilities/_restclient/_patch.py
+-rw-rw-rw-  2.0 fat      495 b- defN 23-May-31 23:56 mltable/_aml_utilities/_restclient/_version.py
+-rw-rw-rw-  2.0 fat      365 b- defN 23-May-31 23:56 mltable/_aml_utilities/_restclient/models.py
+-rw-rw-rw-  2.0 fat      893 b- defN 23-May-31 23:57 mltable/_aml_utilities/_restclient/dataset/__init__.py
+-rw-rw-rw-  2.0 fat     3954 b- defN 23-May-31 23:57 mltable/_aml_utilities/_restclient/dataset/_azure_machine_learning_workspaces.py
+-rw-rw-rw-  2.0 fat     3209 b- defN 23-May-31 23:57 mltable/_aml_utilities/_restclient/dataset/_configuration.py
+-rw-rw-rw-  2.0 fat     1561 b- defN 23-May-31 23:57 mltable/_aml_utilities/_restclient/dataset/_patch.py
+-rw-rw-rw-  2.0 fat     1255 b- defN 23-May-31 23:57 mltable/_aml_utilities/_restclient/dataset/_vendor.py
+-rw-rw-rw-  2.0 fat      495 b- defN 23-May-31 23:57 mltable/_aml_utilities/_restclient/dataset/_version.py
+-rw-rw-rw-  2.0 fat    10737 b- defN 23-May-31 23:58 mltable/_aml_utilities/_restclient/dataset/models/__init__.py
+-rw-rw-rw-  2.0 fat     4514 b- defN 23-May-31 23:58 mltable/_aml_utilities/_restclient/dataset/models/_azure_machine_learning_workspaces_enums.py
+-rw-rw-rw-  2.0 fat   116583 b- defN 23-May-31 23:58 mltable/_aml_utilities/_restclient/dataset/models/_models.py
+-rw-rw-rw-  2.0 fat   125881 b- defN 23-May-31 23:58 mltable/_aml_utilities/_restclient/dataset/models/_models_py3.py
+-rw-rw-rw-  2.0 fat      585 b- defN 23-May-31 23:58 mltable/_aml_utilities/_restclient/dataset/operations/__init__.py
+-rw-rw-rw-  2.0 fat    67091 b- defN 23-May-31 23:58 mltable/_aml_utilities/_restclient/dataset/operations/_data_version_operations.py
+-rw-rw-rw-  2.0 fat      893 b- defN 23-May-31 23:57 mltable/_aml_utilities/_restclient/token/__init__.py
+-rw-rw-rw-  2.0 fat     3697 b- defN 23-May-31 23:57 mltable/_aml_utilities/_restclient/token/_azure_machine_learning_workspaces.py
+-rw-rw-rw-  2.0 fat     3068 b- defN 23-May-31 23:57 mltable/_aml_utilities/_restclient/token/_configuration.py
+-rw-rw-rw-  2.0 fat     1559 b- defN 23-May-31 23:57 mltable/_aml_utilities/_restclient/token/_patch.py
+-rw-rw-rw-  2.0 fat     1224 b- defN 23-May-31 23:57 mltable/_aml_utilities/_restclient/token/_vendor.py
+-rw-rw-rw-  2.0 fat      495 b- defN 23-May-31 23:57 mltable/_aml_utilities/_restclient/token/_version.py
+-rw-rw-rw-  2.0 fat      837 b- defN 23-May-31 23:58 mltable/_aml_utilities/_restclient/token/aio/__init__.py
+-rw-rw-rw-  2.0 fat     3645 b- defN 23-May-31 23:58 mltable/_aml_utilities/_restclient/token/aio/_azure_machine_learning_workspaces.py
+-rw-rw-rw-  2.0 fat     3021 b- defN 23-May-31 23:58 mltable/_aml_utilities/_restclient/token/aio/_configuration.py
+-rw-rw-rw-  2.0 fat     1559 b- defN 23-May-31 23:58 mltable/_aml_utilities/_restclient/token/aio/_patch.py
+-rw-rw-rw-  2.0 fat      563 b- defN 23-May-31 23:59 mltable/_aml_utilities/_restclient/token/aio/operations/__init__.py
+-rw-rw-rw-  2.0 fat     7639 b- defN 23-May-31 23:59 mltable/_aml_utilities/_restclient/token/aio/operations/_runs_operations.py
+-rw-rw-rw-  2.0 fat    10472 b- defN 23-May-31 23:58 mltable/_aml_utilities/_restclient/token/models/__init__.py
+-rw-rw-rw-  2.0 fat     2244 b- defN 23-May-31 23:58 mltable/_aml_utilities/_restclient/token/models/_azure_machine_learning_workspaces_enums.py
+-rw-rw-rw-  2.0 fat   166455 b- defN 23-May-31 23:58 mltable/_aml_utilities/_restclient/token/models/_models.py
+-rw-rw-rw-  2.0 fat   179783 b- defN 23-May-31 23:58 mltable/_aml_utilities/_restclient/token/models/_models_py3.py
+-rw-rw-rw-  2.0 fat      563 b- defN 23-May-31 23:58 mltable/_aml_utilities/_restclient/token/operations/__init__.py
+-rw-rw-rw-  2.0 fat    10661 b- defN 23-May-31 23:58 mltable/_aml_utilities/_restclient/token/operations/_runs_operations.py
+-rw-rw-rw-  2.0 fat      183 b- defN 23-May-31 23:54 mltable/_common/__init__.py
+-rw-rw-rw-  2.0 fat     2393 b- defN 23-May-31 23:54 mltable/_common/chained_identity.py
+-rw-rw-rw-  2.0 fat      933 b- defN 23-May-31 23:54 mltable/_common/logged_lock.py
+-rw-rw-rw-  2.0 fat      183 b- defN 23-May-31 23:56 mltable/_common/async_utils/__init__.py
+-rw-rw-rw-  2.0 fat     2827 b- defN 23-May-31 23:56 mltable/_common/async_utils/async_task.py
+-rw-rw-rw-  2.0 fat     2329 b- defN 23-May-31 23:56 mltable/_common/async_utils/daemon.py
+-rw-rw-rw-  2.0 fat     5911 b- defN 23-May-31 23:56 mltable/_common/async_utils/task_queue.py
+-rw-rw-rw-  2.0 fat     1112 b- defN 23-May-31 23:56 mltable/_common/async_utils/worker_pool.py
+-rw-rw-rw-  2.0 fat    31957 b- defN 23-May-31 23:54 mltable/schema/MLTable.json
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-31 23:54 mltable/tests/__init__.py
+-rw-rw-rw-  2.0 fat     2129 b- defN 23-May-31 23:54 mltable/tests/conftest.py
+-rw-rw-rw-  2.0 fat     2120 b- defN 23-May-31 23:54 mltable/tests/helper_functions.py
+-rw-rw-rw-  2.0 fat     8543 b- defN 23-May-31 23:54 mltable/tests/test_from_delta_lake.py
+-rw-rw-rw-  2.0 fat     4088 b- defN 23-May-31 23:54 mltable/tests/test_from_json_lines.py
+-rw-rw-rw-  2.0 fat    61372 b- defN 23-May-31 23:54 mltable/tests/test_mltable_authoring_apis.py
+-rw-rw-rw-  2.0 fat    15096 b- defN 23-May-31 23:54 mltable/tests/test_mltable_inner_functions.py
+-rw-rw-rw-  2.0 fat     5122 b- defN 23-May-31 23:54 mltable/tests/test_mltable_load.py
+-rw-rw-rw-  2.0 fat     9605 b- defN 23-May-31 23:54 mltable/tests/test_mltable_load_to_pandas.py
+-rw-rw-rw-  2.0 fat     6897 b- defN 23-May-31 23:54 mltable/tests/test_mltable_save.py
+-rw-rw-rw-  2.0 fat     1481 b- defN 23-May-31 23:54 mltable/tests/test_partition_api.py
+-rw-rw-rw-  2.0 fat     4438 b- defN 23-May-31 23:54 mltable/tests/test_schema.py
+-rw-rw-rw-  2.0 fat     4886 b- defN 23-May-31 23:54 mltable/tests/test_validation_and_error_handler.py
+-rw-rw-rw-  2.0 fat     1021 b- defN 23-Jun-01 00:03 mltable-1.4.0.dist-info/LICENSE.txt
+-rw-rw-rw-  2.0 fat     3992 b- defN 23-Jun-01 00:03 mltable-1.4.0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       97 b- defN 23-Jun-01 00:03 mltable-1.4.0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        8 b- defN 23-Jun-01 00:03 mltable-1.4.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     7336 b- defN 23-Jun-01 00:03 mltable-1.4.0.dist-info/RECORD
+70 files, 1066301 bytes uncompressed, 167011 bytes compressed:  84.3%
```

## zipnote {}

```diff
@@ -189,23 +189,23 @@
 
 Filename: mltable/tests/test_schema.py
 Comment: 
 
 Filename: mltable/tests/test_validation_and_error_handler.py
 Comment: 
 
-Filename: mltable-1.3.0.dist-info/LICENSE.txt
+Filename: mltable-1.4.0.dist-info/LICENSE.txt
 Comment: 
 
-Filename: mltable-1.3.0.dist-info/METADATA
+Filename: mltable-1.4.0.dist-info/METADATA
 Comment: 
 
-Filename: mltable-1.3.0.dist-info/WHEEL
+Filename: mltable-1.4.0.dist-info/WHEEL
 Comment: 
 
-Filename: mltable-1.3.0.dist-info/top_level.txt
+Filename: mltable-1.4.0.dist-info/top_level.txt
 Comment: 
 
-Filename: mltable-1.3.0.dist-info/RECORD
+Filename: mltable-1.4.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## mltable/_utils.py

```diff
@@ -1,18 +1,20 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 import json
-from jsonschema import validate
 import os
 import re
 import yaml
 
+import jsonschema
+import jsonschema.exceptions
 from azureml.dataprep.api.mltable._mltable_helper import _parse_path_format, _PathType
 from azureml.dataprep.api._loggerfactory import _LoggerFactory
+from azureml.dataprep import UserErrorException
 
 
 _logger = _LoggerFactory.get_logger('MLTableUtils')
 _long_form_aml_uri = re.compile(
     r'^azureml://subscriptions/([^\/]+)/resourcegroups/([^\/]+)/'
     r'(?:providers/Microsoft.MachineLearningServices/)?workspaces/([^\/]+)/(.*)',
     re.IGNORECASE)
@@ -52,26 +54,25 @@
 
 
 def _path_is_current_directory_variant(path):
     return path in ['.', './', '.\\']
 
 
 def _validate(mltable_yaml_dict):
-    cwd = os.path.dirname(os.path.abspath(__file__))
-    schema_path = "{}/schema/MLTable.json".format(cwd.rstrip("/"))
-    with open(schema_path, "r") as stream:
+    parent_dirc = os.path.dirname(os.path.abspath(__file__)).rstrip("/")
+    mltable_schema_path = f'{parent_dirc}/schema/MLTable.json'
+    with open(mltable_schema_path, "r") as stream:
         try:
             schema = json.load(stream)
-        except json.decoder.JSONDecodeError:
-            raise RuntimeError("MLTable json schema is not a valid json file.")
-    try:
-        validate(mltable_yaml_dict, schema)
-    except Exception as e:
-        _logger.warning(f"MLTable validation failed with error: {e.args[0]}")
-        raise ValueError(f"Given MLTable does not adhere to the AzureML MLTable schema: {e.args[0]}")
+            jsonschema.validate(mltable_yaml_dict, schema)
+        except (json.decoder.JSONDecodeError, jsonschema.exceptions.SchemaError):
+            raise UserErrorException("MLTable json schema is not a valid json file.")
+        except jsonschema.exceptions.ValidationError as e:
+            _logger.warning(f"MLTable validation failed with error: {e.args[0]}")
+            raise UserErrorException(f"Given MLTable does not adhere to the AzureML MLTable schema: {e.args[0]}")
 
 
 # will switch to the api from dataprep package once new dataprep version is released
 def _parse_workspace_context_from_longform_uri(uri):
     long_form_uri_match = _long_form_aml_uri.match(uri)
 
     if long_form_uri_match:
```

## mltable/_validation_and_error_handler.py

```diff
@@ -1,84 +1,124 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
+from azureml.dataprep import DataPrepException, UserErrorException
 
 
-def _get_and_validate_download_list(download_records, ignore_not_found, logger):
-    if len(download_records) == 0:
+_DATAPREP_EXECEPTION_USER_ERROR_CODES = ('ScriptExecution.StreamAccess.Validation',
+                                         'ScriptExecution.StreamAccess.NotFound',
+                                         'ScriptExecution.StreamAccess.Authentication',
+                                         'ScriptExecution.DatabaseQuery',
+                                         'ScriptExecution.Database.TypeMismatch',
+                                         'ScriptExecution.Validation',
+                                         'ScriptExecution.DatabaseConnection.Authentication',
+                                         'ScriptExecution.DatabaseConnection',
+                                         'ScriptExecution.Database.TypeMismatch',
+                                         'ScriptExecution.WriteStreams.NotFound',
+                                         'ScriptExecution.WriteStreams.Authentication',
+                                         'ScriptExecution.WriteStreams.Validation',
+                                         'ScriptExecution.WriteStreams.AlreadyExists',
+                                         'ScriptExecution.Transformation.Validation')
+
+
+_RSLEX_USER_ERROR_VALUES = ('Microsoft.DPrep.ErrorValues.SourceFileNotFound',
+                            'Microsoft.DPrep.ErrorValues.SourceFilePermissionDenied',
+                            'Microsoft.DPrep.ErrorValues.InvalidArgument',
+                            'Microsoft.DPrep.ErrorValues.ValueWrongKind',
+                            'Microsoft.DPrep.ErrorValues.SourcePermissionDenied',
+                            'Microsoft.DPrep.ErrorValues.DestinationPermissionDenied',
+                            'Microsoft.DPrep.ErrorValues.DestinationDiskFull',
+                            'Microsoft.DPrep.ErrorValues.FileSizeChangedWhileDownloading',
+                            'Microsoft.DPrep.ErrorValues.StreamInfoInvalidPath',
+                            'Microsoft.DPrep.ErrorValues.NoManagedIdentity',
+                            'Microsoft.DPrep.ErrorValues.NoOboEndpoint',
+                            'Microsoft.DPrep.ErrorValues.StreamInfoRequired')
+
+
+_RSLEX_USER_ERROR_MSGS = ('InvalidUriScheme',
+                          'StreamError(NotFound)',
+                          'DataAccessError(NotFound)',
+                          'No such host is known',
+                          'No identity was found on compute',
+                          'Make sure uri is correct',
+                          'Invalid JSON in log record',
+                          'Invalid table version',
+                          'stream did not contain valid UTF-8',
+                          'Got unexpected error: invalid data. Kind(InvalidData)',
+                          'Only one of version or timestamp can be specified but not both.',
+                          'The requested stream was not found. Please make sure the request uri is correct.',
+                          'stream did not contain valid UTF-8',
+                          'Authentication failed when trying to access the stream',
+                          'Unable to find any delta table metadata',
+                          'Range requested from the service is invalid for current stream.',
+                          'Invalid Parquet file.')
+
+
+def _validate_downloads(download_records, ignore_not_found, logger):
+    if not download_records:
         return []
-    from azureml.dataprep.native import DataPrepError, StreamInfo
-    # Check the download error for each record if its user error
-    actual_download_list = []
-    error_list = []
+
+    from azureml.dataprep.native import StreamInfo, DataPrepError
+    downloaded_files = []
+    errors = []
     for record in download_records:
         value = record['DestinationFile']
         if isinstance(value, StreamInfo):
-            actual_download_list.append(value.resource_identifier)
+            downloaded_files.append(value.resource_identifier)
         elif isinstance(value, DataPrepError):
             resource_identifier = value.originalValue
-            if ignore_not_found and value.errorCode == "Microsoft.DPrep.ErrorValues.SourceFileNotFound":
+            error_code = value.errorCode
+            if ignore_not_found and error_code == 'Microsoft.DPrep.ErrorValues.SourceFileNotFound':
                 logger.warning(f"'{resource_identifier}' hasn't been downloaded as it was not present at the source. \
                                Download is proceeding.")
             else:
-                error_list.append((resource_identifier, value.errorCode))
+                errors.append((resource_identifier, error_code))
         else:
-            raise RuntimeError(f'Unexpected error during file download.{value}')
+            raise RuntimeError(f'Unexpected error during file download: {value}')
 
-    if error_list:
-        # this will throw ValueError for user error
-        # or RuntimeError for system error based on set of errors encountered
-        _download_error_handler(error_list)
-    return actual_download_list
-
-
-def _download_error_handler(error_list):
-    def is_user_error(error: str):
-        # TODO needs to check the updated rslex error code
-        return error in [
-            "Microsoft.DPrep.ErrorValues.SourceFileNotFound",
-            "Microsoft.DPrep.ErrorValues.SourceFilePermissionDenied",
-            "Microsoft.DPrep.ErrorValues.InvalidArgument",
-            "Microsoft.DPrep.ErrorValues.ValueWrongKind",
-            "Microsoft.DPrep.ErrorValues.SourcePermissionDenied",
-            "Microsoft.DPrep.ErrorValues.DestinationPermissionDenied",
-            "Microsoft.DPrep.ErrorValues.DestinationDiskFull",
-            "Microsoft.DPrep.ErrorValues.FileSizeChangedWhileDownloading",
-            "Microsoft.DPrep.ErrorValues.StreamInfoInvalidPath",
-            "Microsoft.DPrep.ErrorValues.NoManagedIdentity",
-            "Microsoft.DPrep.ErrorValues.NoOboEndpoint",
-            "Microsoft.DPrep.ErrorValues.StreamInfoRequired"
-        ]
-
-    message = 'Some files have failed to download:' + '\n'.join(
-        [str((file_name, error_code)) for (file_name, error_code) in error_list])
-    all_user_errors = True
-    system_error_msg = "System error happens during downloading: "
-    for (_, error) in error_list:
-        if not is_user_error(error):
-            all_user_errors = False
-            system_error_msg += error
-            break
-    raise ValueError(message) if all_user_errors else RuntimeError(system_error_msg)
-
-
-# temp user error classification
-def _classify_known_user_error(exception_message, ex=None):
-    value_errors = ["InvalidUriScheme", "StreamError(NotFound)", "DataAccessError(NotFound)",
-                    "No such host is known", "No identity was found on compute", "Make sure uri is correct",
-                    "Authentication failed when trying to access the stream",
-                    "Invalid JSON in log record",
-                    "Invalid table version",
-                    "Only one of version or timestamp can be specified but not both.",
-                    "Unable to find any delta table metadata",
-                    "The requested stream was not found"]
-    if 'Python expression parse error' in exception_message:
-        raise ValueError(f'Not a valid python expression in filter. {exception_message}') from ex
-    elif 'ExecutionError(StreamError(PermissionDenied' in exception_message:
-        raise ValueError(f'Getting permission error'
-                         f'please make sure proper access is configured on storage: {exception_message}')
-    elif getattr(ex, 'error_code', None) in ('ScriptExecution.Validation', 'ScriptExecution.StreamAccess.NotFound'):
-        raise ValueError(exception_message)
-    elif any(val_error in exception_message for val_error in value_errors):
-        raise ValueError(exception_message)
-    else:
-        raise SystemError(exception_message)
+    if errors:
+        _download_error_handler(errors)
+    return downloaded_files
+
+
+def _download_error_handler(errors):
+    non_user_errors = [error for error in errors if error[1] not in _RSLEX_USER_ERROR_VALUES]
+    if non_user_errors:
+        raise RuntimeError(f'System errors occured during downloading: {non_user_errors}')
+    errors = '\n'.join(map(str, errors))
+    raise UserErrorException(f'Some files have failed to download: {errors}')
+
+
+def _reclassify_external_error(err):
+    """
+    Reclassifies some errors from outside of MLTable into UserErrorExceptions or RuntimeErrors.
+    """
+    if isinstance(err, (UserErrorException, RuntimeError)):  # just a safety net
+        raise err
+
+    err_msg = err.args[0]
+    # first check remaps errors from RSlex to UserErrorExceptions in following ways:
+    # - is a DataPrepException whose error_code attribute is in _DATAPREP_EXECEPTION_USER_ERROR_CODES or whose message
+    #   attribute contains am error value in _RSLEX_USER_ERROR_VALUES
+    # - error message contains any element in _RSLEX_USER_ERROR_MSGS
+    if ((isinstance(err, DataPrepException) or hasattr(err, 'error_code'))
+        and err.error_code in _DATAPREP_EXECEPTION_USER_ERROR_CODES) \
+            or any(user_err_msg in err_msg for user_err_msg in _RSLEX_USER_ERROR_MSGS) \
+            or (isinstance(err, DataPrepException)
+                and any(user_error_value in err.message for user_error_value in _RSLEX_USER_ERROR_VALUES)):
+        raise UserErrorException(err)
+    if 'Python expression parse error' in err_msg:
+        raise UserErrorException(f'Not a valid python expression in filter. {err_msg}')
+    if 'ExecutionError(StreamError(PermissionDenied' in err_msg:
+        raise UserErrorException(
+            f'Getting permission error please make sure proper access is configured on storage: {err_msg}')
+    raise RuntimeError(err_msg)
+
+
+def _wrap_rslex_function_call(func):
+    """
+    Maps Exceptions from the calling RSlex function to a UserErrorException or RuntimeError based on error context.
+    """
+    try:
+        return func()
+    except Exception as e:
+        _reclassify_external_error(e)
```

## mltable/mltable.py

```diff
@@ -19,21 +19,24 @@
 from azureml.dataprep.api._constants import ACTIVITY_INFO_KEY, ERROR_CODE_KEY, \
     COMPLIANT_MESSAGE_KEY, OUTER_ERROR_CODE_KEY
 from azureml.dataprep.api._dataframereader import get_dataframe_reader, to_pyrecords_with_preppy, \
     _execute
 from azureml.dataprep.api.mltable._mltable_helper import _read_yaml, _download_mltable_yaml, \
     _parse_path_format, _PathType, _is_tabular
 from azureml.dataprep.rslex import PyRsDataflow
+from azureml.dataprep.api.typeconversions import FieldType
+from azureml.dataprep.api._rslex_executor import ensure_rslex_environment
+from azureml.dataprep import UserErrorException
+
 from ._aml_utilities._aml_rest_client_helper import _get_data_asset_by_id, _get_data_asset_by_asset_uri, \
     _try_resolve_workspace_info, _has_sufficient_workspace_info, STORAGE_OPTION_KEY_AZUREML_SUBSCRIPTION, \
     STORAGE_OPTION_KEY_AZUREML_RESOURCEGROUP, STORAGE_OPTION_KEY_AZUREML_WORKSPACE
-
 from ._utils import _validate, _make_all_paths_absolute, _parse_workspace_context_from_longform_uri, _is_local_path, \
     _PATHS_KEY, MLTableYamlCleaner
-from ._validation_and_error_handler import _get_and_validate_download_list, _classify_known_user_error
+from ._validation_and_error_handler import _validate_downloads, _wrap_rslex_function_call, _reclassify_external_error
 from azureml.dataprep.api.typeconversions import FieldType
 from azureml.dataprep.api._rslex_executor import ensure_rslex_environment
 
 _APP_NAME = 'MLTable'
 _PUBLIC_API = 'PublicApi'
 _INTERNAL_API = 'InternalCall'
 _TRAITS_SECTION_KEY = 'traits'
@@ -47,33 +50,35 @@
 _EXTRACT_PARTITION_FORMAT_KEY = 'extract_columns_from_partition_format'
 _PARTITION_FORMAT_KEY = 'partition_format'
 _METADATA_KEY = 'metadata'
 _TRANSFORMATIONS_KEY = 'transformations'
 _READ_DELIMITED_KEY = 'read_delimited'
 _READ_JSON_KEY = 'read_json_lines'
 
-_CONVERT_COLUMNS_TYPES_TYPE_ERROR = ValueError('`convert_column_types` must be a dictionary where '
-                                               'key is Union[str, Tuple[str]] and '
-                                               'value is :class: mltable.DataType')
-
-_WITH_PARITION_SIZE_REQUIRED_TRANSFORMATIONS_ERROR = \
-    ValueError('`read_delimited` or `read_json_lines` transformation step required to update partition_size.')
+_CONVERT_COLUMNS_TYPES_TYPE_ERROR= UserErrorException('`convert_column_types` must be a dictionary where key is '
+                                                       'Union[str, Tuple[str]] and value is :class: mltable.DataType')
 
-_PERMISSION_DENIED_ERROR_MSG = 'ExecutionError(StreamError(PermissionDenied'
 
 _SIMPLE_TYPES = {
     FieldType.INTEGER: 'int',
     FieldType.BOOLEAN: 'boolean',
     FieldType.STRING: 'string',
     FieldType.DECIMAL: 'float',
     FieldType.DATE: 'datetime',
     FieldType.STREAM: 'stream_info'
 }
 
 
+def _log_exception(activity_logger, exception):
+    if hasattr(activity_logger, ACTIVITY_INFO_KEY):
+        activity_logger.activity_info['message'] = getattr(exception, COMPLIANT_MESSAGE_KEY, str(exception))
+        activity_logger.activity_info['error_code'] = getattr(exception, ERROR_CODE_KEY, '')
+        activity_logger.activity_info['outer_error_code'] = getattr(exception, OUTER_ERROR_CODE_KEY, '')
+
+
 class MLTableHeaders(Enum):
     """
     Defines options for how column headers are processed when reading data
     from files to create a MLTable.
 
     These enumeration values are used in the MLTable class.
     """
@@ -87,57 +92,62 @@
 
     @staticmethod
     def _parse(header):
         if isinstance(header, MLTableHeaders):
             return header
 
         if not isinstance(header, str):
-            raise ValueError(
-                'The header should be a string or an MLTableHeader enum')
+            raise UserErrorException('The header should be a string or an MLTableHeader enum')
 
         try:
             return MLTableHeaders[header.lower()]
         except KeyError:
-            raise KeyError(f"Given invalid header {str(header)}, supported "
-                           "headers are: 'no_header', 'from_first_file', "
-                           "'all_files_different_headers', "
-                           "and 'all_files_same_headers'.")
+            raise UserErrorException(f"Given invalid header {str(header)}, supported headers are: 'no_header', "
+                                     "'from_first_file', 'all_files_different_headers', and 'all_files_same_headers'.")
 
 
 class MLTableFileEncoding(Enum):
     """
     Defines options for how encoding are processed when reading data from
     files to create a MLTable.
 
     These enumeration values are used in the MLTable class.
     """
     utf8 = auto()
     iso88591 = auto()
     latin1 = auto()
     ascii = auto()
     utf16 = auto()
-    utf32 = auto()
     utf8bom = auto()
     windows1252 = auto()
 
     @staticmethod
     def _parse(encoding):
         if isinstance(encoding, MLTableFileEncoding):
             return encoding
         if not isinstance(encoding, str):
-            raise ValueError(
-                'The encoding should be a string or an MLTableFileEncoding enum')
+            raise UserErrorException('The encoding should be a string or an MLTableFileEncoding enum')
 
-        try:
-            return MLTableFileEncoding[encoding.lower()]
-        except KeyError:
-            raise KeyError(
-                f"Given invalid encoding '{encoding}', supported encodings "
-                "are 'utf8', 'iso88591', 'latin1', 'ascii', 'utf16', "
-                "'utf32', 'utf8bom' and 'windows1252'.")
+        if encoding in ("utf8", "utf-8", "utf-8 bom"):
+            return MLTableFileEncoding.utf8
+        if encoding in ("iso88591", "iso-8859-1"):
+            return MLTableFileEncoding.iso88591
+        if encoding in ("latin1", "latin-1"):
+            return MLTableFileEncoding.latin1
+        if encoding == "ascii":
+            return MLTableFileEncoding.ascii
+        if encoding in ("windows1252", "windows-1252"):
+            return MLTableFileEncoding.windows1252
+
+        raise UserErrorException(f"""Given invalid encoding '{encoding}', supported encodings are:
+                                 - utf8 as "utf8", "utf-8", "utf-8 bom"
+                                 - iso88591 as "iso88591" or "iso-8859-1"
+                                 - latin1 as "latin1" or "latin-1"
+                                 - utf16 as "utf16" or "utf-16"
+                                 - windows1252 as "windows1252" or "windows-1252\"""")
 
 
 class DataType:
     """
     Helper class for handling the proper manipulation of supported column types (int, bool, string, etc.).
     Currently used  with `MLTable.convert_column_types(...)` & `from_delimited_files(...)` for specifying which types
     to convert columns to. Different types are selected with `DataType.from_*(...)` methods.
@@ -155,32 +165,31 @@
             return DataType.to_int()
         if value == 'float':
             return DataType.to_float()
         if value == 'boolean':
             return DataType.to_bool()
         if value == 'stream_info':
             return DataType.to_stream()
-        raise ValueError("'{}' is not a supported string conversion for `mltable.DataType`, "
-                         "supported types are 'string', 'int', 'float', 'boolean', & 'stream_info'".format(value))
+        raise UserErrorException(f"'{value}' is not a supported string conversion for `mltable.DataType`, "
+                                 "supported types are 'string', 'int', 'float', 'boolean', & 'stream_info'")
 
     @staticmethod
     def _create(data_type):
         dt = DataType()
         dt._data_type = data_type
         dt._arguments = _SIMPLE_TYPES.get(data_type)
         return dt
 
     @staticmethod
     def _format_str_list(values, var_name):
         if values:
             if not isinstance(values, list):
                 values = [values]
             if not all(isinstance(x, str) for x in values):
-                raise ValueError(
-                    '`{0}` must be a non-empty list of strings or None'.format(var_name))
+                raise UserErrorException(f'`{var_name}` must be a non-empty list of strings or None')
             if len(values) == 0:
                 values = None
         return values
 
     @staticmethod
     def to_string():
         """Configure conversion to string."""
@@ -217,28 +226,25 @@
         :param mismatch_as: How cast strings that are neither in `true_values` or `false_values`; 'true' casts all as
             True, 'false' as False, and 'error' will error instead of casting. Defaults to None which equal to 'error'.
         :type mismatch_as: Optional[str]
         """
         dt = DataType._create(FieldType.BOOLEAN)
 
         if mismatch_as is not None and mismatch_as not in DataType._MISMATCH_AS_TYPES:
-            raise ValueError("`mismatch_as` can only be {}".format(
-                DataType._MISMATCH_AS_TYPES))
+            raise UserErrorException(f"`mismatch_as` can only be {DataType._MISMATCH_AS_TYPES}")
 
         true_values = DataType._format_str_list(true_values, 'true_values')
         false_values = DataType._format_str_list(false_values, 'false_values')
 
         if (true_values is None) != (false_values is None):
-            raise ValueError(
-                '`true_values` and `false_values` must both be None or non-empty list of strings')
+            raise UserErrorException('`true_values` and `false_values` must both be None or non-empty list of strings')
 
         if true_values is not None and false_values is not None \
                 and (len(set(true_values).intersection(false_values)) > 0):
-            raise ValueError(
-                '`true_values` and `false_values` can not have overlapping values')
+            raise UserErrorException('`true_values` and `false_values` can not have overlapping values')
 
         type_name = dt._arguments
         args = {type_name: {}}
         if true_values and false_values:
             args[type_name]['true_values'] = true_values
             args[type_name]['false_values'] = false_values
             args[type_name]['mismatch_as'] = 'error'
@@ -323,71 +329,68 @@
     # Used to normalize columns into a list of strings or tuple of strings
     type_to_check = list if not is_tuple else tuple
     if isinstance(columns, str):
         return [columns] if type_to_check == list else columns
     elif isinstance(columns, type_to_check) and len(columns) > 0 and all(isinstance(col, str) for col in columns):
         return columns
     supported_types = {list: 'list', tuple: 'tuple'}
-    raise ValueError(
-        "'columns': {} should be a string or {} of strings with "
-        "at least one element".format(columns, supported_types[type_to_check]))
+    raise UserErrorException(f"'columns': {columns} should be a string or {supported_types[type_to_check]} of "
+                             "strings with at least one element")
 
 
 def _check_no_duplicate_columns(cols):
     """
-    Raises a ValueError is there are any duplicate columns in the given list of column groups (single of multiple
-    columns.
+    Raises a UserErrorException is there are any duplicate columns in the given list of column groups (single of
+    multiple columns.
     :param cols:
     :type cols: List[Union[str, Tuple[str]]]
     :return: None
     :rtype: None
     """
     seen_columns = set()
     for group in cols:
         if isinstance(group, str):
             group = (group,)
 
         for column in group:
             if column in seen_columns:
-                raise ValueError("Found duplicate column. Cannot convert column '{}' to multiple `mltable.DataType`s."
-                                 .format(column))
+                raise UserErrorException(f"Found duplicate column. Cannot convert column '{column}' to multiple "
+                                         "`mltable.DataType`s.")
 
         seen_columns.update(group)
 
 
 def _load_mltable_from_legacy_dataset(asset_id, storage_options=None):
     # only expect asset id in remote job in prod + local e2e test scenario
     asset = _get_data_asset_by_id(asset_id, storage_options)
     mltable_string = asset.legacy_dataflow
     if not mltable_string or mltable_string == '':
-        raise RuntimeError(
-            f'Data asset service returned invalid MLTable yaml for asset {asset_id}')
+        raise RuntimeError(f'Data asset service returned invalid MLTable yaml for asset {asset_id}')
 
     return yaml.safe_load(mltable_string)
 
 
 def _load_mltable_from_data_asset_uri(asset_uri_match, storage_options=None, enable_validation=True):
     # asset uri can be from local or remote
     data_asset = _get_data_asset_by_asset_uri(asset_uri_match, storage_options)
     is_v2 = data_asset.additional_properties['isV2']
     if is_v2:
         if data_asset.data_version.data_type != _APP_NAME:
-            raise ValueError('Can only load MLTable type asset')
+            raise UserErrorException('Can only load MLTable type asset')
         local_path = _download_mltable_yaml(data_asset.data_version.data_uri)
         mltable_dict = _read_yaml(local_path)
         mltable_dict = _make_all_paths_absolute(
             mltable_dict, data_asset.data_version.data_uri)
         if enable_validation:
             _validate(mltable_dict)
         return mltable_dict
 
     mltable_string = data_asset.additional_properties['legacyDataflow']
     if not mltable_string:
-        raise RuntimeError(f'Data asset service returned invalid MLTable '
-                           f'YAML file for asset '
+        raise RuntimeError('Data asset service returned invalid MLTable YAML file for asset '
                            f'{asset_uri_match[3]}:{asset_uri_match[4]}')
     return yaml.safe_load(mltable_string)
 
 @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
 def load(uri, storage_options: dict = None):
     """
     Loads the MLTable file (YAML) present at the given uri.
@@ -430,22 +433,22 @@
     :rtype: mltable.MLTable
     """
     return _load(uri, storage_options, True)
 
 @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
 def _load(uri, storage_options: dict = None, enable_validate = False):
     """
-    Loads the MLTable file(YAML) present at the given uri. This is private api without validations compare to load()
+    Loads the MLTable file (YAML) present at the given uri. This is private api without validations compare to load()
     """
     custom_dimensions = {'app_name': _APP_NAME}
     workspace_context = _parse_workspace_context_from_longform_uri(uri)
     if workspace_context:
         custom_dimensions.update(workspace_context)
     with _LoggerFactory.track_activity(_get_logger(), 'load', _PUBLIC_API,
-                                       custom_dimensions) as activityLogger:
+                                       custom_dimensions) as activity_logger:
         try:
             path_type, base_path, match = _parse_path_format(uri)
             if path_type == _PathType.local:
                 base_path = os.path.abspath(base_path)
                 mltable_dict = _read_yaml(base_path)
                 if enable_validate:
                     _validate(mltable_dict)
@@ -463,33 +466,27 @@
                 base_path = None
             elif path_type == _PathType.data_asset_uri:
                 mltable_dict = _load_mltable_from_data_asset_uri(
                     match, storage_options, enable_validate)
                 # path has been mapped to absolute path in _load_mltable_from_data_asset_uri
                 base_path = None
             else:
-                raise ValueError('The uri should be a valid path to a local or cloud directory which contains an '
-                                 'MLTable file.')
+                raise UserErrorException(
+                    'The uri should be a valid path to a local or cloud directory which contains an MLTable file.')
             # v1 sql dataset doesnt have paths
             orig_paths = mltable_dict.get(_PATHS_KEY)  # keep relative local file paths
             mltable_dict = _make_all_paths_absolute(mltable_dict, base_path)
             mltable_loaded = MLTable._create_from_dict(mltable_dict=mltable_dict, orig_paths=orig_paths)
             workspace_context = _parse_workspace_context_from_longform_uri(uri)
             mltable_loaded._workspace_context = workspace_context
             _LoggerFactory.trace(_get_logger(), "load", workspace_context)
             return mltable_loaded
         except Exception as ex:
-            if hasattr(activityLogger, ACTIVITY_INFO_KEY):
-                activityLogger.activity_info['message'] = getattr(ex, COMPLIANT_MESSAGE_KEY, str(ex))
-                activityLogger.activity_info['error_code'] = getattr(ex, ERROR_CODE_KEY, '')
-                activityLogger.activity_info['outer_error_code'] = getattr(ex, OUTER_ERROR_CODE_KEY, '')
-
-            if _PERMISSION_DENIED_ERROR_MSG in ex.args[0]:
-                raise ValueError(ex.args[0])
-            raise
+            _log_exception(activity_logger, ex)
+            _reclassify_external_error(ex)
 
 
 @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
 def from_delimited_files(paths, header='all_files_same_headers', delimiter=",", support_multi_line=False,
                          empty_as_string=False, encoding='utf8', include_path_column=False, infer_column_types=True):
     """
     Creates a MLTable from the given list of delimited files.
@@ -549,16 +546,21 @@
                 #  0  A1      B1  C1
                 #  1  A2  B\\r\\n2  C2
 
     :type support_multi_line: bool
     :param empty_as_string: How empty fields should be handled. If True will read empty fields as empty strings, else
         read as nulls. If True and column contains datetime or numeric data, empty fields still read as nulls.
     :type empty_as_string: bool
-    :param encoding: Specifies the file encoding using the enum :class:`mltable.MLTableFileEncoding`. Supported file
-        encodings are 'utf8', 'iso88591', 'latin1', 'ascii', 'utf16', 'utf32', 'utf8bom' and 'windows1252'.
+    :param encoding: Specifies the file encoding using the enum :class:`mltable.MLTableFileEncoding`. Supported
+                     encodings are:
+                     - utf8 as "utf8", "utf-8", "utf-8 bom"
+                     - iso88591 as "iso88591" or "iso-8859-1"
+                     - latin1 as "latin1" or "latin-1"
+                     - utf16 as "utf16" or "utf-16"
+                     - windows1252 as "windows1252" or "windows-1252"
     :type encoding: typing.Union[str, mltable.MLTableFileEncoding]
     :param include_path_column: Keep path information as a column in the MLTable, is useful when reading multiple files
         and you want to know which file a particular record came from, or to keep useful information that may be stored
         in a file path.
     :type include_path_column: bool
     :param infer_column_types: If True, automatically infers all column types. If False, leaves columns as strings. If
         a dictionary, represents columns whose types are to be set to given types (with all other columns being
@@ -588,45 +590,45 @@
 
     :type infer_column_types:
         typing.Union[bool, dict[str, typing.Union[str, dict[typing.Union[typing.Tuple[str], str], mltable.DataType]]]
     :return: MLTable instance
     :rtype: mltable.MLTable
     """
     if not isinstance(infer_column_types, (bool, dict)):
-        raise ValueError(
-            '`infer_column_types` must be a bool or a dictionary.')
+        raise UserErrorException('`infer_column_types` must be a bool or a dictionary.')
 
     if isinstance(infer_column_types, dict):
         if not infer_column_types:
             infer_column_types = True
         elif len(set(infer_column_types) - set(['sample_size', 'column_type_overrides'])) > 0:
-            raise ValueError('If `infer_column_types` is a dictionary, may only contain keys '
-                             '`sample_size` and `column_type_overrides`.')
+            raise UserErrorException('If `infer_column_types` is a dictionary, may only contain keys '
+                                     '`sample_size` and `column_type_overrides`.')
         elif 'sample_size' in infer_column_types \
                 and not (isinstance(infer_column_types['sample_size'], int) and infer_column_types['sample_size'] > 0):
-            raise ValueError('If `infer_column_types` is a dictionary with a `sample_size` key, '
-                             'its value must be a positive integer.')
+            raise UserErrorException('If `infer_column_types` is a dictionary with a `sample_size` key, '
+                                     'its value must be a positive integer.')
         elif 'column_type_overrides' in infer_column_types:
             if not isinstance(infer_column_types['column_type_overrides'], dict):
-                raise ValueError('If `infer_column_types` is a dictionary with a `column_type_overrides` key, '
-                                 'its value must be a dictionary of strings to `mltable.DataType`s or strings.')
+                raise UserErrorException('If `infer_column_types` is a dictionary with a `column_type_overrides` key, '
+                                         'its value must be a dictionary of strings to `mltable.DataType`s or strings.')
 
             _check_no_duplicate_columns(
                 infer_column_types['column_type_overrides'])
             overrides = []
             for k, v in infer_column_types['column_type_overrides'].items():
                 if not (isinstance(k, str) or (isinstance(k, tuple) and all(isinstance(x, str) for x in k))):
-                    raise ValueError('If `infer_column_types` is a dictionary with a `column_type_overrides` key,'
-                                     'the keys in its dictionary value must be strings or tuples of strings.')
+                    raise UserErrorException('If `infer_column_types` is a dictionary with a `column_type_overrides` '
+                                             'key, the keys in its dictionary value must be strings or tuples of '
+                                             'strings.')
 
                 v = DataType._from_raw(v)
                 if not isinstance(v, DataType):
-                    raise ValueError('If `infer_column_types` is a dictionary with a `column_type_overrides` key,'
-                                     'the values in its dictionary value must be supported strings or '
-                                     '`mltable.DataType`s.')
+                    raise UserErrorException('If `infer_column_types` is a dictionary with a `column_type_overrides` '
+                                             'key, the values in its dictionary value must be supported strings or '
+                                             '`mltable.DataType`s.')
 
                 overrides.append({'columns': k, 'column_type': v._arguments})
 
             infer_column_types['column_type_overrides'] = overrides
 
     header = MLTableHeaders._parse(header)
     encoding = MLTableFileEncoding._parse(encoding)
@@ -690,25 +692,30 @@
         relative to the current working directory. If the parent directory a local file path is relative to is not the
         current working directory, instead recommend passing that path as a absolute file path.
     :type paths: list[dict[str, str]]
     :param invalid_lines: How to handle lines that are invalid JSON, ct can be 'drop' or 'error'.
         If its 'drop', it just drop invalid lines, otherwise it will fail.
     :type invalid_lines: str
     :param encoding: Specifies the file encoding using the enum :class:`mltable.MLTableFileEncoding`. Supported file
-        encodings are 'utf8', 'iso88591', 'latin1', 'ascii', 'utf16', 'utf32', 'utf8bom' and 'windows1252'.
+        encodings:
+        - utf8 as "utf8", "utf-8", "utf-8 bom"
+        - iso88591 as "iso88591" or "iso-8859-1"
+        - latin1 as "latin1" or "latin-1"
+        - utf16 as "utf16" or "utf-16"
+        - windows1252 as "windows1252" or "windows-1252"
     :type encoding: typing.Union[str, mltable.MLTableFileEncoding]
     :param include_path_column: Keep path information as a column, useful when reading multiple files and you want
         to know which file a particular record came from, or to keep useful information that may be stored in a file
         path.
     :type include_path_column: bool
     :return: MLTable
     :rtype: mltable.MLTable
     """
     if invalid_lines not in ['error', 'drop']:
-        raise ValueError("Invalid value for invalid_lines, the supported values are ['error', 'drop']")
+        raise UserErrorException("Invalid value for invalid_lines, the supported values are ['error', 'drop']")
 
     encoding = MLTableFileEncoding._parse(encoding)
     return from_paths(paths)._add_transformation_step(_READ_JSON_KEY,
                                                      {"invalid_lines": invalid_lines,
                                                       "encoding": encoding.name,
                                                       "include_path_column": include_path_column})
 
@@ -788,24 +795,25 @@
         to know which file a particular record came from, or to keep useful information that may be stored in a file
         path.
     :type include_path_column: bool
     :return: MLTable instance
     :rtype: mltable.MLTable
     """
     if timestamp_as_of and version_as_of:
-        raise KeyError("Both timestamp_as_of and version_as_of parameters were provided, but only one of version_as"
-                       "_of or timestamp_as_of can be specified.")
+        raise UserErrorException("Both timestamp_as_of and version_as_of parameters were provided, but only one of "
+                                 "version_as_of or timestamp_as_of can be specified.")
 
     if timestamp_as_of:
         rfc3339_checker = re.compile(r'^((?:(\d{4}-(0[1-9]|1[0-2])-([0-3]\d))'
                                      r'T(\d{2}:\d{2}:\d{2}(?:\.\d+)?))(Z|[\+-]\d{2}:\d{2})?)$')
         if rfc3339_checker.match(timestamp_as_of) is None:
-            raise ValueError('Provided timestamp_as_of: {} is not in RFC-3339/ISO-8601 format. Please make sure '
-                             'that it adheres to RFC-3339/ISO-8601 format. For example: "2022-10-01T00:00:00Z",'
-                             '"2022-10-01T22:10:57+02:00", "2022-10-01T16:32:11.8+00:00" are correctly formatted.')
+            raise UserErrorException(f'Provided timestamp_as_of: {timestamp_as_of} is not in RFC-3339/ISO-8601 format. '
+                                     'Please make sure that it adheres to RFC-3339/ISO-8601 format. For example: '
+                                     '"2022-10-01T00:00:00Z", "2022-10-01T22:10:57+02:00", '
+                                     '"2022-10-01T16:32:11.8+00:00" are correctly formatted.')
 
     mltable = from_paths([{"folder": delta_table_uri}])
     return mltable._add_transformation_step('read_delta_lake',
                                             {'version_as_of': version_as_of,
                                              'timestamp_as_of': timestamp_as_of,
                                              'include_path_column': include_path_column})
 
@@ -825,83 +833,100 @@
 
         This constructor is not supposed to be invoked directly. MLTable is
         intended to be created using :func:`mltable.load`.
         """
         self._loaded = False
 
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
+    def _to_yaml_dict(self):
+        """
+        Returns all the information associated with MLTable as a YAML-style dictionary.
+
+        :return: dict representation of this MLTable
+        :rtype: dict
+        """
+        return yaml.safe_load(str(self))
+
+    @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def __repr__(self):
         """
         Returns all the information associated with MLTable as a YAML-style
         string representation.
 
         :return: string representation of this MLTable
         :rtype: str
         """
         self._check_loaded()
         # _dataflow.to_yaml_string() serializes Serde units (anonymous value containing no data) as nulls
         # this results in nested fields with empty values being serialized with nulls as value.
-        mltable_yaml_str = self._dataflow.to_yaml_string()
+        mltable_yaml_str = _wrap_rslex_function_call(lambda: self._dataflow.to_yaml_string())
         mltable_yaml_dict = yaml.safe_load(mltable_yaml_str)
         mltable_yaml_helper = MLTableYamlCleaner(mltable_yaml_dict=mltable_yaml_dict)
         return str(mltable_yaml_helper)
 
+    def __str__(self):
+        """
+        Returns all the information associated with MLTable as a YAML-style
+        string representation.
+
+        :return: string representation of this MLTable
+        :rtype: str
+        """
+        return self.__repr__()
+
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def validate(self):
         """
         Validates if this MLTable's data can be loaded, requires the MLTable's
         data source(s) to be accessible from the current compute.
 
         :return: None
         :rtype: None
         """
         failed_to_load_error_msg = \
             'Can not load data from this MLTable\'s associated datastores. Please check the associated paths.'
 
-        mltable_yaml_str = self.take(1)._dataflow.to_yaml_string()
+        mltable_yaml_str = str(self.take(1))
         try:
-            records = to_pyrecords_with_preppy(
-                'MLTable.validate', mltable_yaml_str)
-        except Exception as e:  # this is too broad, but dataprepreader throws different errors
-            raise ValueError(
-                f'{failed_to_load_error_msg} Additional error info: "{e}"')
+            records = _wrap_rslex_function_call(lambda: to_pyrecords_with_preppy('MLTable.validate', mltable_yaml_str))
+        except Exception:  # this is broad, but dataprepreader throws different errors
+            raise RuntimeError(failed_to_load_error_msg)
 
         if len(records) < 1:
-            raise ValueError(failed_to_load_error_msg)
+            raise RuntimeError(failed_to_load_error_msg)
 
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def _ensure_random_seed(self, seed):
         """
-        If the given seed is not an integer or None, raises a ValueError. If
+        If the given seed is not an integer or None, raises a UserErrorException. If
         None selects a random seed randomly between 1 and 1000.
 
         :param seed: possible value for random seed
         :type seed: object
         :return: valid random seed
         :rtype: int
         """
         if seed is None:
             return random.randint(1, 1000)
         elif not isinstance(seed, int):
-            raise ValueError('A random seed must be an integer')
+            raise UserErrorException('A random seed must be an integer')
         return seed
 
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def _check_loaded(self):
         if not self._loaded:
-            raise ValueError('MLTable does not appear to be loaded correctly. '
-                             'Please use MLTable.load() to load a MLTable YAML'
-                             ' file into memory.')
+            raise UserErrorException('MLTable does not appear to be loaded correctly. Please use MLTable.load() to '
+                                     'load a MLTable YAML file into memory.')
 
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def _sanitize_and_check_datatype(self, column_types):
         # Function used to type check the inputs for convert_column_types API
         if len(column_types) == 0 or not isinstance(column_types, dict):
-            raise ValueError(
-                'Input type is dict[Union[str, Tuple[str]]: mltable.DataType] with at least one entry')
+            raise UserErrorException('Input type is dict[Union[str, Tuple[str]]: mltable.DataType] with at least '
+                                     'one entry')
 
         _check_no_duplicate_columns(
             [_normalize_column_inputs(x, is_tuple=True) for x in column_types])
         for conversion in column_types.values():
             if not isinstance(conversion, DataType):
                 raise _CONVERT_COLUMNS_TYPES_TYPE_ERROR
 
@@ -918,26 +943,25 @@
         :param args: arguments for given transformation step
         :type: object
         :param index: optional argument to indicate which index to add the step
         :type: int
         :return: MLTable with resulting PyRsDataflow
         :rtype: mltable.MLTable
         """
-        new_dataflow = self._dataflow.add_transformation(step, args, index)
+        new_dataflow = _wrap_rslex_function_call(lambda: self._dataflow.add_transformation(step, args, index))
         return MLTable._create_from_dataflow(new_dataflow, self.paths)
 
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def _normalize_partition_int_list(self, columns):
         if isinstance(columns, int):
             return [columns]
         elif isinstance(columns, list) and len(columns) > 0 \
                 and all(isinstance(index, int) for index in columns):
             return columns
-        raise ValueError(
-            'Columns should be a int or list of int with at least one element')
+        raise UserErrorException('Columns should be a int or list of int with at least one element')
 
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def _get_columns_in_traits(self):
         """
         Gets all the columns that are set in this MLTable's Traits.
 
         :return: set of all Traits
@@ -1000,46 +1024,39 @@
         # append workspace information for the stream_column for backwards support
         # AmlDatastore://workspaceblobstore/data/images/animals folder/1d.jpg
         workspace_info = _try_resolve_workspace_info(storage_options)
         if _has_sufficient_workspace_info(workspace_info):
             new_mltable = \
                 MLTable._append_workspace_to_stream_info_conversion(new_mltable, workspace_info, stream_column)
 
-        mltable_yaml_str = new_mltable._dataflow.to_yaml_string()
-        try:
-            download_records = to_pyrecords_with_preppy('MLTable._download', mltable_yaml_str)
-            actual_download_list = _get_and_validate_download_list(download_records, ignore_not_found, _get_logger())
-            return actual_download_list
-        except Exception as ex:
-            _classify_known_user_error(ex.args[0])
+        download_records = _wrap_rslex_function_call(
+            lambda: to_pyrecords_with_preppy('MLTable._download', str(new_mltable)))
+        return _validate_downloads(download_records, ignore_not_found, _get_logger())
 
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def _with_partition_size(self, min_batch_size):
         """
         Update a transformation step to use the partition size of defined. Support delimited files and json files.
 
         :param min_batch_size: minimum batch size to partition the data
         :type min_batch_size: int
         :return: MLTable with partition size updated
         :rtype: mltable.MLTable
         """
         self._check_loaded()
         orig_paths = self.paths
-        mltable_yaml_str = self._dataflow.to_yaml_string()
-        mltable_yaml_dict = yaml.safe_load(mltable_yaml_str)
-
-        if _TRANSFORMATIONS_KEY not in mltable_yaml_dict:
-            raise _WITH_PARITION_SIZE_REQUIRED_TRANSFORMATIONS_ERROR
-
-        for key in [_READ_DELIMITED_KEY, _READ_JSON_KEY]:
-            if key in mltable_yaml_dict[_TRANSFORMATIONS_KEY][0]:  # TODO ideally this should be in Rust
+        mltable_yaml_dict = self._to_yaml_dict()
+        support_transformations = [_READ_DELIMITED_KEY, _READ_JSON_KEY]
+        for key in support_transformations:
+            if key in mltable_yaml_dict[_TRANSFORMATIONS_KEY][0]:
                 mltable_yaml_dict[_TRANSFORMATIONS_KEY][0][key]['partition_size'] = min_batch_size
                 return MLTable._create_from_dict(mltable_dict=mltable_yaml_dict, orig_paths=orig_paths)
+        raise UserErrorException(
+            'transformation step read_delimited or read_json_lines is required to update partition_size')
 
-        raise _WITH_PARITION_SIZE_REQUIRED_TRANSFORMATIONS_ERROR
 
     def to_pandas_dataframe(self):
         """
         Load all records from the paths specified in the MLTable file into a
         Pandas DataFrame.
 
         .. remarks::
@@ -1055,51 +1072,37 @@
                 pdf = tbl.to_pandas_dataframe()
                 print(pdf.shape)
 
         :return: Pandas Dataframe containing the records from paths in this
                  MLTable
         :rtype: pandas.DataFrame
         """
+        self._check_loaded()
         custom_dimensions = {'app_name': _APP_NAME}
         if self._workspace_context:
             custom_dimensions.update(self._workspace_context)
 
-        with _LoggerFactory.track_activity(_get_logger(), 'to_pandas_dataframe', _PUBLIC_API,
-                                           custom_dimensions) as activityLogger:
+        mltable_yaml_str = str(self)
+        with _LoggerFactory.track_activity(_get_logger(), 'to_pandas_dataframe', _PUBLIC_API, custom_dimensions) \
+                as activity_logger:
             try:
-                self._check_loaded()
-                try:
-                    mltable_yaml_str = self._dataflow.to_yaml_string()
-                    dataframe_reader = get_dataframe_reader()
-                    df = dataframe_reader.to_pandas_dataframe(mltable_yaml_str)
-                    return df
-                except Exception as e:
-                    message = e.args[0]
-                    _classify_known_user_error(message, e)
+                return _wrap_rslex_function_call(lambda: get_dataframe_reader().to_pandas_dataframe(mltable_yaml_str))
             except Exception as e:
-                if hasattr(activityLogger, ACTIVITY_INFO_KEY):
-                    activityLogger.activity_info['error_code'] = getattr(
-                        e, ERROR_CODE_KEY, '')
-                    activityLogger.activity_info['message'] = getattr(
-                        e, COMPLIANT_MESSAGE_KEY, str(e))
-                    activityLogger.activity_info['outer_error_code'] = getattr(
-                        e, OUTER_ERROR_CODE_KEY, '')
-
+                _log_exception(activity_logger, e)
                 raise e
 
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def get_partition_count(self) -> int:
         """
         Calculates the partitions for the current mltable and returns their count.
 
         :return: The count of partitions.
         """
-        mltable_yaml_str = self._dataflow.to_yaml_string()
         from azureml.dataprep.api._dataframereader import get_partition_count_with_rslex
-        return get_partition_count_with_rslex(mltable_yaml_str)
+        return _wrap_rslex_function_call(lambda: get_partition_count_with_rslex(str(self)))
 
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def select_partitions(self, partition_index_list):
         """
         Adds a transformation step to select the partition.
 
         .. remarks::
@@ -1168,21 +1171,20 @@
         :param partition_keys: partition keys
         :type partition_keys: builtin.list[str]
         """
         self._check_loaded()
         if not partition_keys:
             partition_keys = self.partition_keys
         if not self.partition_keys:
-            raise Exception("cannot retrieve partition key values for a mltable that has no "
-                            "partition keys")
+            raise UserErrorException("cannot retrieve partition key values for a mltable that has no partition keys")
 
         invalid_keys = [
             x for x in partition_keys if x not in self.partition_keys]
         if len(invalid_keys) != 0:
-            raise ValueError(f"{invalid_keys} are invalid partition keys")
+            raise UserErrorException(f"{invalid_keys} are invalid partition keys")
 
         # currently use summarize to find the distinct result
         mltable = self.take(count=1)
         pd = mltable.to_pandas_dataframe()
         no_partition_key_columns = [
             x for x in pd.columns if x not in partition_keys]
         mltable = self
@@ -1306,22 +1308,22 @@
                 else:
                     pattern += c
                 i += 1
             if date_column is not None:
                 columns.append(date_column)
 
             if defined_date_parts and 'yyyy' not in defined_date_parts:
-                raise ValueError(
-                    f'Invalid partition_format "{partition_format}". {validation_error["NO_YEAR"]}')
+                raise UserErrorException(f'Invalid partition_format "{partition_format}". '
+                                         f'{validation_error["NO_YEAR"]}')
             return pattern, defined_date_parts, columns
 
         if len(self._partition_keys) > 0:
             return self._partition_keys
-        mltable_yaml_str = self._dataflow.to_yaml_string()
-        mltable_dict = yaml.safe_load(mltable_yaml_str)
+
+        mltable_dict = self._to_yaml_dict()
         if _TRANSFORMATIONS_KEY in mltable_dict:
             for mltable_transformation in mltable_dict[_TRANSFORMATIONS_KEY]:
                 if _EXTRACT_PARTITION_FORMAT_KEY in mltable_transformation:
                     parsed_result = parse_partition_format(
                         mltable_transformation[_EXTRACT_PARTITION_FORMAT_KEY][_PARTITION_FORMAT_KEY])
                     if len(parsed_result) == 3 and parsed_result[2]:
                         self._partition_keys = parsed_result[2]
@@ -1331,16 +1333,15 @@
     @property
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def _is_tabular(self):
         """
         check if this mltable is tabular using its yaml
         """
         self._check_loaded()
-        mltable_yaml_str = self._dataflow.to_yaml_string()
-        mltable_yaml = yaml.safe_load(mltable_yaml_str)
+        mltable_yaml = self._to_yaml_dict()
         return _is_tabular(mltable_yaml)
 
     @staticmethod
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def _create_from_dict(mltable_dict, orig_paths):
         """
         Creates a new MLTable from a dictionary containing information from
@@ -1350,16 +1351,16 @@
         :type mltable_dict: dict
         :param orig_paths: paths (relative or absolute) contained in the given
                            MLTable
         :type orig_paths: list[str]
         :return: MLTable from given dict
         :rtype: mltable.MLTable
         """
-        mltable_yaml_string = yaml.dump(mltable_dict)
-        dataflow = PyRsDataflow(mltable_yaml_string)
+        mltable_yaml_string = yaml.safe_dump(mltable_dict)
+        dataflow = _wrap_rslex_function_call(lambda: PyRsDataflow(mltable_yaml_string))
         return MLTable._create_from_dataflow(dataflow, orig_paths)
 
     @staticmethod
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def _create_from_dataflow(dataflow, orig_paths):
         """
         Creates a new MLTable from a PyRsDataflow.
@@ -1386,15 +1387,15 @@
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def _append_workspace_to_stream_info_conversion(mltable, workspace_info, stream_column):
         def _is_stream_column_in_column_conversion(columns_item):
             return 'stream_info' == columns_item['column_type']\
                    and ((isinstance(columns_item['columns'], str) and columns_item['columns'] == stream_column)
                         or (isinstance(columns_item['columns'], list) and stream_column in columns_item['columns']))
 
-        mltable_dict = yaml.safe_load(mltable._dataflow.to_yaml_string())
+        mltable_dict = mltable._to_yaml_dict()
         if _TRANSFORMATIONS_KEY in mltable_dict:
             columns_conversion_list = [columns_item for t in mltable_dict[_TRANSFORMATIONS_KEY]
                                        for k, v in t.items()
                                        if k == 'convert_column_types'
                                        for columns_item in v
                                        if _is_stream_column_in_column_conversion(columns_item)]
             if len(columns_conversion_list) == 0:
@@ -1425,15 +1426,15 @@
         :param count: number of rows from top of table to select
         :type count: int
         :return: MLTable with added "take" transformation step
         :rtype: mltable.MLTable
         """
         self._check_loaded()
         if not (isinstance(count, int) and count > 0):
-            raise ValueError('Number of rows must be a positive integer')
+            raise UserErrorException('Number of rows must be a positive integer')
         return self._add_transformation_step('take', count)
 
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def show(self, count=20):
         """
         Retrieves the first `count` rows of this MLTable as a Pandas Dataframe.
 
@@ -1456,16 +1457,15 @@
         :param seed: optional random seed
         :type seed: Optional[int]
         :return: MLTable with added transformation step
         :rtype: mltable.MLTable
         """
         self._check_loaded()
         if not (isinstance(probability, float) and 0 < probability < 1):
-            raise ValueError(
-                'Probability should an float greater than 0 and less than 1')
+            raise UserErrorException('Probability should an float greater than 0 and less than 1')
         seed = self._ensure_random_seed(seed)
         return self._add_transformation_step('take_random_sample',
                                              {"probability": probability,
                                               "seed": seed})
 
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def drop_columns(self, columns: Union[str, List[str]]):
@@ -1480,16 +1480,15 @@
         :return: MLTable with added transformation step
         :rtype: mltable.MLTable
         """
         self._check_loaded()
         columns = _normalize_column_inputs(columns)
         columns_in_traits = self._get_columns_in_traits()
         if not columns_in_traits.isdisjoint(columns):
-            raise ValueError(
-                'Columns in traits must be kept and cannot be dropped')
+            raise UserErrorException('Columns in traits must be kept and cannot be dropped')
         return self._add_transformation_step('drop_columns', columns)
 
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def keep_columns(self, columns: Union[str, List[str]]):
         """
         Adds a transformation step to keep the specified columns and drop all
         others from the dataset.
@@ -1502,16 +1501,15 @@
         :return: MLTable with added transformation step
         :rtype: mltable.MLTable
         """
         self._check_loaded()
         columns = _normalize_column_inputs(columns)
         columns_in_traits = self._get_columns_in_traits()
         if not columns_in_traits.issubset(columns):
-            raise ValueError(
-                'Columns in traits must be kept and cannot be dropped')
+            raise UserErrorException('Columns in traits must be kept and cannot be dropped')
         return self._add_transformation_step('keep_columns', columns)
 
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def random_split(self, percent=.5, seed=None):
         """
         Randomly splits this MLTable into two MLTables, one having
         approximately "percent"% of the original MLTable's data and the other
@@ -1522,16 +1520,15 @@
         :param seed: optional random seed
         :type seed: Optional[int]
         :return: two MLTables with this MLTable's data split between them by
                  "percent"
         :rtype: Tuple[mltable.MLTable, mltable.MLTable]
         """
         if not (isinstance(percent, float) and 0 < percent < 1):
-            raise ValueError(
-                'Percent should be a float greater than 0 and less than 1')
+            raise UserErrorException('Percent should be a float greater than 0 and less than 1')
         seed = self._ensure_random_seed(seed)
         split_a = self._add_transformation_step('sample', {"sampler": "random_percent",
                                                            "sampler_arguments": {
                                                                "probability": percent,
                                                                "probability_lower_bound": 0.0,
                                                                "seed": seed}})
         split_b = self._add_transformation_step('sample', {"sampler": "random_percent",
@@ -1585,22 +1582,22 @@
         self._check_loaded()
         if path is None:
             path = os.getcwd()
 
         abs_dirc_path = os.path.abspath(path)
 
         if os.path.isfile(abs_dirc_path):
-            raise ValueError(f'The given path {abs_dirc_path} points to a file.')
+            raise UserErrorException('The given path points to a file.')
 
         if not os.path.exists(abs_dirc_path):
             os.makedirs(abs_dirc_path, exist_ok=True)
 
         save_path = os.path.join(abs_dirc_path, 'MLTable')
         if not overwrite and os.path.exists(save_path):
-            raise ValueError(f'The given directory path {abs_dirc_path} already contains an MLTable YAML file.')
+            raise UserErrorException('The given directory path already contains an MLTable YAML file.')
 
         mltable_yaml_dict = yaml.safe_load(str(self))
 
         def format_path(file_path):
             if not _is_local_path(file_path):
                 return file_path
 
@@ -1639,15 +1636,15 @@
         :param count: number of rows to skip
         :type count: int
         :return: MLTable with added transformation step
         :type: mltable.MLTable
         """
         self._check_loaded()
         if not isinstance(count, int) or count < 1:
-            raise ValueError('Count must be an integer > 0.')
+            raise UserErrorException('Count must be an integer > 0.')
         return self._add_transformation_step('skip', count)
 
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def convert_column_types(self, column_types):
         """
         Adds a transformation step to convert the specified columns into their respective specified new types.
 
@@ -1729,72 +1726,62 @@
                     # before we create it. In this case, we can no-op as though the directory already existed.
                     pass
 
             is_empty = not any(files or dirnames for _,
                                dirnames, files in os.walk(path))
             return (os.path.abspath(path), is_empty)
 
-        mltable_yaml_str = self._dataflow.to_yaml_string()
-        hash_object = hashlib.md5(mltable_yaml_str.encode()).hexdigest()
+        mltable_yaml_str = str(self)
+        hash_object = hashlib.md5(str(self).encode()).hexdigest()
         dataflow_in_memory_uri = f'inmemory://dataflow/{hash_object}'
-        ensure_rslex_environment()
+
+        _wrap_rslex_function_call(ensure_rslex_environment)
         from azureml.dataprep.rslex import add_in_memory_stream
-        add_in_memory_stream(dataflow_in_memory_uri, mltable_yaml_str)
+        _wrap_rslex_function_call(lambda: add_in_memory_stream(dataflow_in_memory_uri, mltable_yaml_str))
 
         dataflow_in_memory_uri_encoded = urllib.parse.quote(dataflow_in_memory_uri.encode('utf8'), safe='')
 
         stream_column_encode = urllib.parse.quote(stream_column.encode('utf8'), safe='')
         dataflow_uri = f"rsdf://dataflowfs/{dataflow_in_memory_uri_encoded}/{stream_column_encode}/"
 
         mount_point, is_empty = _ensure_path(mount_point)
         if os.path.ismount(mount_point):
-            raise ValueError(
+            raise UserErrorException(
                 f'"{mount_point}" is already mounted. Run `sudo umount "{mount_point}"` to unmount it.')
+
         if not is_empty:
-            raise ValueError(
+            raise UserErrorException(
                 'mltable mount point must be empty, mounting to non-empty folder is not supported.')
 
         from azureml.dataprep.fuse.dprepfuse import rslex_uri_volume_mount, MountOptions
         mount_options = kwargs.get('mount_options', None)
         # this can be remove after default permission set for MountOption is ready
         if not mount_options:
             mount_options = MountOptions(data_dir_suffix=None)
 
-        try:
-            mount_context = rslex_uri_volume_mount(
-                uri=dataflow_uri, mount_point=mount_point, options=mount_options)
-            return mount_context
-        except BaseException as e:
-            message = str(e)
-            if any(errorName in message for errorName in ["StreamError(NotFound)",
-                                                          "DataAccessError(NotFound)",
-                                                          "DataAccessError(PermissionDenied)"]):
-                raise ValueError(message)
-            else:
-                raise e
+        return _wrap_rslex_function_call(
+            lambda: rslex_uri_volume_mount(uri=dataflow_uri, mount_point=mount_point, options=mount_options))
 
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def _execute(self) -> None:
         """
         Runs the current mltable using the local execution runtime.
         """
         force_clex = False
         allow_fallback_to_clex = True
         if '_TEST_USE_CLEX' in os.environ and os.environ['_TEST_USE_CLEX'] == 'True':
             force_clex = True
         elif '_TEST_USE_CLEX' in os.environ and os.environ['_TEST_USE_CLEX'] == 'False':
             allow_fallback_to_clex = False
 
-        mltable_yaml_str = self._dataflow.to_yaml_string()
-        try:
-            _execute('mltable._execute', mltable_yaml_str, force_clex=force_clex,
-                     allow_fallback_to_clex=allow_fallback_to_clex)
-        except Exception as ex:
-            _classify_known_user_error(ex.args[0])
-
+        _wrap_rslex_function_call(
+            lambda: _execute('mltable._execute',
+                             dataflow=str(self),
+                             force_clex=force_clex,
+                             allow_fallback_to_clex=allow_fallback_to_clex))
 
 class Metadata:
     """
     Class that maps to the metadata section of the MLTable.
 
     Supports the getting & adding of arbritrary metadata properties.
     """
@@ -1805,42 +1792,41 @@
         metadata = Metadata()
         metadata._mltable = mltable
         return metadata
 
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def _key_name_type_check(self, name):
         if not isinstance(name, str):
-            raise TypeError(
-                f'Metadata only supports string property names, but encountered {type(name)}.')
+            raise UserErrorException(f'Metadata only supports string property names, but encountered {type(name)}.')
 
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def has(self, key):
         """
         Returns if this MLTable's metadata has a property named `key`.
 
         :param key: property name to check for
         :type key: str
         :return: if metadata contains a property named `key`
         :rtype: bool
         """
         self._key_name_type_check(key)
-        return self._mltable._dataflow.has_schema_property(_METADATA_SCHEMA_NAME, key)
+        return _wrap_rslex_function_call(lambda: self._mltable._dataflow.has_schema_property(_METADATA_SCHEMA_NAME, key))
 
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def get(self, key):
         """
         Returns the value associated with the property `key` in this
         MLTable's metadata. If no such property exists, returns None.
 
         :param key: property name to retrieve value of
         :type key: str
         :return: value associated with `key`, or None if nonexistant
         :rtype: Optional[object]
         """
-        return self._mltable._dataflow.get_schema_property(_METADATA_SCHEMA_NAME, key) \
+        return _wrap_rslex_function_call(lambda: self._mltable._dataflow.get_schema_property(_METADATA_SCHEMA_NAME, key)) \
             if self.has(key) else None
 
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def add(self, key, value):
         """
         Sets the value of the property named `key` in this MLTable's metadata
         to `value`. If the value of `key` was previously set, the value is
@@ -1850,17 +1836,16 @@
         :type key: str
         :param value: value to set
         :type value: object
         :return: None
         :rtype: None
         """
         self._key_name_type_check(key)
-        self._mltable._dataflow = \
-            self._mltable._dataflow.set_schema_property(
-                _METADATA_SCHEMA_NAME, key, value)
+        self._mltable._dataflow = _wrap_rslex_function_call(
+            lambda: self._mltable._dataflow.set_schema_property(_METADATA_SCHEMA_NAME, key, value))
 
 
 class Traits:
     """
     Class that maps to the traits section of the MLTable.
 
     Currently supported traits: timestamp_column and index_columns
@@ -1871,84 +1856,82 @@
     def _create(mltable):
         traits = Traits()
         traits._mltable = mltable
         return traits
 
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def _has_trait(self, trait_name):
-        return self._mltable._dataflow.has_schema_property(_TRAITS_SCHEMA_NAME, trait_name)
+        return _wrap_rslex_function_call(
+            lambda: self._mltable._dataflow.has_schema_property(_TRAITS_SCHEMA_NAME, trait_name))
 
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def _get_trait(self, trait_name):
-        return self._mltable._dataflow.get_schema_property(_TRAITS_SCHEMA_NAME, trait_name)
+        return _wrap_rslex_function_call(
+            lambda: self._mltable._dataflow.get_schema_property(_TRAITS_SCHEMA_NAME, trait_name))
 
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def _check_and_get_trait(self, trait_name):
         return self._get_trait(trait_name) if self._has_trait(trait_name) else None
 
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def _set_trait(self, trait_name, trait_value):
-        self._mltable._dataflow = \
-            self._mltable._dataflow.set_schema_property(
-                _TRAITS_SCHEMA_NAME, trait_name, trait_value)
+        self._mltable._dataflow = _wrap_rslex_function_call(
+            lambda: self._mltable._dataflow.set_schema_property(_TRAITS_SCHEMA_NAME, trait_name, trait_value))
 
     @property
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def timestamp_column(self):
         """
-        If set returns timestamp column name, else rasies a KeyError.
+        If set returns timestamp column name, else rasies a UserErrorException.
 
         :return: timestamp column name
         :rtype: str
         """
         col = self._check_and_get_trait(_TIMESTAMP_COLUMN_KEY)
         if col is None:
-            raise KeyError(
-                'Timestamp column does not appear to be set. Please make sure you have set it.')
+            raise UserErrorException('Timestamp column does not appear to be set. Please make sure you have set it.')
         return col
 
     @timestamp_column.setter
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def timestamp_column(self, column_name):
         """
         Setter for timestamp_column trait.
 
         :param column_name: Name of the timestamp column.
         :type column_name: str
         :return: MLTable with timestamp column set to given column
         :rtype: mltable.MLTable
         """
         if not isinstance(column_name, str):
-            raise TypeError(
-                f'An object of type string is expected, but encountered type: {type(column_name)}')
+            raise UserErrorException(f'An object of type string is expected, but encountered type: {type(column_name)}')
         self._set_trait(_TIMESTAMP_COLUMN_KEY, column_name)
 
     @property
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def index_columns(self):
         """
-        If set returns a list of index columns' names, else raises a KeyError
+        If set returns a list of index columns' names, else raises a UserErrorException.
 
         :return: list of index column names
         :rtype: list[str]
         """
         col = self._check_and_get_trait(_INDEX_COLUMNS_KEY)
         if col is None:
-            raise KeyError(
-                'Index columns do not appear to be set. Please make sure you have set them.')
+            raise UserErrorException('Index columns do not appear to be set. Please make sure you have set them.')
         return col
 
     @index_columns.setter
     @track(_get_logger, custom_dimensions={'app_name': _APP_NAME})
     def index_columns(self, index_columns_list):
         """
         Setter for index_columns trait.
 
         :param index_columns_list: List containing names of index columns.
         :type index_columns_list: list[str]
         :return: MLTable with timestamp column set to given column
         :rtype: mltable.MLTable
         """
         if index_columns_list and not isinstance(index_columns_list, list):
-            raise TypeError(
-                f'An object of type list is expected, but encountered type: {type(index_columns_list)}')
+            raise UserErrorException('An object of type list is expected, but encountered type: '
+                                     f'{type(index_columns_list)}')
         self._set_trait(_INDEX_COLUMNS_KEY, index_columns_list)
```

## mltable/_aml_utilities/_aml_rest_client_helper.py

```diff
@@ -75,19 +75,23 @@
                          f'`{STORAGE_OPTION_KEY_AZUREML_WORKSPACE}`')
 
     return workspace_info
 
 
 def _get_aml_service_base_url(location=None):
     host_env = os.environ.get(HISTORY_SERVICE_ENDPOINT_KEY)
+    use_master = os.environ.get("USE_MASTER")
 
     # default to master
     if host_env is None:
         if location is None or location == 'centraluseuap':
-            host_env = 'https://int.api.azureml-test.ms'
+            if use_master is not None and use_master == "True":
+                host_env = 'https://master.api.azureml-test.ms'
+            else:
+                host_env = 'https://int.api.azureml-test.ms'
         else:
             host_env = F'https://{location}.api.azureml.ms'
 
     return host_env
 
 
 def _get_rest_client(storage_options, auth):
```

## mltable/schema/MLTable.json

### Pretty-printed

 * *Similarity: 0.9999999984891821%*

 * *Differences: {"'properties'": "{'transformations': {'items': {'oneOf': {5: {'properties': {'read_delimited': "*

 * *                 "{'properties': {'encoding': {'enum': {insert: [(1, 'utf-8'), (2, 'utf8 bom'), "*

 * *                 "(4, 'iso-8859-1'), (6, 'latin-1'), (9, 'utf-16'), (11, 'windows-1252')], delete: "*

 * *                 "[6, 5]}}}}}}, 7: {'properties': {'read_json_lines': {'properties': {'encoding': "*

 * *                 "{'enum': {delete: [6]}}}}}}}}}}"}*

```diff
@@ -288,21 +288,25 @@
                                         "default": false,
                                         "type": "boolean"
                                     },
                                     "encoding": {
                                         "default": "utf8",
                                         "enum": [
                                             "utf8",
+                                            "utf-8",
+                                            "utf8 bom",
                                             "iso88591",
+                                            "iso-8859-1",
                                             "latin1",
+                                            "latin-1",
                                             "ascii",
                                             "utf16",
-                                            "utf32",
-                                            "utf8bom",
-                                            "windows1252"
+                                            "utf-16",
+                                            "windows1252",
+                                            "windows-1252"
                                         ],
                                         "type": "string"
                                     },
                                     "header": {
                                         "default": "all_files_same_headers",
                                         "enum": [
                                             "no_header",
@@ -478,15 +482,14 @@
                                         "enum": [
                                             "utf8",
                                             "utf-8",
                                             "iso88591",
                                             "latin1",
                                             "ascii",
                                             "utf16",
-                                            "utf32",
                                             "utf8bom",
                                             "windows1252"
                                         ],
                                         "type": "string"
                                     },
                                     "include_path_column": {
                                         "default": false,
```

## mltable/tests/test_from_delta_lake.py

```diff
@@ -1,204 +1,159 @@
 import os
-from mltable.mltable import from_delta_lake, load
-from .helper_functions import can_load_mltable
+
 import pandas as pd
 import pytest
+from azureml.dataprep import UserErrorException
+
+from mltable.mltable import from_delta_lake, load
+from .helper_functions import can_load_mltable
 
 
 @pytest.mark.mltable_sdk_unit_test
+@pytest.mark.skip(reason=("mltable-sdk-unit only picks up azureml-dataprep from public pypi that is currently broken. "
+                          "Disabling to merge the fix and unblock PRs"))
 class TestFromDeltaLake:
     def test_from_local_delta_table_version(self, get_dir_folder_path):
         cwd = get_dir_folder_path
         os.chdir(cwd)
         delta_table_path = "data/mltable/delta-table"
+
         mltable_v0 = from_delta_lake(delta_table_path, version_as_of=0)
         df_v0 = mltable_v0.to_pandas_dataframe()
-        print(df_v0.to_string())
         assert df_v0.shape == (5, 1)
         assert df_v0["id"].equals(pd.Series([0, 1, 2, 3, 4]))
-        mltable_v1 = from_delta_lake(
-            delta_table_path, version_as_of=1, include_path_column=True)
+
+        mltable_v1 = from_delta_lake(delta_table_path, version_as_of=1, include_path_column=True)
         df_v1 = mltable_v1.to_pandas_dataframe()
-        print(df_v1.to_string())
         assert df_v1.shape == (5, 2)
         assert df_v1["id"].equals(pd.Series([5, 6, 7, 8, 9]))
 
     def test_from_local_delta_table_timestamp(self, get_dir_folder_path):
         cwd = get_dir_folder_path
         os.chdir(cwd)
         delta_table_path = "data/mltable/delta-table"
-        mltable_t0 = from_delta_lake(
-            delta_table_path, timestamp_as_of="2021-01-01T00:00:00Z")
+
+        mltable_t0 = from_delta_lake(delta_table_path, timestamp_as_of="2021-01-01T00:00:00Z")
         df_t0 = mltable_t0.to_pandas_dataframe()
-        print(df_t0.to_string())
         assert df_t0.shape == (5, 1)
         assert df_t0["id"].equals(pd.Series([0, 1, 2, 3, 4]))
-        mltable_t1 = from_delta_lake(delta_table_path, timestamp_as_of="2021-08-09T01:30:00Z",
+
+        mltable_t1 = from_delta_lake(delta_table_path,
+                                     timestamp_as_of="2021-08-09T01:30:00Z",
                                      include_path_column=True)
         df_t1 = mltable_t1.to_pandas_dataframe()
-        print(df_t1.to_string())
         assert df_t1.shape == (5, 2)
         assert df_t1["id"].equals(pd.Series([5, 6, 7, 8, 9]))
 
     def test_load_local_mltable_from_delta_lake_version_1(self, get_dir_folder_path):
         wd = get_dir_folder_path
         mltable_path = os.path.join(wd, "data/mltable/delta-table/")
         df_v1 = can_load_mltable(mltable_path)
-        print(df_v1.to_string())
         assert df_v1.shape == (5, 1)
         assert df_v1["id"].equals(pd.Series([5, 6, 7, 8, 9]))
 
     def test_from_delta_lake_with_both_timestamp_and_version(self, get_dir_folder_path):
         cwd = get_dir_folder_path
         os.chdir(cwd)
-
         delta_table_path = "data/mltable/delta-table"
-        with pytest.raises(KeyError) as e:
-            from_delta_lake(
-                delta_table_path, version_as_of=1, timestamp_as_of="2021-08-09T01:30:00Z")
-            assert e.value == ("Both timestamp_as_of and version_as_of parameters were provided, but only one of "
-                               "version_as_of or timestamp_as_of can be specified.")
+        exp_err_msg = ("Both timestamp_as_of and version_as_of parameters were provided, "
+                       "but only one of version_as_of or timestamp_as_of can be specified.")
+        with pytest.raises(UserErrorException, match=exp_err_msg):
+            from_delta_lake(delta_table_path, version_as_of=1, timestamp_as_of="2021-08-09T01:30:00Z")
 
-    def test_from_delta_lake_with_non_rfc3339_format_timestamps(self, get_dir_folder_path):
-        cwd = get_dir_folder_path
-        os.chdir(cwd)
+    def test_from_delta_lake_with_non_rfc3339_format_timestamp_missing_0_in_month(self, get_dir_folder_path):
+        self.run_from_delta_lake_with_non_rfc3339_format_timestamps(get_dir_folder_path, '2021-8-09T01:30:00Z')
 
-        delta_table_path = "data/mltable/delta-table"
-        t0 = "2021-8-09T01:30:00Z"  # missing 0 in month
-        with pytest.raises(ValueError) as e:
-            from_delta_lake(delta_table_path, timestamp_as_of=t0)
-            assert e.value == ('Provided timestamp_as_of: {} is not in RFC-3339/ISO-8601 format. Please make sure '
-                               'that it adheres to RFC-3339/ISO-8601 format. For example: "2022-10-01T00:00:00Z",'
-                               '"2022-10-01T22:10:57+02:00", "2022-10-01T16:32:11.8+00:00" '
-                               'are correctly formatted.').format(t0)
-
-        t1 = "2021-08-9T01:30:00Z"  # missing 0 in date
-        with pytest.raises(ValueError) as e:
-            from_delta_lake(delta_table_path, timestamp_as_of=t1)
-            assert e.value == ('Provided timestamp_as_of: {} is not in RFC-3339/ISO-8601 format. Please make sure '
-                               'that it adheres to RFC-3339/ISO-8601 format. For example: "2022-10-01T00:00:00Z",'
-                               '"2022-10-01T22:10:57+02:00", "2022-10-01T16:32:11.8+00:00" '
-                               'are correctly formatted.').format(t1)
-
-        t2 = "2021-08-09T1:30:00Z"  # missing 0 in hour
-        with pytest.raises(ValueError) as e:
-            from_delta_lake(delta_table_path, timestamp_as_of=t2)
-            assert e.value == ('Provided timestamp_as_of: {} is not in RFC-3339/ISO-8601 format. Please make sure '
-                               'that it adheres to RFC-3339/ISO-8601 format. For example: "2022-10-01T00:00:00Z",'
-                               '"2022-10-01T22:10:57+02:00", "2022-10-01T16:32:11.8+00:00" '
-                               'are correctly formatted.').format(t2)
-
-        t3 = "2021-08-09T01:30Z"  # missing seconds
-        with pytest.raises(ValueError) as e:
-            from_delta_lake(delta_table_path, timestamp_as_of=t3)
-            assert e.value == ('Provided timestamp_as_of: {} is not in RFC-3339/ISO-8601 format. Please make sure '
-                               'that it adheres to RFC-3339/ISO-8601 format. For example: "2022-10-01T00:00:00Z",'
-                               '"2022-10-01T22:10:57+02:00", "2022-10-01T16:32:11.8+00:00" '
-                               'are correctly formatted.').format(t3)
-
-        t4 = "2021-08-09"  # missing time
-        with pytest.raises(ValueError) as e:
-            from_delta_lake(delta_table_path, timestamp_as_of=t4)
-            assert e.value == ('Provided timestamp_as_of: {} is not in RFC-3339/ISO-8601 format. Please make sure '
-                               'that it adheres to RFC-3339/ISO-8601 format. For example: "2022-10-01T00:00:00Z",'
-                               '"2022-10-01T22:10:57+02:00", "2022-10-01T16:32:11.8+00:00" '
-                               'are correctly formatted.').format(t4)
-
-        t5 = "2021-08-09T01:30:00Z-07:00"  # both zero timezone and time offset specified
-        with pytest.raises(ValueError) as e:
-            from_delta_lake(delta_table_path, timestamp_as_of=t5)
-            assert e.value == ('Provided timestamp_as_of: {} is not in RFC-3339/ISO-8601 format. Please make sure '
-                               'that it adheres to RFC-3339/ISO-8601 format. For example: "2022-10-01T00:00:00Z",'
-                               '"2022-10-01T22:10:57+02:00", "2022-10-01T16:32:11.8+00:00" '
-                               'are correctly formatted.').format(t5)
-
-        t6 = "2021-08-09T01:30:00-07:00:00"  # seconds on time offset specified
-        with pytest.raises(ValueError) as e:
-            from_delta_lake(delta_table_path, timestamp_as_of=t6)
-            assert e.value == ('Provided timestamp_as_of: {} is not in RFC-3339/ISO-8601 format. Please make sure '
-                               'that it adheres to RFC-3339/ISO-8601 format. For example: "2022-10-01T00:00:00Z",'
-                               '"2022-10-01T22:10:57+02:00", "2022-10-01T16:32:11.8+00:00" '
-                               'are correctly formatted.').format(t6)
-
-        t7 = "2021-08-09 01:30:00-07:00"  # uses a space to separate the date and time
-        with pytest.raises(ValueError) as e:
-            from_delta_lake(delta_table_path, timestamp_as_of=t7)
-            assert e.value == ('Provided timestamp_as_of: {} is not in RFC-3339/ISO-8601 format. Please make sure '
-                               'that it adheres to RFC-3339/ISO-8601 format. For example: "2022-10-01T00:00:00Z",'
-                               '"2022-10-01T22:10:57+02:00", "2022-10-01T16:32:11.8+00:00" '
-                               'are correctly formatted.').format(t7)
-
-        t8 = "20210809T013000Z"  # omits hyphens and colons
-        with pytest.raises(ValueError) as e:
-            from_delta_lake(delta_table_path, timestamp_as_of=t8)
-            assert e.value == ('Provided timestamp_as_of: {} is not in RFC-3339/ISO-8601 format. Please make sure '
-                               'that it adheres to RFC-3339/ISO-8601 format. For example: "2022-10-01T00:00:00Z",'
-                               '"2022-10-01T22:10:57+02:00", "2022-10-01T16:32:11.8+00:00" '
-                               'are correctly formatted.').format(t8)
+    def test_from_delta_lake_with_non_rfc3339_format_timestamp_missing_0_in_date(self, get_dir_folder_path):
+        self.run_from_delta_lake_with_non_rfc3339_format_timestamps(get_dir_folder_path, '2021-08-9T01:30:00Z')
+
+    def test_from_delta_lake_with_non_rfc3339_format_timestamp_missing_0_in_hour(self, get_dir_folder_path):
+        self.run_from_delta_lake_with_non_rfc3339_format_timestamps(get_dir_folder_path, '2021-08-09T1:30:00Z')
+
+    def test_from_delta_lake_with_non_rfc3339_format_timestamp_missing_seconds(self, get_dir_folder_path):
+        self.run_from_delta_lake_with_non_rfc3339_format_timestamps(get_dir_folder_path, '2021-08-09T01:30Z')
+
+    def test_from_delta_lake_with_non_rfc3339_format_timestamp_missing_time(self, get_dir_folder_path):
+        self.run_from_delta_lake_with_non_rfc3339_format_timestamps(get_dir_folder_path, '2021-08-09')
+
+    def test_from_delta_lake_with_non_rfc3339_format_timestamp_zero_timezone_and_time_offset_specified(
+            self, get_dir_folder_path):
+        self.run_from_delta_lake_with_non_rfc3339_format_timestamps(get_dir_folder_path, '2021-08-09T01:30:00Z-07:00')
+
+    def test_from_delta_lake_with_non_rfc3339_format_timestamp_seconds_on_time_offset_specified(
+            self, get_dir_folder_path):
+        self.run_from_delta_lake_with_non_rfc3339_format_timestamps(
+            get_dir_folder_path, '2021-08-09T01:30:00-07:00:00')
+
+    def test_from_delta_lake_with_non_rfc3339_format_timestamp_space_separates_date_and_time(
+            self, get_dir_folder_path):
+        self.run_from_delta_lake_with_non_rfc3339_format_timestamps(get_dir_folder_path, '2021-08-09 01:30:00-07:00')
+
+    def test_from_delta_lake_with_non_rfc3339_format_timestamp_no_hypens_and_colons(self, get_dir_folder_path):
+        self.run_from_delta_lake_with_non_rfc3339_format_timestamps(get_dir_folder_path, '20210809T013000Z')
+
+    def run_from_delta_lake_with_non_rfc3339_format_timestamps(self, cwd, timestamp):
+        os.chdir(cwd)
+        exp_err_msg = (f'Provided timestamp_as_of: {timestamp} is not in RFC-3339/ISO-8601 format\\. Please make sure '
+                       'that it adheres to RFC-3339/ISO-8601 format\\. For example: "2022-10-01T00:00:00Z", '
+                       '"2022-10-01T22:10:57\\+02:00", "2022-10-01T16:32:11\\.8\\+00:00" are correctly formatted\\.')
+        with pytest.raises(UserErrorException, match=exp_err_msg):
+            from_delta_lake("data/mltable/delta-table", timestamp_as_of=timestamp)
 
     def test_from_delta_lake_with_valid_rfc3339_format_timestamps(self, get_dir_folder_path):
         cwd = get_dir_folder_path
         os.chdir(cwd)
-
         delta_table_path = "data/mltable/delta-table"
         t0 = "2021-01-01T00:00:00+08:00"  # using + time offset
         mltable_t0 = from_delta_lake(delta_table_path, timestamp_as_of=t0)
         df_t0 = mltable_t0.to_pandas_dataframe()
-        print(df_t0.to_string())
         assert df_t0.shape == (5, 1)
         assert df_t0["id"].equals(pd.Series([0, 1, 2, 3, 4]))
 
         t1 = "2021-08-09T01:30:00-08:00"  # using - time offset
-        mltable_t1 = from_delta_lake(
-            delta_table_path, timestamp_as_of=t1, include_path_column=True)
+        mltable_t1 = from_delta_lake(delta_table_path, timestamp_as_of=t1, include_path_column=True)
         df_t1 = mltable_t1.to_pandas_dataframe()
-        print(df_t1.to_string())
         assert df_t1.shape == (5, 2)
         assert df_t1["id"].equals(pd.Series([5, 6, 7, 8, 9]))
 
         # using fractional second digits + time offset
         t2 = "2021-01-01T00:00:00.238016+08:00"
         mltable_t2 = from_delta_lake(delta_table_path, timestamp_as_of=t2)
         df_t2 = mltable_t2.to_pandas_dataframe()
-        print(df_t2.to_string())
         assert df_t2.shape == (5, 1)
         assert df_t2["id"].equals(pd.Series([0, 1, 2, 3, 4]))
 
         # using fractional second digits + zero time offset
         t3 = "2021-08-09T01:30:00.238016Z"
-        mltable_t3 = from_delta_lake(
-            delta_table_path, timestamp_as_of=t3, include_path_column=True)
+        mltable_t3 = from_delta_lake(delta_table_path, timestamp_as_of=t3, include_path_column=True)
         df_t3 = mltable_t3.to_pandas_dataframe()
-        print(df_t3.to_string())
         assert df_t3.shape == (5, 2)
         assert df_t3["id"].equals(pd.Series([5, 6, 7, 8, 9]))
 
 
 @pytest.mark.mltable_sdk_unit_test
 class TestFromDeltaLakeUserErrors:
     def test_from_local_both_version_and_timestamp(self, get_dir_folder_path):
         cwd = get_dir_folder_path
         mltable_path = os.path.join(cwd, "data/mltable/delta-table-both-version-and-timestamp/")
-        with pytest.raises(ValueError) as excinfo:
+        with pytest.raises(UserErrorException,
+                           match="Only one of version or timestamp can be specified but not both..*"):
             mltable = load(mltable_path)
             mltable.to_pandas_dataframe()
-        assert "Only one of version or timestamp can be specified but not both." in str(excinfo.value)
 
     def test_from_local_unable_to_find_metadata(self, get_dir_folder_path):
         cwd = get_dir_folder_path
         os.chdir(cwd)
         # test using a non delta table MLTable directory
         delta_table_path = "data/mltable/mltable_folder"
-        with pytest.raises(ValueError) as excinfo:
+        with pytest.raises(UserErrorException, match="Unable to find any delta table metadata.*"):
             mltable = from_delta_lake(delta_table_path, version_as_of=0)
             mltable.to_pandas_dataframe()
-        assert "Unable to find any delta table metadata" in str(excinfo.value)
 
     def test_from_local_invalid_table_version(self, get_dir_folder_path):
         cwd = get_dir_folder_path
         os.chdir(cwd)
         delta_table_path = "data/mltable/delta-table"
-        with pytest.raises(ValueError) as excinfo:
+        with pytest.raises(UserErrorException,
+                           match="Error when opening delta table: Invalid table version: 100000.*"):
             mltable = from_delta_lake(delta_table_path, version_as_of=100000)
             mltable.to_pandas_dataframe()
-        assert "Error when opening delta table: Invalid table version: 100000" in str(excinfo.value)
```

## mltable/tests/test_from_json_lines.py

```diff
@@ -1,11 +1,14 @@
 import os
+
+import pytest
+from azureml.dataprep import UserErrorException
+
 from mltable.mltable import from_json_lines_files
 from .helper_functions import mltable_was_loaded, can_load_mltable
-import pytest
 
 
 @pytest.mark.mltable_sdk_unit_test
 class TestFromJsonLines():
     def test_create_mltable_from_json_files_with_local_path(self, get_dir_folder_path):
         cwd = get_dir_folder_path
         os.chdir(cwd)
@@ -69,17 +72,17 @@
 
     def test_from_json_lines_files_error_on_invalid_rows(self, get_dir_folder_path):
         cwd = get_dir_folder_path
         exp_path_1 = os.path.normpath(
             os.path.join(cwd, 'data/order_invalid.jsonl'))
         paths = [{'file': exp_path_1}]
         mltable = from_json_lines_files(paths, invalid_lines='error')
-        with pytest.raises(Exception) as e:
+        with pytest.raises(UserErrorException) as e:
             mltable.to_pandas_dataframe()
-            print(e)
+            assert e.error_code == 'ScriptExecution.Transformation.Validation'
 
     def test_from_json_lines_files(self, get_dir_folder_path):
         wd = get_dir_folder_path
         mltable_paths = [os.path.join(
             wd, "data/mltable/jsonlines_order/")]
         for mltable_path in mltable_paths:
             df = can_load_mltable(mltable_path)
```

## mltable/tests/test_mltable_authoring_apis.py

```diff
@@ -1,18 +1,19 @@
 import os
 import tempfile
 import shutil
 
 import pytest
 import yaml
+from azureml.dataprep.native import StreamInfo
+from azureml.dataprep import UserErrorException
 
 from mltable.mltable import load, from_paths, from_delimited_files, from_parquet_files, from_json_lines_files,\
     DataType, MLTableHeaders
 from .helper_functions import get_mltable_and_dicts, mltable_as_dict, mltable_was_loaded, list_of_dicts_equal
-from azureml.dataprep.native import StreamInfo
 
 
 @pytest.mark.mltable_sdk_unit_test
 class TestMLTableAuthoringApis:
     def test_convert_column_types_with_simple_types_sdk(self, get_data_folder_path):
         path = os.path.join(get_data_folder_path, 'mltable_with_type')
         mltable = load(path)
@@ -95,74 +96,58 @@
         # data types are not automatically inferred for sake of this test
         path = os.path.join(get_data_folder_path, 'mltable_with_type')
         mltable = load(path)
         old_column_types = mltable.to_pandas_dataframe().dtypes
         assert old_column_types['Sex'].name == 'object'
 
         # Using incorrect mismatch_as string
-        with pytest.raises(ValueError) as e:
-            mltable.convert_column_types(
-                {'Sex': DataType.to_bool(false_values=['0'], mismatch_as='dummyVar')})
-        assert "`mismatch_as` can only be" in str(e.value)
+        with pytest.raises(UserErrorException, match='.*`mismatch_as` can only be.*'):
+            mltable.convert_column_types({'Sex': DataType.to_bool(false_values=['0'], mismatch_as='dummyVar')})
 
         # false_values & true_values must either both be None, empty lists, or non-empty lists
-        with pytest.raises(ValueError) as e:
-            mltable.convert_column_types(
-                {'Sex': DataType.to_bool(true_values=['1'])})
-        assert "`true_values` and `false_values` must both be None or non-empty list of strings" in str(
-            e.value)
-
-        with pytest.raises(ValueError) as e:
-            mltable.convert_column_types(
-                {'Sex': DataType.to_bool(false_values=['0'])})
-        assert "`true_values` and `false_values` must both be None or non-empty list of strings" in str(
-            e.value)
+        with pytest.raises(UserErrorException,
+                           match="`true_values` and `false_values` must both be None or non-empty list of strings"):
+            mltable.convert_column_types({'Sex': DataType.to_bool(true_values=['1'])})
+
+        with pytest.raises(UserErrorException,
+                           match="`true_values` and `false_values` must both be None or non-empty list of strings"):
+            mltable.convert_column_types({'Sex': DataType.to_bool(false_values=['0'])})
 
-        mltable_without_inputs = mltable.convert_column_types(
-            {'Sex': DataType.to_bool()})
+        mltable_without_inputs = mltable.convert_column_types({'Sex': DataType.to_bool()})
         mltable_with_full_inputs = mltable.convert_column_types(
             {'Sex': DataType.to_bool(true_values=['1'], false_values=['0'], mismatch_as='error')})
 
         mltable_without_inputs_types = mltable_without_inputs.to_pandas_dataframe().dtypes
         mltable_with_full_inputs_types = mltable_with_full_inputs.to_pandas_dataframe().dtypes
 
         assert mltable_without_inputs_types['Sex'].name == 'bool'
         assert mltable_with_full_inputs_types['Sex'].name == 'bool'
 
     def test_convert_column_types_errors_sdk(self, get_data_folder_path):
         path = os.path.join(get_data_folder_path, 'traits_timeseries')
         mltable = load(path)
-        with pytest.raises(TypeError) as e:
-            mltable.convert_column_types({'datetime': DataType.to_datetime()})
-        assert "to_datetime() missing 1 required positional argument: 'formats'" in str(e.value)
-
-        with pytest.raises(ValueError) as e:
-            mltable.convert_column_types(
-                {'datetime': DataType.to_datetime(formats=None)})
-        assert "'columns': [None] should be a string or list of strings with at least one element" in str(
-            e.value)
 
-        with pytest.raises(ValueError) as e:
+        exp_err_msg = "'columns': \\[None\\] should be a string or list of strings with at least one element"
+        with pytest.raises(UserErrorException, match=exp_err_msg):
+            mltable.convert_column_types({'datetime': DataType.to_datetime(formats=None)})
+
+        exp_err_msg = '`convert_column_types` must be a dictionary where key is Union\\[str, Tuple\\[str\\]\\] and ' \
+                      'value is :class: mltable.DataType'
+        with pytest.raises(UserErrorException, match=exp_err_msg):
             mltable.convert_column_types({'datetime': 'float'})
-        assert '`convert_column_types` must be a dictionary where ' \
-               'key is Union[str, Tuple[str]] and ' \
-               'value is :class: mltable.DataType' in str(e.value)
 
-        with pytest.raises(ValueError) as e:
+        exp_err_msg \
+            = 'Input type is dict\\[Union\\[str, Tuple\\[str\\]\\]: mltable.DataType\\] with at least one entry'
+        with pytest.raises(UserErrorException, match=exp_err_msg):
             mltable.convert_column_types({})
-        assert 'Input type is dict[Union[str, Tuple[str]]: mltable.DataType] ' \
-               'with at least one entry' in str(e.value)
 
-        with pytest.raises(ValueError) as e:
-            mltable.convert_column_types({
-                ('latitude', 'windSpeed'): DataType.to_float(),
-                ('wban', 'latitude'): DataType.to_int()
-            })
-        assert "Found duplicate column. Cannot convert column 'latitude' to multiple `mltable.DataType`s." in str(
-            e.value)
+        exp_err_msg = "Found duplicate column. Cannot convert column 'latitude' to multiple `mltable.DataType`s."
+        with pytest.raises(UserErrorException, match=exp_err_msg):
+            mltable.convert_column_types({('latitude', 'windSpeed'): DataType.to_float(),
+                                          ('wban', 'latitude'): DataType.to_int()})
 
     def test_convert_column_types_with_mltable_yaml(self, get_data_folder_path):
         string_path = 'mltable_convert_column_types/simple_types_yaml'
         path = os.path.join(get_data_folder_path, string_path)
         mltable = load(path)
         column_types = mltable.to_pandas_dataframe().dtypes
         assert column_types['datetime'].name == 'datetime64[ns]'
@@ -259,25 +244,25 @@
 
         assert '- take: 5' in new_mltable._dataflow.to_yaml_string()
         df = new_mltable.to_pandas_dataframe()
         assert df.shape[0] == 5
 
     def test_take_invalid_count(self, get_mltable):
         for invalid in -1, 0, "number":
-            with pytest.raises(ValueError):
+            with pytest.raises(UserErrorException, match='Number of rows must be a positive integer'):
                 get_mltable.take(count=invalid)
 
     def test_show(self, get_mltable):
         mltable = get_mltable
         new_mltable = mltable.show(count=5)
         assert new_mltable.shape[0] == 5
 
     def test_show_invalid_count(self, get_mltable):
         for invalid in -1, 0, "number":
-            with pytest.raises(ValueError):
+            with pytest.raises(UserErrorException, match='Number of rows must be a positive integer'):
                 get_mltable.show(count=invalid)
 
     def test_take_random_sample_no_seed(self, get_mltable):
         mltable = get_mltable
         new_mltable = mltable.take_random_sample(probability=.05, seed=None)
         new_mltable.to_pandas_dataframe()
         assert 'probability: 0.05' in new_mltable._dataflow.to_yaml_string()
@@ -286,15 +271,15 @@
         mltable = get_mltable
         new_mltable = mltable.take_random_sample(probability=.05, seed=5)
         new_mltable.to_pandas_dataframe()
         assert 'probability: 0.05' in new_mltable._dataflow.to_yaml_string()
 
     def test_take_random_sample_invalid_prob(self, get_mltable):
         for invalid in -.01, 0.0, 'number':
-            with pytest.raises(ValueError):
+            with pytest.raises(UserErrorException, match='Probability should an float greater than 0 and less than 1'):
                 get_mltable.take_random_sample(probability=invalid)
 
     def add_step_at_start(self, mltable, idx):
         mltable_info_dict = mltable_as_dict(mltable)
         added_transformations = mltable_info_dict['transformations']
 
         # only one transformation added
@@ -396,15 +381,15 @@
         post_drop_columns = new_mltable.to_pandas_dataframe().columns
         assert all(col not in post_drop_columns for col in columns_to_drop)
 
     def test_drop_columns_traits(self, get_data_folder_path):
         path = os.path.join(get_data_folder_path, 'traits_timeseries')
         mltable = load(path)
         assert "datetime" == mltable.traits.timestamp_column
-        with pytest.raises(ValueError):
+        with pytest.raises(UserErrorException, match='Columns in traits must be kept and cannot be dropped'):
             mltable.drop_columns(columns="datetime")
 
     def test_keep_columns_with_string(self, get_data_folder_path):
         path = os.path.join(get_data_folder_path, 'mltable_with_type')
         mltable = load(path)
         pre_keep_columns = mltable.to_pandas_dataframe().columns
         assert "Name" in pre_keep_columns
@@ -425,15 +410,15 @@
         assert all(col in post_keep_columns for col in columns_to_keep)
 
     def test_keep_columns_traits(self, get_data_folder_path):
         path = os.path.join(get_data_folder_path, 'traits_timeseries')
         mltable = load(path)
         assert "elevation" != mltable.traits.timestamp_column
         assert "elevation" != mltable.traits.index_columns[0]
-        with pytest.raises(ValueError):
+        with pytest.raises(UserErrorException, match='Columns in traits must be kept and cannot be dropped'):
             mltable.keep_columns(columns="elevation")
 
     def test_create_mltable_from_delimited_files_with_local_path(self, get_dir_folder_path):
         cwd = get_dir_folder_path
         os.chdir(cwd)
         paths = [{'file': 'data/crime-spring.csv'}]
         mltable = from_delimited_files(paths)
@@ -502,18 +487,16 @@
         ]
 
     def test_create_mltable_from_delimited_files_with_header_no_string(self, get_dir_folder_path):
         cwd = get_dir_folder_path
         os.chdir(cwd)
         paths = [{'file': 'data/crime-spring.csv'},
                  {'file': 'data/crime-winter.csv'}]
-        with pytest.raises(ValueError) as e:
+        with pytest.raises(UserErrorException, match='The header should be a string or an MLTableHeader enum'):
             from_delimited_files(paths, header=1)
-        assert str(
-            e.value) == 'The header should be a string or an MLTableHeader enum'
 
     def test_from_delimited_files_all_file_same_header(self, get_dir_folder_path):
         cwd = get_dir_folder_path
         os.chdir(cwd)
         paths = [{'file': 'data/crime-spring.csv'},
                  {'file': 'data/crime-winter.csv'}]
         mltable = from_delimited_files(paths, header='all_files_same_headers')
@@ -532,79 +515,64 @@
         paths = [{'file': exp_path_1}]
         mltable = from_delimited_files(paths, header='no_header')
         df = mltable.to_pandas_dataframe()
         assert df.shape == (11, 22)
 
     def test_from_delimited_with_auto_type_conversion_incorrect_type(self, get_dataset_data_folder_path):
         dirc = get_dataset_data_folder_path
-        paths = [{'file': os.path.normpath(
-            os.path.join(dirc, 'crime-spring.csv'))}]
-        with pytest.raises(ValueError) as e:
+        paths = [{'file': os.path.normpath(os.path.join(dirc, 'crime-spring.csv'))}]
+        with pytest.raises(UserErrorException, match='`infer_column_types` must be a bool or a dictionary.'):
             from_delimited_files(paths, infer_column_types=5)
-        assert str(
-            e.value) == '`infer_column_types` must be a bool or a dictionary.'
 
     def test_from_delimited_with_auto_type_conversion_extraneous_keys(self, get_dataset_data_folder_path):
         dirc = get_dataset_data_folder_path
-        paths = [{'file': os.path.normpath(
-            os.path.join(dirc, 'crime-spring.csv'))}]
-        with pytest.raises(ValueError) as e:
+        paths = [{'file': os.path.normpath(os.path.join(dirc, 'crime-spring.csv'))}]
+        exp_err_msg = 'If `infer_column_types` is a dictionary, may only contain keys ' \
+                      '`sample_size` and `column_type_overrides`.'
+        with pytest.raises(UserErrorException, match=exp_err_msg):
             from_delimited_files(paths, infer_column_types={'some-key': 5})
-        assert str(e.value) == 'If `infer_column_types` is a dictionary, ' \
-                               'may only contain keys `sample_size` and `column_type_overrides`.'
 
     def test_from_delimited_with_auto_type_conversion_incorrect_sample_size(self, get_dataset_data_folder_path):
         for sample_size in '5', -2, 0:
             dirc = get_dataset_data_folder_path
-            paths = [{'file': os.path.normpath(
-                os.path.join(dirc, 'crime-spring.csv'))}]
-            with pytest.raises(ValueError) as e:
-                from_delimited_files(paths, infer_column_types={
-                    'sample_size': sample_size})
-            assert str(e.value) == 'If `infer_column_types` is a dictionary with a `sample_size` key, ' \
-                                   'its value must be a positive integer.'
+            paths = [{'file': os.path.normpath(os.path.join(dirc, 'crime-spring.csv'))}]
+            exp_err_msg = 'If `infer_column_types` is a dictionary with a `sample_size` key, its value must be a ' \
+                          'positive integer.'
+            with pytest.raises(UserErrorException, match=exp_err_msg):
+                from_delimited_files(paths, infer_column_types={'sample_size': sample_size})
 
     def test_from_delimited_with_auto_type_conversion_column_type_overrides_unsupported_value_type(
             self, get_dataset_data_folder_path):
 
         for sample_size in '5', -2, 0:
             dirc = get_dataset_data_folder_path
-            paths = [{'file': os.path.normpath(
-                os.path.join(dirc, 'crime-spring.csv'))}]
-            with pytest.raises(ValueError) as e:
-                from_delimited_files(paths, infer_column_types={
-                    'column_type_overrides': {'foo': sample_size}})
-            assert str(
-                e.value) == "'{}' is not a supported string conversion for `mltable.DataType`, " \
-                            "supported types are 'string', 'int', 'float', 'boolean', & 'stream_info'".format(
-                sample_size)
+            paths = [{'file': os.path.normpath(os.path.join(dirc, 'crime-spring.csv'))}]
+            exp_err_msg = f"'{sample_size}' is not a supported string conversion for `mltable.DataType`, " \
+                          "supported types are 'string', 'int', 'float', 'boolean', & 'stream_info'"
+            with pytest.raises(UserErrorException, match=exp_err_msg):
+                from_delimited_files(paths, infer_column_types={'column_type_overrides': {'foo': sample_size}})
 
     def test_from_delimited_with_auto_type_conversion_column_type_overrides_unsupported_string(
             self, get_dataset_data_folder_path):
-
         dirc = get_dataset_data_folder_path
-        paths = [{'file': os.path.normpath(
-            os.path.join(dirc, 'crime-spring.csv'))}]
-        with pytest.raises(ValueError) as e:
-            from_delimited_files(paths, infer_column_types={
-                'column_type_overrides': {'foo': 'datetime'}})
-        assert str(e.value) == "'datetime' is not a supported string conversion for `mltable.DataType`, " \
-                               "supported types are 'string', 'int', 'float', 'boolean', & 'stream_info'"
+        paths = [{'file': os.path.normpath(os.path.join(dirc, 'crime-spring.csv'))}]
+        exp_err_msg = "'datetime' is not a supported string conversion for `mltable.DataType`, " \
+                      "supported types are 'string', 'int', 'float', 'boolean', & 'stream_info'"
+        with pytest.raises(UserErrorException, match=exp_err_msg):
+            from_delimited_files(paths, infer_column_types={'column_type_overrides': {'foo': 'datetime'}})
 
     def test_from_delimited_with_auto_type_conversion_incorrect_column_type_overrides_type(
             self, get_dataset_data_folder_path):
 
         dirc = get_dataset_data_folder_path
-        paths = [{'file': os.path.normpath(
-            os.path.join(dirc, 'crime-spring.csv'))}]
-        with pytest.raises(ValueError) as e:
-            from_delimited_files(paths, infer_column_types={
-                'column_type_overrides': '5'})
-        assert str(e.value) == 'If `infer_column_types` is a dictionary with a `column_type_overrides` key, ' \
-                               'its value must be a dictionary of strings to `mltable.DataType`s or strings.'
+        paths = [{'file': os.path.normpath(os.path.join(dirc, 'crime-spring.csv'))}]
+        exp_err_msg = 'If `infer_column_types` is a dictionary with a `column_type_overrides` key, ' \
+                      'its value must be a dictionary of strings to `mltable.DataType`s or strings.'
+        with pytest.raises(UserErrorException, match=exp_err_msg):
+            from_delimited_files(paths, infer_column_types={'column_type_overrides': '5'})
 
     def test_from_delimited_with_auto_type_conversion_true(self, get_dataset_data_folder_path):
         dirc = get_dataset_data_folder_path
         paths = [{'file': os.path.normpath(
             os.path.join(dirc, 'crime-spring.csv'))}]
         mltable = from_delimited_files(paths, infer_column_types=True)
         df = mltable.to_pandas_dataframe()
@@ -686,21 +654,19 @@
                                                                   }})
         df = mltable.to_pandas_dataframe()
         assert str(df['ID'].dtype) == 'object'
         assert str(df['Date'].dtype) == 'object'
 
     def test_from_delimited_with_auto_type_conversion_incorrect_keys(self, get_dataset_data_folder_path):
         dirc = get_dataset_data_folder_path
-        paths = [{'file': os.path.normpath(
-            os.path.join(dirc, 'crime-spring.csv'))}]
-        with pytest.raises(ValueError) as e:
-            from_delimited_files(paths, infer_column_types={
-                'sample_size': 200, 'ID': 'int'})
-        assert str(e.value) == \
+        paths = [{'file': os.path.normpath(os.path.join(dirc, 'crime-spring.csv'))}]
+        exp_err_msg = \
             'If `infer_column_types` is a dictionary, may only contain keys `sample_size` and `column_type_overrides`.'
+        with pytest.raises(UserErrorException, match=exp_err_msg):
+            from_delimited_files(paths, infer_column_types={'sample_size': 200, 'ID': 'int'})
 
     def test_from_delimited_with_auto_type_conversion_dup_col_overrides_both_single_gives_last_type(
             self, get_dataset_data_folder_path):
 
         dirc = get_dataset_data_folder_path
         paths = [{'file': os.path.normpath(
             os.path.join(dirc, 'crime-spring.csv'))}]
@@ -709,25 +675,22 @@
             'ID': DataType.to_int()  # noqa: F601
         }})
         df = mltable.to_pandas_dataframe()
         assert str(df['ID'].dtype) == 'int64'
 
     def test_from_delimited_with_auto_type_conversion_dup_col_overrides_multi_and_single(
             self, get_dataset_data_folder_path):
-
         dirc = get_dataset_data_folder_path
-        paths = [{'file': os.path.normpath(
-            os.path.join(dirc, 'crime-spring.csv'))}]
-        with pytest.raises(ValueError) as e:
+        paths = [{'file': os.path.normpath(os.path.join(dirc, 'crime-spring.csv'))}]
+        exp_err_msg = "Found duplicate column. Cannot convert column 'ID' to multiple `mltable.DataType`s."
+        with pytest.raises(UserErrorException, match=exp_err_msg):
             from_delimited_files(paths, infer_column_types={'column_type_overrides': {
                 'ID': DataType.to_float(),
                 ('ID', 'Latitude'): DataType.to_float()
             }})
-        assert str(
-            e.value) == "Found duplicate column. Cannot convert column 'ID' to multiple `mltable.DataType`s."
 
     def test_from_delimited_with_auto_type_conversion_true_yaml(self, get_data_folder_path):
         dirc = get_data_folder_path
         mltable_yaml_path = os.path.join(
             dirc, 'mltable_from_delimited/with_true')
         mltable = load(mltable_yaml_path)
         df = mltable.to_pandas_dataframe()
@@ -778,20 +741,17 @@
             dirc, 'mltable_from_delimited/with_overrides')
         mltable = load(mltable_yaml_path)
         df = mltable.to_pandas_dataframe()
         assert str(df['ID'].dtype) == 'float64'
 
     def test_from_delimited_with_auto_type_conversion_malformed_overrides_yaml(self, get_data_folder_path):
         dirc = get_data_folder_path
-        mltable_yaml_path = os.path.join(
-            dirc, 'mltable_from_delimited/malformed_overrides')
-        with pytest.raises(ValueError) as e:
+        mltable_yaml_path = os.path.join(dirc, 'mltable_from_delimited/malformed_overrides')
+        with pytest.raises(UserErrorException, match='Given MLTable does not adhere to the AzureML MLTable schema.*'):
             load(mltable_yaml_path)
-        assert str(e.value).startswith(
-            'Given MLTable does not adhere to the AzureML MLTable schema')
 
     """
     this test case is tno working now. Need to clarify.
     def test_from_delimited_files_different_header(self, get_dir_folder_path):
         cwd = get_dir_folder_path
         os.chdir(cwd)
         paths = [{'file': 'data/mltable/mltable_file_different_header/crime-spring.csv'},
@@ -818,18 +778,19 @@
             'ID', 'Case Number', 'Date', 'Block', 'IUCR', 'Primary Type', 'Description', 'Location Description',
             'Arrest', 'Domestic', 'Beat', 'District', 'Ward', 'Community Area', 'FBI Code', 'X Coordinate',
             'Y Coordinate', 'Year', 'Updated On', 'Latitude', 'Longitude', 'Location'
         ]
 
     def test_from_delimited_files_unknown_header_option(self, get_dir_folder_path):
         cwd = get_dir_folder_path
-        exp_path_1 = os.path.normpath(
-            os.path.join(cwd, 'data/crime-spring.csv'))
+        exp_path_1 = os.path.normpath(os.path.join(cwd, 'data/crime-spring.csv'))
         paths = [{'file': exp_path_1}]
-        with pytest.raises(KeyError):
+        exp_err_msg = "Given invalid header some_unknown_option, supported headers are: 'no_header', " \
+                      "'from_first_file', 'all_files_different_headers', and 'all_files_same_headers'."
+        with pytest.raises(UserErrorException, match=exp_err_msg):
             from_delimited_files(paths, header='some_unknown_option')
 
     def test_from_delimited_files_with_encoding(self, get_dir_folder_path):
         cwd = get_dir_folder_path
         exp_path_1 = os.path.normpath(
             os.path.join(cwd, 'data/latin1encoding.csv'))
         paths = [{'file': exp_path_1}]
@@ -1042,17 +1003,18 @@
         mltable = mltable._with_partition_size(mini_batch_size)
         new_partition_count = mltable.get_partition_count()
         assert new_partition_count == 11
 
     def test_update_partition_size_with_parquet(self, get_dir_folder_path):
         cwd = get_dir_folder_path
         os.chdir(cwd)
-        mltable = from_parquet_files([{'file': 'data/crime.parquet'}])
-        exp_error_msg = '`read_delimited` or `read_json_lines` transformation step required to update partition_size.'
-        with pytest.raises(ValueError, match=exp_error_msg):
+        paths = [{'file': 'data/crime.parquet'}]
+        mltable = from_parquet_files(paths)
+        exp_err_msg = 'transformation step read_delimited or read_json_lines is required to update partition_size'
+        with pytest.raises(UserErrorException, match=exp_err_msg):
             mltable._with_partition_size(min_batch_size=200)
 
     def test_mltable_from_delimited_files_is_tabular(self, get_dir_folder_path):
         cwd = get_dir_folder_path
         exp_path_1 = os.path.normpath(
             os.path.join(cwd, 'data/crime-spring.csv'))
         paths = [{'file': exp_path_1}]
```

## mltable/tests/test_mltable_inner_functions.py

```diff
@@ -1,14 +1,17 @@
 import copy
-from mltable.mltable import MLTable, _normalize_column_inputs, _check_no_duplicate_columns, load, DataType
-from mltable._utils import _validate, _path_is_current_directory_variant, _make_all_paths_absolute
-import pytest
 import os
+
+from azureml.dataprep import UserErrorException
+import pytest
 import yaml
 
+from mltable.mltable import MLTable, _normalize_column_inputs, _check_no_duplicate_columns, load, DataType
+from mltable._utils import _validate, _path_is_current_directory_variant, _make_all_paths_absolute
+
 
 @pytest.fixture(scope='module', autouse=True)
 def get_sample_mltable(get_data_folder_path):
     path = os.path.join(get_data_folder_path, 'mltable_with_type')
     return load(path)
 
 
@@ -19,15 +22,15 @@
     """
     def test_update_workspace_info(self):
         mlt_strs = ["""
         paths:
           - file: ./train_annotations.jsonl
         transformations:
           - read_json_lines:
-                encoding: utf8
+                encoding: utf-8
                 include_path_column: false
           - convert_column_types:
               - columns: image_url
                 column_type: stream_info
         """, """
         paths:
           - file: ./train_annotations.jsonl
@@ -92,17 +95,16 @@
             'workspace': 'test_ws'
         }
 
         stream_column = 'image_url'
         for mlt_str, exp_mlt_str in zip(mlt_strs, expected_mlt_strs):
             mlt_dict = yaml.safe_load(mlt_str)
             mlt = MLTable._create_from_dict(mlt_dict, None)
-            new_mlt = MLTable._append_workspace_to_stream_info_conversion(
-                mlt, ws_info, stream_column)
-            new_mlt_str = new_mlt._dataflow.to_yaml_string()
+            new_mlt = MLTable._append_workspace_to_stream_info_conversion(mlt, ws_info, stream_column)
+            new_mlt_str = str(new_mlt)
             assert yaml.safe_load(new_mlt_str) == yaml.safe_load(exp_mlt_str)
 
     def test_normalize_column_inputs(self):
         single_col, single_col_tuple = 'colA', ('colA',)
         list_of_cols, list_of_cols_error = ['colA', 'colB'], ['colA', 0.1]
         tuple_of_cols, tuple_of_cols_error = ('colA', 'colB'), ('colA', 0.1)
 
@@ -114,59 +116,52 @@
 
         list_of_cols = _normalize_column_inputs(list_of_cols)
         assert type(list_of_cols) == list and list_of_cols == ['colA', 'colB']
 
         tuple_of_cols = _normalize_column_inputs(tuple_of_cols, is_tuple=True)
         assert type(tuple_of_cols) == tuple and tuple_of_cols == ('colA', 'colB')
 
-        with pytest.raises(ValueError) as e:
+        exp_err_msg = "'columns': \\['colA', 0.1\\] should be a string or list of strings with at least one element"
+        with pytest.raises(UserErrorException, match=exp_err_msg):
             _normalize_column_inputs(list_of_cols_error)
-        assert "'columns': ['colA', 0.1] should be " \
-               "a string or list of strings with at least one element" in str(e.value)
 
-        with pytest.raises(ValueError) as e:
+        exp_err_msg = "'columns': \\('colA', 0.1\\) should be a string or tuple of strings with at least one element"
+        with pytest.raises(UserErrorException, match=exp_err_msg):
             _normalize_column_inputs(tuple_of_cols_error, is_tuple=True)
-        assert "'columns': ('colA', 0.1) should be " \
-               "a string or tuple of strings with at least one element" in str(e.value)
 
-    def test_check_no_duplicate_columns(self):
-        list_of_cols_error = ['colA', 'colA']
-        tuple_of_cols_error = ('colA', 'colA')
-        list_of_tuples = [('colA', 'colB'), ('colC', 'colA')]
-
-        error_str = "Found duplicate column. Cannot convert column 'colA' to multiple `mltable.DataType`s."
-
-        with pytest.raises(ValueError) as e:
-            _check_no_duplicate_columns(list_of_cols_error)
-        assert error_str in str(e.value)
-
-        with pytest.raises(ValueError) as e:
-            _check_no_duplicate_columns(tuple_of_cols_error)
-        assert error_str in str(e.value)
-
-        with pytest.raises(ValueError) as e:
-            _check_no_duplicate_columns(list_of_tuples)
-        assert error_str in str(e.value)
+    def run_no_dup_cols(self, cols):
+        exp_err_msg = "Found duplicate column. Cannot convert column 'colA' to multiple `mltable.DataType`s."
+        with pytest.raises(UserErrorException, match=exp_err_msg):
+            _check_no_duplicate_columns(cols)
+
+    def test_no_dup_cols_tuple(self):
+        self.run_no_dup_cols(('colA', 'colA'))
+
+    def test_no_dup_columns_list(self):
+        self.run_no_dup_cols(['colA', 'colA'])
+
+    def test_no_dup_cols_list_of_tuples(self):
+        self.run_no_dup_cols([('colA', 'colB'), ('colC', 'colA')])
 
     def test_get_columns_in_traits(self, get_data_folder_path):
         path = os.path.join(get_data_folder_path, 'traits_timeseries')
         mltable = load(path)
         assert mltable._get_columns_in_traits() == {'datetime'}
 
     def test_normalize_partition_int_list(self, get_sample_mltable):
         single_int = 10
         int_list = [10, 11, 1]
         error_int_list = [10, 11, '11']
 
         assert get_sample_mltable._normalize_partition_int_list(single_int) == [10]
         assert get_sample_mltable._normalize_partition_int_list(int_list) == int_list
 
-        with pytest.raises(ValueError) as e:
+        exp_err_msg = "Columns should be a int or list of int with at least one element"
+        with pytest.raises(UserErrorException, match=exp_err_msg):
             get_sample_mltable._normalize_partition_int_list(error_int_list)
-        assert "Columns should be a int or list of int with at least one element" in str(e.value)
 
     def test_sanitize_and_check_datatype(self, get_sample_mltable):
         cols_no_list = {
             'PassengerId': DataType.to_int(),
             'Fare': DataType.to_float(),
             'datetime': DataType.to_datetime('%Y-%m-%d %H:%M:%S'),
             "booleanType": DataType.to_bool(),
@@ -181,21 +176,22 @@
             ('float1', 'float2'): DataType.to_float(),
             ('bool1', 'bool2'): DataType.to_bool(),
             ('str1', 'str2'): DataType.to_string(),
             ('stream1', 'stream2'): DataType.to_stream()
         }
         get_sample_mltable._sanitize_and_check_datatype(data_types_tuples)
 
-        with pytest.raises(ValueError) as e:
+        exp_err_type \
+            = 'Input type is dict\\[Union\\[str, Tuple\\[str\\]\\]: mltable.DataType\\] with at least one entry'
+        with pytest.raises(UserErrorException, match=exp_err_type):
             get_sample_mltable._sanitize_and_check_datatype({})
-        assert 'Input type is dict[Union[str, Tuple[str]]: mltable.DataType] with at least one entry' in str(e.value)
 
-        with pytest.raises(ValueError) as e:
+        exp_err_type = "'columns': 1 should be a string or tuple of strings with at least one element"
+        with pytest.raises(UserErrorException, match=exp_err_type):
             get_sample_mltable._sanitize_and_check_datatype({1: DataType.to_int()})
-        assert "'columns': 1 should be a string or tuple of strings with at least one element" in str(e.value)
 
     def test_validate(self):
         error_key_mltable_yaml_dict = {
             'type': 'mltable',
             'path': [{'file': 'https://dprepdata.blob.core.windows.net/demo/Titanic2.csv'}],
             'transformations': [{'take': 1}]
         }
@@ -207,22 +203,22 @@
                 'read_delimited': {
                     'delimiter': ',', 'encoding': 'ascii', 'empty_as_string': False,
                     'header': 'dummy_value', 'infer_column_types': False
                 }
             }]
         }
 
-        with pytest.raises(ValueError) as e:
+        exp_err_msg = "Additional properties are not allowed \\('path' was unexpected\\)"
+        with pytest.raises(UserErrorException, match=exp_err_msg):
             _validate(error_key_mltable_yaml_dict)
-        assert "Additional properties are not allowed ('path' was unexpected)" in str(e.value)
 
-        with pytest.raises(ValueError) as e:
+        exp_err_msg = "'dummy_value' is not one of " \
+                      "\\['no_header', 'from_first_file', 'all_files_different_headers', 'all_files_same_headers'\\]"
+        with pytest.raises(UserErrorException, match=exp_err_msg):
             _validate(error_value_mltable_yaml_dict)
-        assert "'dummy_value' is not one of ['no_header', 'from_first_file', " \
-               "'all_files_different_headers', 'all_files_same_headers']" in str(e.value)
 
     def test_path_is_current_directory_variant(self):
         base_dir_linux, base_dir_windows, base_dir_dot = './', '.\\', '.'
         non_base_dir = './Titanic2.csv'
         assert _path_is_current_directory_variant(base_dir_linux)
         assert _path_is_current_directory_variant(base_dir_windows)
         assert _path_is_current_directory_variant(base_dir_dot)
```

## mltable/tests/test_mltable_load.py

```diff
@@ -1,25 +1,25 @@
 import os
 
+from azureml.dataprep import UserErrorException
 import pytest
-from azureml.dataprep.api.mltable._mltable_helper import _read_yaml, UserErrorException
 
-from mltable.mltable import load
+from mltable import load
 from .helper_functions import can_load_mltable, get_invalid_mltable, get_mltable_and_dicts, list_of_dicts_equal, \
-    mltable_as_dict
+    mltable_as_dict, _read_yaml
 
 
 @pytest.mark.mltable_sdk_unit_test
 class TestMLTableLoad:
     def test_load_mltable(self, get_mltable):
         mltable = get_mltable
         assert mltable is not None
 
     def test_load_invalid_mltable(self, get_invalid_data_folder_path):
-        with pytest.raises(ValueError):
+        with pytest.raises(UserErrorException, match='Given MLTable does not adhere to the AzureML MLTable schema:.*'):
             get_invalid_mltable(get_invalid_data_folder_path)
 
     def test_load_mltable_with_type_prop(self, get_data_folder_path):
         data_folder_path = os.path.join(get_data_folder_path, 'mltable_with_type')
         can_load_mltable(uri=data_folder_path)
 
     def test_load_mltable_with_mixed_casing(self, get_data_folder_path):
@@ -69,34 +69,32 @@
 
     def test_load_mltable_with_arbitrary_metadata(self, get_data_folder_path):
         mltable_dirc = get_data_folder_path
         mltable_path = os.path.join(mltable_dirc, 'mltable_arb_metadata')
         can_load_mltable(mltable_path)
 
     def test_load_mltable_with_invalid_url(self):
-        with pytest.raises(UserErrorException) as excinfo:
-            mltable_url = 'https://raw.githubusercontent.com/microsoft/arcticseals/master/data/test.csv'
-            load(mltable_url)
-        assert "Not able to find MLTable file" in str(excinfo.value)
+        # TODO redo this _download_yaml exceptions
+        with pytest.raises(UserErrorException, match='.*Not able to find MLTable file'):
+            load('https://raw.githubusercontent.com/microsoft/arcticseals/master/data/test.csv')
 
     def test_load_mltable_pattern_invalid_file_path(self, get_data_folder_path):
         data_folder_path = os.path.join(get_data_folder_path, 'mltable_pattern')
-        with pytest.raises(ValueError) as excinfo:
+        with pytest.raises(UserErrorException) as e:
             can_load_mltable(uri=data_folder_path)
-        assert "Invalid argument \"file-path\"" in str(excinfo.value)
+            e.error_code == 'ScriptExecution.Validation'
 
 
 @pytest.mark.mltable_sdk_unit_test_windows
 class TestMLTableLoadWindowsOnly:
     def test_load(self, get_data_folder_path):
         mltable_dirc_path = get_data_folder_path
         mltable_path = os.path.join(mltable_dirc_path, 'mltable_windows')
         og_mltable, og_mltable_yaml_dict, og_mltable_yaml_file_dict = get_mltable_and_dicts(mltable_path)
 
-        # paths before loading
         # paths before loading are preserved in paths attribute
         list_of_dicts_equal([{'file': 'D:\\absolute\\path\\file.csv'}, {'file': 'relative\\path\\file.csv'}],
                             og_mltable_yaml_file_dict['paths'],
                             og_mltable.paths)
 
         # paths in MLTable Dataflow after loading
         list_of_dicts_equal([{'file': 'file://D:\\absolute\\path\\file.csv'},
```

## mltable/tests/test_mltable_load_to_pandas.py

```diff
@@ -1,13 +1,16 @@
 import os
-from mltable.mltable import load, from_delimited_files, from_parquet_files, _load
-from .helper_functions import mltable_was_loaded, can_load_mltable
+
+from azureml.dataprep import UserErrorException
 import pandas as pd
 import pytest
 
+from mltable.mltable import load, from_delimited_files, from_parquet_files, _load
+from .helper_functions import mltable_was_loaded, can_load_mltable
+
 
 @pytest.mark.mltable_sdk_unit_test
 class TestMLTable_load_to_pandas:
     def test_to_pandas_dataframe_from_mltable(self, get_mltable):
         mltable = get_mltable
         pdf = mltable_was_loaded(mltable)
         assert pdf.shape == (40, 12)
@@ -48,25 +51,21 @@
                            {'file_name': 'dog1', 'file_type': 'dogs'},
                            {'file_name': 'dog2', 'file_type': 'dogs'}]
         assert sorted(mltable._get_partition_key_values(), key=lambda ele: sorted(ele.items())) == \
             expected_result
 
     def test_get_partition_key_values_with_invalid_key(self, get_dir_folder_path):
         cwd = get_dir_folder_path
-        test_mltable_dir = os.path.join(
-            cwd, 'data/mltable/partition_format')
+        test_mltable_dir = os.path.join(cwd, 'data/mltable/partition_format')
         mltable = load(test_mltable_dir)
         partition_format = '{file_type}/{file_name}.txt'
-        mltable = mltable.extract_columns_from_partition_format(
-            partition_format)
-        with pytest.raises(Exception) as e:
+        mltable = mltable.extract_columns_from_partition_format(partition_format)
+        with pytest.raises(UserErrorException, match="\\['Invalid_key'\\] are invalid partition keys"):
             mltable._get_partition_key_values(['Invalid_key'])
-        assert str(e.value) == "['Invalid_key'] are invalid partition keys"
 
-    # this test could be added back after distinct step is added.
     def test_get_partition_key_values_with_key_name(self, get_dir_folder_path):
         cwd = get_dir_folder_path
         test_mltable_dir = os.path.join(
             cwd, 'data/mltable/partition_format')
         mltable = load(test_mltable_dir)
         partition_format = '{file_type}/{file_name}.txt'
         mltable = mltable.extract_columns_from_partition_format(
@@ -139,17 +138,17 @@
         cwd = get_dir_folder_path
         exp_path_1 = os.path.normpath(
             os.path.join(cwd, 'data/crime-spring.csv'))
         paths = [{'file': exp_path_1}]
         mltable = from_delimited_files(paths, infer_column_types=False)
         expression = 'FBI Code == \"11\"'
         filtered_mltable = mltable.filter(expression)
-        with pytest.raises(Exception) as e:
-            mltable_was_loaded(filtered_mltable)
-        assert 'Not a valid python expression in filter' in str(e.value)
+        with pytest.raises(UserErrorException) as e:
+            filtered_mltable.to_pandas_dataframe()
+            assert e.error_code == 'ScriptExecution.Validation'
 
     def test_extract_partition_format_into_columns_using_mltable_file(self, get_dir_folder_path):
         cwd = get_dir_folder_path
         test_mltable_dir = os.path.join(
             cwd, 'data/mltable/partition_format_specified')
         mltable = load(test_mltable_dir)
         df = mltable.to_pandas_dataframe()
```

## mltable/tests/test_mltable_save.py

```diff
@@ -1,15 +1,16 @@
-import os
 import tempfile
+import os
 
+from azureml.dataprep import UserErrorException
 import pytest
 from azureml.dataprep.api.mltable._mltable_helper import _read_yaml
 
-from .helper_functions import mltable_as_dict, save_mltable_yaml_dict, list_of_dicts_equal
 from mltable.mltable import load
+from .helper_functions import mltable_as_dict, save_mltable_yaml_dict, list_of_dicts_equal
 
 
 @pytest.mark.mltable_sdk_unit_test
 class TestMLTableSave:
     def test_save(self, get_mltable_data_folder_path):
         mltable_path = get_mltable_data_folder_path
         og_mltable = load(mltable_path)
@@ -74,29 +75,30 @@
     def test_save_to_existing_dirc_with_mltable_overwrite_false(self, get_mltable_data_folder_path):
         # try to save to directory that has a MLTable file
         # in this case the same directory the MLTable was originally loaded
         mltable_path = get_mltable_data_folder_path
         existing_mltable_save_path = os.path.join(mltable_path, 'MLTable')
         assert os.path.exists(existing_mltable_save_path)
         mltable = load(mltable_path)
-        with pytest.raises(ValueError):
+        with pytest.raises(UserErrorException,
+                           match='The given directory path already contains an MLTable YAML file\\.'):
             mltable.save(mltable_path, overwrite=False)
 
     def test_save_to_file_path(self, get_mltable):
         # try to save to *existing* file path
         mltable = get_mltable
         with tempfile.TemporaryDirectory() as save_dirc:
             save_path = os.path.join(save_dirc, 'foo.yml')
             assert not os.path.exists(save_path)
             with open(save_path, 'w') as f:
                 f.write('foo')
             assert os.path.isfile(save_path)
 
-            with pytest.raises(ValueError):
-                mltable.save(save_path)  # try to save to file
+            with pytest.raises(UserErrorException, match='The given path points to a file\\.'):
+                mltable.save(save_path)
 
 
 @pytest.mark.mltable_sdk_unit_test_windows
 class TestMLTableSaveWindowsOnly:
     def test_save_diff_mount(self, get_data_folder_path):
         mltable_dirc_path = get_data_folder_path
         mltable_path = os.path.join(mltable_dirc_path, 'mltable_windows')
```

## mltable/tests/test_partition_api.py

```diff
@@ -1,11 +1,13 @@
 import os
+
+import pytest
+
 from mltable.mltable import from_delimited_files, from_json_lines_files
 from .helper_functions import mltable_was_loaded
-import pytest
 
 
 @pytest.mark.mltable_sdk_unit_test
 class TestPartitionApi:
     def test_get_partition_count_json(self, get_dir_folder_path):
         cwd = get_dir_folder_path
         exp_path_1 = os.path.normpath(
```

## mltable/tests/test_validation_and_error_handler.py

```diff
@@ -1,88 +1,80 @@
-from mltable._validation_and_error_handler import _classify_known_user_error, _download_error_handler, \
-    _get_and_validate_download_list
-from mltable.mltable import _get_logger
-from azureml.dataprep.native import DataPrepError, StreamInfo
 import pytest
+from azureml.dataprep.native import DataPrepError, StreamInfo
+from azureml.dataprep import UserErrorException
+
+from mltable._validation_and_error_handler import _reclassify_external_error, _download_error_handler, \
+    _validate_downloads
+from mltable.mltable import _get_logger
 
 
 @pytest.mark.mltable_sdk_unit_test
 class TestValidationErrorHandler:
     def test_classify_known_user_error(self):
-        exception_str_value_error = ["InvalidUriScheme", "StreamError(NotFound)", "DataAccessError(NotFound)",
-                                     "No such host is known", "No identity was found on compute"]
+        exception_str_value_error = ["InvalidUriScheme",
+                                     "StreamError(NotFound)",
+                                     "DataAccessError(NotFound)",
+                                     "No such host is known",
+                                     "No identity was found on compute"]
 
         exception_str_system_error = ["Microsoft.DPrep.ErrorValues.PythonNumpyDatetimeParseFailure",
                                       "Microsoft.DPrep.ErrorValues.IntegerOverflow",
                                       "Microsoft.DPrep.ErrorValues.UnsupportedPythonObject"]
 
-        with pytest.raises(ValueError) as e:
-            err_string = 'ExecutionError(StreamError(PermissionDenied(Some(Server failed to authenticate the request'
-            _classify_known_user_error(err_string)
-        assert err_string in str(e.value)
+        with pytest.raises(UserErrorException):
+            ex = Exception(
+                'ExecutionError(StreamError(PermissionDenied(Some(Server failed to authenticate the request')
+            _reclassify_external_error(ex)
 
         for val_error in exception_str_value_error:
-            with pytest.raises(ValueError) as e:
-                _classify_known_user_error(val_error)
-            assert val_error in str(e.value)
+            match = val_error.replace('(', '\\(',).replace(')', '\\)')
+            with pytest.raises(UserErrorException, match=match):
+                _reclassify_external_error(Exception(val_error))
 
         for sys_error in exception_str_system_error:
-            with pytest.raises(SystemError) as e:
-                _classify_known_user_error(sys_error)
-            assert sys_error in str(e.value)
+            with pytest.raises(RuntimeError, match=sys_error):
+                _reclassify_external_error(Exception(sys_error))
 
     def test_download_error_handler(self):
         value_error_list = [('dummyFile1.csv', "Microsoft.DPrep.ErrorValues.SourceFileNotFound"),
                             ('dummyFile2.csv', "Microsoft.DPrep.ErrorValues.SourceFilePermissionDenied"),
                             ('dummyFile3.csv', "Microsoft.DPrep.ErrorValues.InvalidArgument"),
                             ('dummyFile4.csv', "Microsoft.DPrep.ErrorValues.SourcePermissionDenied"),
                             ('dummyFile5.csv', "Microsoft.DPrep.ErrorValues.DestinationPermissionDenied"),
                             ('dummyFile6.csv', "Microsoft.DPrep.ErrorValues.DestinationDiskFull"),
                             ('dummyFile7.csv', "Microsoft.DPrep.ErrorValues.FileSizeChangedWhileDownloading"),
                             ('dummyFile8.csv', "Microsoft.DPrep.ErrorValues.StreamInfoInvalidPath"),
                             ('dummyFile9.csv', "Microsoft.DPrep.ErrorValues.NoManagedIdentity"),
                             ('dummyFile10.csv', "Microsoft.DPrep.ErrorValues.NoOboEndpoint"),
                             ('dummyFile11.csv', "Microsoft.DPrep.ErrorValues.StreamInfoRequired")]
 
-        value_error_msg = 'Some files have failed to download:' + '\n'.join(
-            [str((file_name, error_code)) for (file_name, error_code) in value_error_list])
-
-        with pytest.raises(ValueError) as e:
+        with pytest.raises(UserErrorException, match='Some files have failed to download: .*'):
             _download_error_handler(value_error_list)
-        assert value_error_msg in str(e.value)
 
-        with pytest.raises(RuntimeError) as e:
+        with pytest.raises(RuntimeError, match='System errors occured during downloading: .*'):
             _download_error_handler([('dummyFile.csv', 'Microsoft.DPrep.ErrorValues.IntegerOverflow')])
-        assert "System error happens during downloading: Microsoft.DPrep.ErrorValues.IntegerOverflow" in str(e.value)
 
-    def test_get_and_validate_download_list(self, get_data_folder_path):
+    def test_validate_downloads(self):
         # Empty download_records
-        empty_download_records = _get_and_validate_download_list([], False, _get_logger())
-        assert empty_download_records == []
+        assert _validate_downloads([], False, _get_logger()) == []
 
+        # Does not raise exception since the error is related to no file found in source thus returns empty list
         download_records = [{'DestinationFile': DataPrepError("Microsoft.DPrep.ErrorValues.SourceFileNotFound",
                                                               originalValue="Path",
                                                               properties="")}]
-
-        # Does not raise exception since the error is related to no file found in source thus returns empty list
-        empty_download_list_not_present = _get_and_validate_download_list(download_records, True, _get_logger())
-        assert empty_download_list_not_present == []
+        assert _validate_downloads(download_records, True, _get_logger()) == []
 
         # Error found in download_records
-        download_records = [
-            {'DestinationFile': DataPrepError(
-                "Microsoft.DPrep.ErrorValues.InvalidArgument", originalValue="Path", properties="")}]
-        with pytest.raises(ValueError) as e:
-            _get_and_validate_download_list(download_records, False, _get_logger())
-        assert "Some files have failed to download:" \
-               "('Path', 'Microsoft.DPrep.ErrorValues.InvalidArgument')" in str(e.value)
+        download_records = [{'DestinationFile': DataPrepError("Microsoft.DPrep.ErrorValues.InvalidArgument",
+                                                              originalValue="Path",
+                                                              properties="")}]
+        with pytest.raises(UserErrorException, match='Some files have failed to download:.*'):
+            _validate_downloads(download_records, False, _get_logger())
 
         # Raises RuntimeError
-        with pytest.raises(RuntimeError) as e:
-            _get_and_validate_download_list([{'DestinationFile': 'expectingRuntimeError'}], False, _get_logger())
-        assert "Unexpected error during file download" in str(e.value)
+        with pytest.raises(RuntimeError, match="Unexpected error during file download.*"):
+            _validate_downloads([{'DestinationFile': 'expectingRuntimeError'}], False, _get_logger())
 
         # Happy path
         stream_info_object = StreamInfo(handler='Local', arguments={}, resource_identifier='C:/path/Titanic2.csv')
         download_records = [{'DestinationFile': stream_info_object}]
-        download_records_happy_path = _get_and_validate_download_list(download_records, False, _get_logger())
-        assert download_records_happy_path == ['C:/path/Titanic2.csv']
+        assert _validate_downloads(download_records, False, _get_logger()) == ['C:/path/Titanic2.csv']
```

## Comparing `mltable-1.3.0.dist-info/LICENSE.txt` & `mltable-1.4.0.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `mltable-1.3.0.dist-info/METADATA` & `mltable-1.4.0.dist-info/METADATA`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: mltable
-Version: 1.3.0
+Version: 1.4.0
 Summary: Contains MLTable loading and authoring apis for the mltable package.
 Home-page: https://docs.microsoft.com/python/api/overview/azure/ml/?view=azure-ml-py
 Author: Microsoft Corp
 License: Proprietary https://aka.ms/azureml-preview-sdk-license 
 Platform: UNKNOWN
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Intended Audience :: Developers
@@ -19,15 +19,15 @@
 Classifier: Programming Language :: Python :: 3.10
 Classifier: Operating System :: MacOS
 Classifier: Operating System :: Microsoft :: Windows
 Classifier: Operating System :: POSIX :: Linux
 Classifier: Topic :: Scientific/Engineering
 Requires-Python: >=3.6,< 4.0
 Description-Content-Type: text/x-rst
-Requires-Dist: azureml-dataprep[parquet] (<4.11.0a,>=4.10.0a)
+Requires-Dist: azureml-dataprep[parquet] (<4.12.0a,>=4.11.1)
 Requires-Dist: pyyaml (<7.0.0,>=5.1.0)
 Requires-Dist: jsonschema (<5.0.0,>=4.0.0)
 Requires-Dist: msrest (>=0.6.18)
 Requires-Dist: azure-core (!=1.22.0,<2.0.0,>=1.8.0)
 Requires-Dist: azure-mgmt-core (<2.0.0,>=1.3.0)
 Requires-Dist: python-dateutil (<3.0.0,>=2.7.3)
 Requires-Dist: cryptography (!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<41)
@@ -63,14 +63,18 @@
 
 The official documentation is hosted on [Create a mltable data asset.](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-data-assets?tabs=cli#create-a-mltable-data-asset) 
 
 MLTable artifact’s metadata file is called  MLTable which adheres to the [AzureML MLTable schema](https://learn.microsoft.com/en-us/azure/machine-learning/reference-yaml-mltable). 
 
 # Release History
 
+## 1.4.0 (2023-05-31)
+ - Updating runtime dependencies
+ - Imporved error handling and argument validation
+
 ## 1.3.0 (2023-04-07)
 
 ### Features Added
  - bugfix (user error mapping, mltable save/load roundtrip)
 
 ## 1.2.0 (2023-02-22)
```

## Comparing `mltable-1.3.0.dist-info/RECORD` & `mltable-1.4.0.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 mltable/__init__.py,sha256=KWXO09yBdXoBfbE56ZFQN72ICn_-XPx_ltjVksTSWgU,937
-mltable/_utils.py,sha256=9lb3CfUI8Luq0euX4DXiBLrETWWtORAnS09MX9Ie5NQ,4193
-mltable/_validation_and_error_handler.py,sha256=gSyVnbPtHUtQJmdx7OTJC5jOI-Dd3UxekaKpbihIkFo,4404
-mltable/mltable.py,sha256=yysbW6JZqczSp_ZhaS30Dd1u89EktuZuDXxEU_ZQAzY,90746
+mltable/_utils.py,sha256=Rxia-ls-W1vhGKXsieN6Qim4GYbS6F8sYsarFbjp2kY,4378
+mltable/_validation_and_error_handler.py,sha256=lziukx98E7GAmVutkQk3-JFYAJhfSJrJkpJGakaVI6A,6793
+mltable/mltable.py,sha256=YnMvP-QVJNuk-5hLh6FjvHdBPCmPvdED-WsE6MmB6aI,91002
 mltable/_aml_utilities/__init__.py,sha256=JlCCObuOLZyNhIZlsoL51KtFxiY4xKIIzIRDBcdeu6Y,183
-mltable/_aml_utilities/_aml_rest_client_helper.py,sha256=dB5A_9wDumn31yvXgthwRUoBCrLZ1r6jm8J6G1SUpEw,7321
+mltable/_aml_utilities/_aml_rest_client_helper.py,sha256=R62dlL6f6_X8GdBe4D_mDy_xd-X8qJAqjLqbkE0wSA8,7521
 mltable/_aml_utilities/_azureml_token_authentication.py,sha256=ixmzyWC4gvT8ia16q3aXjYWH58KYBWycZPZEWEQ2xGw,23222
 mltable/_aml_utilities/_restclient/__init__.py,sha256=38lKUIqL59KqhES7ZGBUGcrELWICWet0VFLxwY4W0fo,893
 mltable/_aml_utilities/_restclient/_azure_machine_learning_workspaces.py,sha256=OwUbiGVcnH-AkIrCkU4S3W_Nrde5-IsricQQVwXZMqE,4218
 mltable/_aml_utilities/_restclient/_configuration.py,sha256=E52K3BmA4ZqAE6oP5VjRK32HsBrl6W9Y1oOsWCJu8Ig,3538
 mltable/_aml_utilities/_restclient/_patch.py,sha256=wuqrJGWK488sJvWwSq6iwPTqil7TPaRadoxE7BMK0tA,1561
 mltable/_aml_utilities/_restclient/_version.py,sha256=72yoX3gRVc4OFrnlCjEq_1cS0FLSIvofYIXtMN1ElvI,495
 mltable/_aml_utilities/_restclient/models.py,sha256=tjqC6v_UMQnzxwfUWjLDMCV7YM7Td__4crr8zzNy4hM,365
@@ -45,26 +45,26 @@
 mltable/_common/chained_identity.py,sha256=1eExz4Ga4oEtrpD7oYcDkRfBgK1HUJidj9T-SsgU4Pw,2393
 mltable/_common/logged_lock.py,sha256=ZIINoMdzt4j-myA1nofPIZN7Gee5vUFU-dBjslFKrAA,933
 mltable/_common/async_utils/__init__.py,sha256=JlCCObuOLZyNhIZlsoL51KtFxiY4xKIIzIRDBcdeu6Y,183
 mltable/_common/async_utils/async_task.py,sha256=F6iilclJAWER2RfJg71LypoQbnNyVSDng2sitvddbtQ,2827
 mltable/_common/async_utils/daemon.py,sha256=SvsUKj-NGExutabI-Vaj6w2vC_XXicf8pLsNwB-8FRs,2329
 mltable/_common/async_utils/task_queue.py,sha256=mP4Ajv_fMH83VHS1yxHfPAJWK_npmWdjLZ-U-z0dLBw,5911
 mltable/_common/async_utils/worker_pool.py,sha256=2KOSfRMzIjrTmJ6y5cMa5mhe0bwoS3ELawGt2EGWWnc,1112
-mltable/schema/MLTable.json,sha256=T9kp_NOQu_NCYRtSO4uUvnbxunhD1rbv3oUEZQGzvoA,31845
+mltable/schema/MLTable.json,sha256=b37hnymBIIuaGje0MQjH2yYzH242zmmZf6tVkZJP7Lc,31957
 mltable/tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 mltable/tests/conftest.py,sha256=647QZtFYv7mKDXbD6nckadaamC5O7YCEb37oG0wDPqs,2129
 mltable/tests/helper_functions.py,sha256=np4dzULX4gywmwr7o3QKTqs-5opIOzBFo4d-aKkA7P8,2120
-mltable/tests/test_from_delta_lake.py,sha256=I4AcJuSMVEsaqYJnFHMjlT-M4GVsQfrYEnMKD15qKUQ,11329
-mltable/tests/test_from_json_lines.py,sha256=jKTEhZa9cQR-za4CFoMCpt2uCTN1lhZdG7XqPjUxW_Q,3968
-mltable/tests/test_mltable_authoring_apis.py,sha256=XaHRFy7NnabRC1dolQOUyf7ippFKC5-QiREZghHK_uY,61415
-mltable/tests/test_mltable_inner_functions.py,sha256=hjdMDU3D5KyPJrZ7AeXrClulalsy5TpnhvU8taUvLAg,15184
-mltable/tests/test_mltable_load.py,sha256=j-RGFG7D6noFRSPzZEptW98hZ5mEDyqY8WHFhihb8p8,5148
-mltable/tests/test_mltable_load_to_pandas.py,sha256=GcW3uuTGvQFnIq_XBGo9h8CoJ9w_eHrQjwfSaqXHRGk,9662
-mltable/tests/test_mltable_save.py,sha256=uLYJtEjdG4iqwUWyv5jLrQnTJCkVvX1Agfn8UEHDiCk,6708
-mltable/tests/test_partition_api.py,sha256=re7_p8O2Uhb3O52E9UGHmxx5VU_Ajtj6fr9iFH00Rko,1477
+mltable/tests/test_from_delta_lake.py,sha256=f0UEEEOVDxrleU1CFmvKVmwveobLvrn_yzKGrWHaXU0,8543
+mltable/tests/test_from_json_lines.py,sha256=zTxTTsTYPU4TFaSXhnQWmsaHwT9ze_e_VXD38hMZtuY,4088
+mltable/tests/test_mltable_authoring_apis.py,sha256=nmXHr3Si2JTbrvFijfAQCADp3ZFksPNkdU8uTLxt52Y,61372
+mltable/tests/test_mltable_inner_functions.py,sha256=tKk7Sp1z4lKGpU-f32aMYWrkA4hF26SWC0-R8-jjfDU,15096
+mltable/tests/test_mltable_load.py,sha256=XH9WAa3rt1Ov26HdcrYcQwrgIKHS8og5lqgUBMIim1U,5122
+mltable/tests/test_mltable_load_to_pandas.py,sha256=cR9Qa1bcXDmIXzBfGV2R6Wd8lj074HskrypDU2eHXWY,9605
+mltable/tests/test_mltable_save.py,sha256=lYd76Gl5D0LujsvmFxbI12dmYz18-zqvQltxJB2cGno,6897
+mltable/tests/test_partition_api.py,sha256=k9oD9bU_0gS4aNOyif7jB4e2oMuJtN98rajZhM13N_M,1481
 mltable/tests/test_schema.py,sha256=dSqvIBDMl1_lJAIcUt1AD6i2UHOIW1OTterEdJgzguI,4438
-mltable/tests/test_validation_and_error_handler.py,sha256=pkCsByeBM56pXY70X6x4X-cL6GEpn48OimdNMO1P0Eg,5294
-mltable-1.3.0.dist-info/LICENSE.txt,sha256=FOfkEEz4uS7g278F9Rq12rqKF3lLyBUZhVpLetuZrTg,1021
-mltable-1.3.0.dist-info/METADATA,sha256=5ecreF_8eJ2L6QLaYaCum89D4VgnYq_iV_BBz9E0WHY,3886
-mltable-1.3.0.dist-info/WHEEL,sha256=YUYzQ6UQdoqxXjimOitTqynltBCkwY6qlTfTh2IzqQU,97
-mltable-1.3.0.dist-info/top_level.txt,sha256=I64OiB-4blTtBhX1oQN-PS7OthFSwr7mUOLruISDi4c,8
-mltable-1.3.0.dist-info/RECORD,,
+mltable/tests/test_validation_and_error_handler.py,sha256=_h9cFSXpWnPpfSqABdw4ewd7zjtyfF-hY7WkAf0N8BA,4886
+mltable-1.4.0.dist-info/LICENSE.txt,sha256=FOfkEEz4uS7g278F9Rq12rqKF3lLyBUZhVpLetuZrTg,1021
+mltable-1.4.0.dist-info/METADATA,sha256=x-X0FlghMzM6KJNWXzWXm4JlnhouQHpoa9vKliARBZQ,3992
+mltable-1.4.0.dist-info/WHEEL,sha256=YUYzQ6UQdoqxXjimOitTqynltBCkwY6qlTfTh2IzqQU,97
+mltable-1.4.0.dist-info/top_level.txt,sha256=I64OiB-4blTtBhX1oQN-PS7OthFSwr7mUOLruISDi4c,8
+mltable-1.4.0.dist-info/RECORD,,
```

